{
    "Yonatan Bisk": [
        {
            "title": "Toward General-Purpose Robots via Foundation Models: A Survey and Meta-Analysis",
            "authors": "Yafei Hu, Quanting Xie, Vidhi Jain, Jonathan Francis, Jay Patrikar, Nikhil Keetha, Seungchan Kim, Yaqi Xie, Tianyi Zhang, Zhibo Zhao, Yu-Quan Chong, Chen Wang, Katia Sycara, Matthew Johnson-Roberson, Dhruv Batra, Xiaolong Wang, Sebastian Scherer, Zsolt Kira, Fei Xia, Yonatan Bisk",
            "year": "2023",
            "abstract": "Building general-purpose robots that can operate seamlessly, in any environment, with any object, and utilizing various skills to complete diverse tasks has been a long-standing goal in Artificial Intelligence. Unfortunately, however, most existing robotic systems have been constrained - having been designed for specific tasks, trained on specific datasets, and deployed within specific environments. These systems usually require extensively-labeled data, rely on task-specific models, have numerous generalization issues when deployed in real-world scenarios, and struggle to remain robust to distribution shifts. Motivated by the impressive open-set performance and content generation capabilities of web-scale, large-capacity pre-trained models (i.e., foundation models) in research fields such as Natural Language Processing (NLP) and Computer Vision (CV), we devote this survey to exploring (i) how these existing foundation models from NLP and CV can be applied to the field of robotics, and also exploring (ii) what a robotics-specific foundation model would look like. We begin by providing an overview of what constitutes a conventional robotic system and the fundamental barriers to making it universally applicable. Next, we establish a taxonomy to discuss current work exploring ways to leverage existing foundation models for robotics and develop ones catered to robotics. Finally, we discuss key challenges and promising future directions in using foundation models for enabling general-purpose robotic systems. We encourage readers to view our ``living`` GitHub repository of resources, including papers reviewed in this survey as well as related\u00a0\u2026",
            "url": "https://arxiv.org/pdf/2312.08782",
            "publicationVenue": "arXiv preprint arXiv:2312.08782",
            "citation_count": 3
        },
        {
            "title": "Open x-embodiment: Robotic learning datasets and RT-x models",
            "authors": "Quan Vuong, Sergey Levine, Homer Rich Walke, Karl Pertsch, Anikait Singh, Ria Doshi, Charles Xu, Jianlan Luo, Liam Tan, Dhruv Shah, Chelsea Finn, Max Du, Moo Jin Kim, Alexander Khazatsky, Jonathan Heewon Yang, Tony Z Zhao, Ken Goldberg, Ryan Hoque, Lawrence Yunliang Chen, Simeon Adebola, Gaurav S Sukhatme, Gautam Salhotra, Shivin Dass, Lerrel Pinto, Zichen Jeff Cui, Siddhant Haldar, Anant Rai, Nur Muhammad Mahi Shafiullah, Yuke Zhu, Yifeng Zhu, Soroush Nasiriany, Shuran Song, Cheng Chi, Chuer Pan, Wolfram Burgard, Oier Mees, Chenguang Huang, Deepak Pathak, Shikhar Bahl, Russell Mendonca, Gaoyue Zhou, Mohan Kumar Srirama, Sudeep Dasari, Cewu Lu, Hao-Shu Fang, Hongjie Fang, Henrik I Christensen, Masayoshi Tomizuka, Wei Zhan, Mingyu Ding, Chenfeng Xu, Xinghao Zhu, Ran Tian, Youngwoon Lee, Dorsa Sadigh, Yuchen Cui, Suneel Belkhale, Priya Sundaresan, Trevor Darrell, Jitendra Malik, Ilija Radosavovic, Jeannette Bohg, Krishnan Srinivasan, Xiaolong Wang, Nicklas Hansen, Yueh-Hua Wu, Ge Yan, Hao Su, Jiayuan Gu, Xuanlin Li, Niko Suenderhauf, Krishan Rana, Ben Burgess-Limerick, Federico Ceola, Kento Kawaharazuka, Naoaki Kanazawa, Tatsuya Matsushima, Yutaka Matsuo, Yusuke Iwasawa, Hiroki Furuta, Jihoon Oh, Tatsuya Harada, Takayuki Osa, Yujin Tang, Oliver Kroemer, Mohit Sharma, Kevin Lee Zhang, Beomjoon Kim, Yoonyoung Cho, Junhyek Han, Jaehyung Kim, Joseph J Lim, Edward Johns, Norman Di Palo, Freek Stulp, Antonin Raffin, Samuel Bustamante, Jo\u00e3o Silverio, Abhishek Padalkar, Jan Peters, Bernhard Sch\u00f6lkopf, Dieter B\u00fcchler, Jan Schneider, Simon Guist, Jiajun Wu, Stephen Tian, Haochen Shi, Yunzhu Li, Yixuan Wang, Mingtong Zhang, Heni Ben Amor, Yifan Zhou, Keyvan Majd, Lionel Ott, Giulio Schiavi, Roberto Mart\u00edn-Mart\u00edn, Rutav Shah, Yonatan Bisk, Jeffrey T Bingham, Tianhe Yu, Vidhi Jain, Ted Xiao, Karol Hausman, Christine Chan, Alexander Herzog, Zhuo Xu, Sean Kirmani, Vincent Vanhoucke, Ryan Julian, Lisa Lee, Tianli Ding, Yevgen Chebotar, Jie Tan, Jacky Liang, Igor Mordatch, Kanishka Rao, Yao Lu, Keerthana Gopalakrishnan, Stefan Welker, Nikhil J Joshi, Coline Manon Devin, Alex Irpan, Sherry Moore, Ayzaan Wahid, Jialin Wu, Xi Chen, Paul Wohlhart, Alex Bewley, Wenxuan Zhou, Isabel Leal",
            "year": "2023",
            "abstract": "Large, high-capacity models trained on diverse datasets have shown remarkable successes on efficiently tackling downstream applications. In domains from NLP to Computer Vision, this has led to a consolidation of pretrained models, with general pretrained backbones serving as a starting point for many applications. Can such a consolidation happen in robotics? Conventionally, robotic learning methods train a separate model for every application, every robot, and even every environment. Can we instead train ``generalist\u2019\u2019 X-robot policy that can be adapted efficiently to new robots, tasks, and environments? In this paper, we provide datasets in standardized data formats and models to make it possible to explore this possibility in the context of robotic manipulation, alongside experimental results that provide an example of effective X-robot policies. We assemble a dataset from 22 different robots collected through a collaboration between 21 institutions, demonstrating 527 skills (160266 tasks). We show that a high-capacity model trained on this data, which we call RT-X, exhibits positive transfer and improves the capabilities of multiple robots by leveraging experience from other platforms.",
            "url": "https://openreview.net/pdf?id=zraBtFgxT0",
            "publicationVenue": "Towards Generalist Robots: Learning Paradigms for Scalable Skill Acquisition@ CoRL2023",
            "citation_count": 4
        },
        {
            "title": "SOTOPIA: Interactive Evaluation for Social Intelligence in Language Agents",
            "authors": "Xuhui Zhou, Hao Zhu, Leena Mathur, Ruohong Zhang, Haofei Yu, Zhengyang Qi, Louis-Philippe Morency, Yonatan Bisk, Daniel Fried, Graham Neubig, Maarten Sap",
            "year": "2023",
            "abstract": "Humans are social beings; we pursue social goals in our daily interactions, which is a crucial aspect of social intelligence. Yet, AI systems' abilities in this realm remain elusive. We present SOTOPIA, an open-ended environment to simulate complex social interactions between artificial agents and evaluate their social intelligence. In our environment, agents role-play and interact under a wide variety of scenarios; they coordinate, collaborate, exchange, and compete with each other to achieve complex social goals. We simulate the role-play interaction between LLM-based agents and humans within this task space and evaluate their performance with a holistic evaluation framework called SOTOPIA-Eval. With SOTOPIA, we find significant differences between these models in terms of their social intelligence, and we identify a subset of SOTOPIA scenarios, SOTOPIA-hard, that is generally challenging for all models. We find that on this subset, GPT-4 achieves a significantly lower goal completion rate than humans and struggles to exhibit social commonsense reasoning and strategic communication skills. These findings demonstrate SOTOPIA's promise as a general platform for research on evaluating and improving social intelligence in artificial agents.",
            "url": "https://arxiv.org/pdf/2310.11667",
            "publicationVenue": "The Twelfth International Conference on Learning Representations (ICLR)",
            "citation_count": 13
        },
        {
            "title": "Open X-Embodiment: Robotic Learning Datasets and RT-X Models",
            "authors": "Abhishek Padalkar, Acorn Pooley, Ajinkya Jain, Alex Bewley, Alex Herzog, Alex Irpan, Alexander Khazatsky, Anant Rai, Anikait Singh, Anthony Brohan, Antonin Raffin, Ayzaan Wahid, Ben Burgess-Limerick, Beomjoon Kim, Bernhard Sch\u00f6lkopf, Brian Ichter, Cewu Lu, Charles Xu, Chelsea Finn, Chenfeng Xu, Cheng Chi, Chenguang Huang, Christine Chan, Chuer Pan, Chuyuan Fu, Coline Devin, Danny Driess, Deepak Pathak, Dhruv Shah, Dieter B\u00fcchler, Dmitry Kalashnikov, Dorsa Sadigh, Edward Johns, Federico Ceola, Fei Xia, Freek Stulp, Gaoyue Zhou, Gaurav S Sukhatme, Gautam Salhotra, Ge Yan, Giulio Schiavi, Gregory Kahn, Hao Su, Hao-Shu Fang, Haochen Shi, Heni Ben Amor, Henrik I Christensen, Hiroki Furuta, Homer Walke, Hongjie Fang, Igor Mordatch, Ilija Radosavovic, Isabel Leal, Jacky Liang, Jad Abou-Chakra, Jaehyung Kim, Jan Peters, Jan Schneider, Jasmine Hsu, Jeannette Bohg, Jeffrey Bingham, Jiajun Wu, Jialin Wu, Jianlan Luo, Jiayuan Gu, Jie Tan, Jihoon Oh, Jitendra Malik, Jonathan Tompson, Jonathan Yang, Joseph J Lim, Jo\u00e3o Silverio, Junhyek Han, Kanishka Rao, Karl Pertsch, Karol Hausman, Keegan Go, Keerthana Gopalakrishnan, Ken Goldberg, Kendra Byrne, Kenneth Oslund, Kento Kawaharazuka, Kevin Zhang, Krishan Rana, Krishnan Srinivasan, Lawrence Yunliang Chen, Lerrel Pinto, Liam Tan, Lionel Ott, Lisa Lee, Masayoshi Tomizuka, Maximilian Du, Michael Ahn, Mingtong Zhang, Mingyu Ding, Mohan Kumar Srirama, Mohit Sharma, Moo Jin Kim, Naoaki Kanazawa, Nicklas Hansen, Nicolas Heess, Nikhil J Joshi, Niko Suenderhauf, Norman Di Palo, Nur Muhammad Mahi Shafiullah, Oier Mees, Oliver Kroemer, Pannag R Sanketi, Paul Wohlhart, Peng Xu, Pierre Sermanet, Priya Sundaresan, Quan Vuong, Rafael Rafailov, Ran Tian, Ria Doshi, Roberto Mart\u00edn-Mart\u00edn, Russell Mendonca, Rutav Shah, Ryan Hoque, Ryan Julian, Samuel Bustamante, Sean Kirmani, Sergey Levine, Sherry Moore, Shikhar Bahl, Shivin Dass, Shubham Sonawani, Shuran Song, Sichun Xu, Siddhant Haldar, Simeon Adebola, Simon Guist, Soroush Nasiriany, Stefan Schaal, Stefan Welker, Stephen Tian, Sudeep Dasari, Suneel Belkhale, Takayuki Osa, Tatsuya Harada, Tatsuya Matsushima, Ted Xiao, Tianhe Yu, Tianli Ding, Todor Davchev, Tony Z Zhao, Travis Armstrong, Trevor Darrell, Vidhi Jain",
            "year": "2023",
            "abstract": "Large, high-capacity models trained on diverse datasets have shown remarkable successes on efficiently tackling downstream applications. In domains from NLP to Computer Vision, this has led to a consolidation of pretrained models, with general pretrained backbones serving as a starting point for many applications. Can such a consolidation happen in robotics? Conventionally, robotic learning methods train a separate model for every application, every robot, and even every environment. Can we instead train generalist X-robot policy that can be adapted efficiently to new robots, tasks, and environments? In this paper, we provide datasets in standardized data formats and models to make it possible to explore this possibility in the context of robotic manipulation, alongside experimental results that provide an example of effective X-robot policies. We assemble a dataset from 22 different robots collected through a collaboration between 21 institutions, demonstrating 527 skills (160266 tasks). We show that a high-capacity model trained on this data, which we call RT-X, exhibits positive transfer and improves the capabilities of multiple robots by leveraging experience from other platforms.",
            "url": "https://arxiv.org/html/2310.08864v4",
            "publicationVenue": "arXiv preprint arXiv:2310.08864",
            "citation_count": 47
        },
        {
            "title": "Reasoning about the Unseen for Efficient Outdoor Object Navigation",
            "authors": "Quanting Xie, Tianyi Zhang, Kedi Xu, Matthew Johnson-Roberson, Yonatan Bisk",
            "year": "2023",
            "abstract": "Robots should exist anywhere humans do: indoors, outdoors, and even unmapped environments. In contrast, the focus of recent advancements in Object Goal Navigation(OGN) has targeted navigating in indoor environments by leveraging spatial and semantic cues that do not generalize outdoors. While these contributions provide valuable insights into indoor scenarios, the broader spectrum of real-world robotic applications often extends to outdoor settings. As we transition to the vast and complex terrains of outdoor environments, new challenges emerge. Unlike the structured layouts found indoors, outdoor environments lack clear spatial delineations and are riddled with inherent semantic ambiguities. Despite this, humans navigate with ease because we can reason about the unseen. We introduce a new task OUTDOOR, a new mechanism for Large Language Models (LLMs) to accurately hallucinate possible futures, and a new computationally aware success metric for pushing research forward in this more complex domain. Additionally, we show impressive results on both a simulated drone and physical quadruped in outdoor environments. Our agent has no premapping and our formalism outperforms naive LLM-based approaches",
            "url": "https://arxiv.org/pdf/2309.10103",
            "publicationVenue": "arXiv preprint arXiv:2309.10103",
            "citation_count": 0
        },
        {
            "title": "SLAP: Spatial-Language Attention Policies",
            "authors": "Priyam Parashar, Vidhi Jain, Xiaohan Zhang, Jay Vakil, Sam Powers, Yonatan Bisk, Chris Paxton",
            "year": "2023",
            "abstract": "Despite great strides in language-guided manipulation, existing work has been constrained to table-top settings. Table-tops allow for perfect and consistent camera angles, properties are that do not hold in mobile manipulation. Task plans that involve moving around the environment must be robust to egocentric views and changes in the plane and angle of grasp. A further challenge is ensuring this is all true while still being able to learn skills efficiently from limited data. We propose Spatial-Language Attention Policies (SLAP) as a solution. SLAP uses three-dimensional tokens as the input representation to train a single multi-task, language-conditioned action prediction policy. Our method shows an 80% success rate in the real world across eight tasks with a single model, and a 47.5% success rate when unseen clutter and unseen object configurations are introduced, even with only a handful of examples per task. This represents an improvement of 30% over prior work (20% given unseen distractors and configurations). We see a 4x improvement over baseline in mobile manipulation setting. In addition, we show how SLAPs robustness allows us to execute Task Plans from open-vocabulary instructions using a large language model for multi-step mobile manipulation. For videos, see the website: https://robotslap.github.io",
            "url": "https://openreview.net/pdf?id=7Pkzm2FgUmq",
            "publicationVenue": "7th Annual Conference on Robot Learning",
            "citation_count": 0
        },
        {
            "title": "WebArena: A Realistic Web Environment for Building Autonomous Agents",
            "authors": "Shuyan Zhou, Frank F Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Yonatan Bisk, Daniel Fried, Uri Alon, Graham Neubig",
            "year": "2023",
            "abstract": "With generative AI advances, the exciting potential for autonomous agents to manage daily tasks via natural language commands has emerged. However, cur rent agents are primarily created and tested in simplified synthetic environments, substantially limiting real-world scenario representation. In this paper, we build an environment for agent command and control that is highly realistic and reproducible. Specifically, we focus on agents that perform tasks on websites, and we create an environment with fully functional websites from four common domains: e-commerce, social forum discussions, collaborative software development, and content management. Our environment is enriched with tools (e.g., a map) and external knowledge bases (e.g., user manuals) to encourage human-like task-solving. Building upon our environment, we release a set of benchmark tasks focusing on evaluating the functional correctness of task completions. The tasks in our benchmark are diverse, long-horizon, and are designed to emulate tasks that humans routinely perform on the internet. We design and implement several autonomous agents, integrating recent techniques such as reasoning before acting. The results demonstrate that solving complex tasks is challenging: our best GPT-4-based agent only achieves an end-to-end task success rate of 10.59%. These results highlight the need for further development of robust agents, that current state-of-the-art LMs are far from perfect performance in these real-life tasks, and that WebArena can be used to measure such progress. Our code, data, environment reproduction resources, and video demonstrations are\u00a0\u2026",
            "url": "https://arxiv.org/pdf/2307.13854",
            "publicationVenue": "The Twelfth International Conference on Learning Representations (ICLR)",
            "citation_count": 59
        },
        {
            "title": "SPAE: Semantic Pyramid AutoEncoder for Multimodal Generation with Frozen LLMs",
            "authors": "Lijun Yu, Yong Cheng, Zhiruo Wang, Vivek Kumar, Wolfgang Macherey, Yanping Huang, David A Ross, Irfan Essa, Yonatan Bisk, Ming-Hsuan Yang, Kevin Murphy, Alexander G Hauptmann, Lu Jiang",
            "year": "2023",
            "abstract": "In this work, we introduce Semantic Pyramid AutoEncoder (SPAE) for enabling frozen LLMs to perform both understanding and generation tasks involving non-linguistic modalities such as images or videos. SPAE converts between raw pixels and interpretable lexical tokens (or words) extracted from the LLM's vocabulary. The resulting tokens capture both the rich semantic meaning and the fine-grained details needed for visual reconstruction, effectively translating the visual content into a language comprehensible to the LLM, and empowering it to perform a wide array of multimodal tasks. Our approach is validated through in-context learning experiments with frozen PaLM 2 and GPT 3.5 on a diverse set of image understanding and generation tasks. Our method marks the first successful attempt to enable a frozen LLM to generate image content while surpassing state-of-the-art performance in image understanding tasks, under the same setting, by over 25%.",
            "url": "https://proceedings.neurips.cc/paper_files/paper/2023/file/a526cc8f6ffb74bedb6ff313e3fdb450-Paper-Conference.pdf",
            "publicationVenue": "NeurIPS",
            "citation_count": 9
        },
        {
            "title": "HomeRobot: Open-Vocabulary Mobile Manipulation",
            "authors": "Sriram Yenamandra, Arun Ramachandran, Karmesh Yadav, Austin Wang, Mukul Khanna, Theophile Gervet, Tsung-Yen Yang, Vidhi Jain, Alexander William Clegg, John Turner, Zsolt Kira, Manolis Savva, Angel Chang, Devendra Singh Chaplot, Dhruv Batra, Roozbeh Mottaghi, Yonatan Bisk, Chris Paxton",
            "year": "2023",
            "abstract": "HomeRobot (noun): An affordable compliant robot that navigates homes and manipulates a wide range of objects in order to complete everyday tasks. Open-Vocabulary Mobile Manipulation (OVMM) is the problem of picking any object in any unseen environment, and placing it in a commanded location. This is a foundational challenge for robots to be useful assistants in human environments, because it involves tackling sub-problems from across robotics: perception, language understanding, navigation, and manipulation are all essential to OVMM. In addition, integration of the solutions to these sub-problems poses its own substantial challenges. To drive research in this area, we introduce the HomeRobot OVMM benchmark, where an agent navigates household environments to grasp novel objects and place them on target receptacles. HomeRobot has two components: a simulation component, which uses a large and diverse curated object set in new, high-quality multi-room home environments; and a real-world component, providing a software stack for the low-cost Hello Robot Stretch to encourage replication of real-world experiments across labs. We implement both reinforcement learning and heuristic (model-based) baselines and show evidence of sim-to-real transfer. Our baselines achieve a 20% success rate in the real world; our experiments identify ways future research work improve performance. See videos on our website: https://ovmm.github.io/.",
            "url": "https://arxiv.org/pdf/2306.11565",
            "publicationVenue": "7th Annual Conference on Robot Learning",
            "citation_count": 20
        },
        {
            "title": "Plan, Eliminate, and Track--Language Models are Good Teachers for Embodied Agents",
            "authors": "Yue Wu, So Yeon Min, Yonatan Bisk, Ruslan Salakhutdinov, Amos Azaria, Yuanzhi Li, Tom Mitchell, Shrimai Prabhumoye",
            "year": "2023",
            "abstract": "Pre-trained large language models (LLMs) capture procedural knowledge about the world. Recent work has leveraged LLM's ability to generate abstract plans to simplify challenging control tasks, either by action scoring, or action modeling (fine-tuning). However, the transformer architecture inherits several constraints that make it difficult for the LLM to directly serve as the agent: e.g. limited input lengths, fine-tuning inefficiency, bias from pre-training, and incompatibility with non-text environments. To maintain compatibility with a low-level trainable actor, we propose to instead use the knowledge in LLMs to simplify the control problem, rather than solving it. We propose the Plan, Eliminate, and Track (PET) framework. The Plan module translates a task description into a list of high-level sub-tasks. The Eliminate module masks out irrelevant objects and receptacles from the observation for the current sub-task. Finally, the Track module determines whether the agent has accomplished each sub-task. On the AlfWorld instruction following benchmark, the PET framework leads to a significant 15% improvement over SOTA for generalization to human goal specifications.",
            "url": "https://arxiv.org/pdf/2305.02412",
            "publicationVenue": "arXiv preprint arXiv:2305.02412",
            "citation_count": 20
        },
        {
            "title": "The Framework Tax: Disparities Between Inference Efficiency in Research and Deployment",
            "authors": "Jared Fernandez, Jacob Kahn, Clara Na, Yonatan Bisk, Emma Strubell",
            "year": "2023",
            "abstract": "Increased focus on the deployment of machine learning systems has led to rapid improvements in hardware accelerator performance and neural network model efficiency. However, the resulting reductions in floating point operations and increases in computational throughput of accelerators have not directly translated to improvements in real-world inference latency. We demonstrate that these discrepancies can be largely attributed to mis-alignments between model architectures and the capabilities of underlying hardware due to bottlenecks introduced by deep learning frameworks. We denote this phenomena as the \\textit{framework tax}, and observe that the disparity is growing as hardware speed increases over time. In this work, we examine this phenomena through a series of case studies analyzing the effects of model design decisions, framework paradigms, and hardware platforms on total model latency. Based on our findings, we provide actionable recommendations to ML researchers and practitioners aimed at narrowing the gap between efficient ML model research and practice.",
            "url": "https://arxiv.org/pdf/2302.06117",
            "publicationVenue": "arXiv preprint arXiv:2302.06117",
            "citation_count": 1
        },
        {
            "title": "OPERA: Operations-oriented Probabilistic Extraction, Reasoning, and Analysis",
            "authors": "Yonatan Bisk, Hans Chalupsky, Carnegie Mellon University",
            "year": "2023",
            "abstract": "The OPERA system (for Operations-oriented Probabilistic Extraction, Reasoning, and Analysis) developed jointly by CMU and USC/ISI is an integrated solution to the challenges of DARPAs Active Interpretation of Disparate Alternatives (AIDA) program in the form of:(i) high-performance media analysis (TA1) for text, speech, and image/video data,(ii) semantic representation and reasoning support (TA1 and TA2),(iii) cross-medium and cross-language integration (TA2), and (iv) hypothesis creation, management, and hypothesis exploration (TA3). Given that all required components of such a systemare still active areas of research, the creation of a single system (pipelined or otherwise) has the potential for a substantialrate of compounded errors. Early versions of the system created had strong abstraction boundaries for limited informationsharing between systems. Later incarnations benefited from allowing for the output of extractors to be coupled with raw textstrings and embedding vectors. These prove especially advantageous in the presence of large-scale language models thatencode world knowledge, and when aligning predictions to an open-domain ontology, like that of WikiData.",
            "url": "https://apps.dtic.mil/sti/trecms/pdf/AD1190961.pdf",
            "publicationVenue": null,
            "citation_count": 0
        },
        {
            "title": "HomeRobot: An Open Source Software Stack for Mobile Manipulation Research",
            "authors": "Chris Paxton, Austin Wang, Binit Shah, Blaine Matulevich, Dhruv Shah, Karmesh Yadav, Santhosh Ramakrishnan, Sriram Yenamandra, Yonatan Bisk",
            "year": "2023",
            "abstract": "Reproducibility in robotics research requires capable, shared hardware platforms which can be used for a wide variety of research. We\u2019ve seen the power of these sorts of shared platforms in more general machine learning research, where there is constant iteration on shared AI platforms like PyTorch. To be able to make rapid progress in robotics in the same way, we propose that we need:(1) shared real-world platforms which allow different teams to test and compare methods at low cost;(2) challenging simulations that reflect real-world environments and especially can drive perception and planning research; and (3) low-cost platforms with enough software to get started addressing all of these problems. To this end, we propose HomeRobot, a mobile manipulator software stack with associated benchmark in simulation, which is initially based on the low-cost, human-safe Hello Robot Stretch.",
            "url": "https://ojs.aaai.org/index.php/AAAI-SS/article/download/27723/27496",
            "publicationVenue": "Proceedings of the AAAI Symposium Series",
            "citation_count": 0
        },
        {
            "title": "EXCALIBUR: Encouraging and Evaluating Embodied Exploration",
            "authors": "Hao Zhu, Raghav Kapoor, So Yeon Min, Winson Han, Jiatai Li, Kaiwen Geng, Graham Neubig, Yonatan Bisk, Aniruddha Kembhavi, Luca Weihs",
            "year": "2023",
            "abstract": "Experience precedes understanding. Humans constantly explore and learn about their environment out of curiosity, gather information, and update their models of the world. On the other hand, machines are either trained to learn passively from static and fixed datasets, or taught to complete specific goal-conditioned tasks. To encourage the development of exploratory interactive agents, we present the EXCALIBUR benchmark. EXCALIBUR allows agents to explore their environment for long durations and then query their understanding of the physical world via inquiries like:\" is the small heavy red bowl made from glass?\" or\" is there a silver spoon heavier than the egg?\". This design encourages agents to perform free-form home exploration without myopia induced by goal conditioning. Once the agents have answered a series of questions, they can renter the scene to refine their knowledge, update their beliefs, and improve their performance on the questions. Our experiments demonstrate the challenges posed by this dataset for the present-day state-of-the-art embodied systems and the headroom afforded to develop new innovative methods. Finally, we present a virtual reality interface that enables humans to seamlessly interact within the simulated world and use it to gather human performance measures. EXCALIBUR affords unique challenges in comparison to present-day benchmarks and represents the next frontier for embodied AI research.",
            "url": "http://openaccess.thecvf.com/content/CVPR2023/papers/Zhu_EXCALIBUR_Encouraging_and_Evaluating_Embodied_Exploration_CVPR_2023_paper.pdf",
            "publicationVenue": null,
            "citation_count": 5
        },
        {
            "title": "Computational Language Acquisition with Theory of Mind",
            "authors": "Andy Liu, Hao Zhu, Emmy Liu, Yonatan Bisk, Graham Neubig",
            "year": "2023",
            "abstract": "Unlike current state-of-the-art language models, young children actively acquire language through interactions with their surrounding environment and caretakers. One mechanism that has been argued to be critical to language learning is the ability to infer the mental states of other agents in social environments, coined Theory of Mind (ToM) by Premack & Woodruff (1978). Drawing inspiration from the modern operationalized versions of ToM implemented in Rabinowitz et al. (2018) and Zhu et al. (2021), we build language-learning agents equipped with ToM, and measure its effects on the learning process. We model ToM by giving the speaker agent an internal listener model that is trained alongside the speaker and used to rerank potential utterances. We experiment with varying task difficulty, hypothesizing that models will acquire more complex language to adapt to stronger environmental pressures. We find that training speakers with a highly weighted ToM listener component leads to performance gains in our image referential game setting. We also find some evidence that increasing task difficulty in the training process results in more fluent and precise utterances in evaluation. This suggests the potential utility of further incorporating ToM, as well as other insights from child language acquisition, into computational models of language acquisition.",
            "url": "https://arxiv.org/pdf/2303.01502",
            "publicationVenue": null,
            "citation_count": 6
        },
        {
            "title": "Self-Supervised Object Goal Navigation with In-Situ Finetuning",
            "authors": "So Yeon Min, Yao-Hung Hubert Tsai, Wei Ding, Ali Farhadi, Ruslan Salakhutdinov, Yonatan Bisk, Jian Zhang",
            "year": "2023",
            "abstract": "A household robot should be able to navigate to target objects without requiring users to first annotate everything in their home. Most current approaches to object navigation do not test on real robots and rely solely on reconstructed scans of houses and their expensively labeled semantic 3D meshes. In this work, our goal is to build an agent that builds self-supervised models of the world via exploration, the same as a child might - thus we (1) eschew the expense of labeled 3D mesh and (2) enable self-supervised in-situ finetuning in the real world. We identify a strong source of self-supervision (Location Consistency - LocCon) that can train all components of an ObjectNav agent, using unannotated simulated houses. Our key insight is that embodied agents can leverage location consistency as a self-supervision signal - collecting images from different views/angles and applying contrastive learning. We show that\u00a0\u2026",
            "url": null,
            "publicationVenue": "IEEE/RSJ International Conference on Intelligent Robots and Systems",
            "citation_count": 5
        },
        {
            "title": "Training Vision-Language Transformers from Captions",
            "authors": "Liangke Gui, Yingshan Chang, Qiuyuan Huang, Subhojit Som, Alex Hauptmann, Jianfeng Gao, Yonatan Bisk",
            "year": "2023",
            "abstract": "Vision-Language Transformers can be learned without low-level human labels (e.g. class labels, bounding boxes, etc). Existing work, whether explicitly utilizing bounding boxes (Chen et al., 2020b; Tan & Bansal, 2019; Lu et al., 2019) or patches (Kim et al., 2021), assumes that the visual backbone must first be trained on ImageNet (Russakovsky et al., 2015) class prediction before being integrated into a multimodal linguistic pipeline. We show that this is not necessary and introduce a new model Vision-Language from Captions (VLC) built on top of Masked Auto-Encoders (He et al., 2022) that does not require this supervision. We seek to provide general advice on multimodal pretraining by examining the roles of (a) unimodal initialization, (b) unimodal architectural components and (c) data annotation in the pretraining corpus. Our extensive and carefully controlled studies suggest that none of the above factors is absolutely important in achieving versatile vision-language representations. We conclude our analysis with suggestions on the choices of initialization, architectural components, and annotation formats targeting a better balance between data efficiency and representation quality.",
            "url": "https://openreview.net/pdf?id=xLnbSpozWS",
            "publicationVenue": "Transactions on Machine Learning Research",
            "citation_count": 8
        }
    ],
    "Ralf Brown": [],
    "Jamie Callan": [],
    "Justine Cassell": [
        {
            "title": "When to generate hedges in peer-tutoring interactions",
            "authors": "Alafate Abulimiti, Chloe Clavel, Justine Cassell",
            "year": "2023",
            "abstract": "This paper explores the application of machine learning techniques to predict where hedging occurs in peer-tutoring interactions. The study uses a naturalistic face-to-face dataset annotated for natural language turns, conversational strategies, tutoring strategies, and nonverbal behaviours. These elements are processed into a vector representation of the previous turns, which serves as input to several machine learning models. Results show that embedding layers, that capture the semantic information of the previous turns, significantly improves the model's performance. Additionally, the study provides insights into the importance of various features, such as interpersonal rapport and nonverbal behaviours, in predicting hedges by using Shapley values for feature explanation. We discover that the eye gaze of both the tutor and the tutee has a significant impact on hedge prediction. We further validate this observation through a follow-up ablation study.",
            "url": "https://arxiv.org/pdf/2307.15582",
            "publicationVenue": "arXiv preprint arXiv:2307.15582",
            "citation_count": 0
        },
        {
            "title": "How About Kind of Generating Hedges using End-to-End Neural Models?",
            "authors": "Alafate Abulimiti, Chloe Clavel, Justine Cassell",
            "year": "2023",
            "abstract": "Hedging is a strategy for softening the impact of a statement in conversation. In reducing the strength of an expression, it may help to avoid embarrassment (more technically, ``face threat'') to one's listener. For this reason, it is often found in contexts of instruction, such as tutoring. In this work, we develop a model of hedge generation based on i) fine-tuning state-of-the-art language models trained on human-human tutoring data, followed by ii) reranking to select the candidate that best matches the expected hedging strategy within a candidate pool using a hedge classifier. We apply this method to a natural peer-tutoring corpus containing a significant number of disfluencies, repetitions, and repairs. The results show that generation in this noisy environment is feasible with reranking. By conducting an error analysis for both approaches, we reveal the challenges faced by systems attempting to accomplish both social and task-oriented goals in conversation.",
            "url": "https://arxiv.org/pdf/2306.14696",
            "publicationVenue": "arXiv preprint arXiv:2306.14696",
            "citation_count": 1
        },
        {
            "title": "\" You might think about slightly revising the title\": Identifying Hedges in Peer-tutoring Interactions",
            "authors": "Yann Raphalen, Chloe Clavel, Justine Cassell",
            "year": "2023",
            "abstract": "Hedges play an important role in the management of conversational interaction. In peer tutoring, they are notably used by tutors in dyads (pairs of interlocutors) experiencing low rapport to tone down the impact of instructions and negative feedback. Pursuing the objective of building a tutoring agent that manages rapport with students in order to improve learning, we used a multimodal peer-tutoring dataset to construct a computational framework for identifying hedges. We compared approaches relying on pre-trained resources with others that integrate insights from the social science literature. Our best performance involved a hybrid approach that outperforms the existing baseline while being easier to interpret. We employ a model explainability tool to explore the features that characterize hedges in peer-tutoring conversations, and we identify some novel features, and the benefits of such a hybrid model approach.",
            "url": "https://arxiv.org/pdf/2306.14911",
            "publicationVenue": "arXiv preprint arXiv:2306.14911",
            "citation_count": 9
        },
        {
            "title": "Literacy learning by storytelling with a virtual peer",
            "authors": "Kimiko Ryokai, Catherine Vaucelle, Justine Cassell",
            "year": "2023",
            "abstract": "In this paper, we present Sam, an embodied conversational storyteller who tells stories interactively with children. Sam was designed to appear as a peer to preschool children, but to tell stories in a developmentally advanced way in order to model narrative skills important for literacy. Literacy learning - learning how to read and write, begins long before children enter school. One of the key skills to reading and writing is the ability to represent thoughts symbolically and share them in language with an audience who does not share the same background. Children learn and practice such important language skills in the informal setting of everyday storytelling with their peers and adults available around them. In particular, storytelling in a context of peer collaboration provides a perfect place where children not only learn language skills important for literacy, but also learn to be critical listeners of others' stories\u00a0\u2026",
            "url": "https://repository.isls.org/bitstream/1/3797/1/352-360.pdf",
            "publicationVenue": "Computer Support for Collaborative Learning",
            "citation_count": 74
        },
        {
            "title": "Beyond single\u2010mindedness: A figure\u2010ground reversal for the cognitive sciences",
            "authors": "Mark Dingemanse, Andreas Liesenfeld, Marlou Rasenberg, Saul Albert, Felix K Ameka, Abeba Birhane, Dimitris Bolis, Justine Cassell, Rebecca Clift, Elena Cuffari, Hanne De Jaegher, Catarina Dutilh Novaes, NJ Enfield, Riccardo Fusaroli, Eleni Gregoromichelaki, Edwin Hutchins, Ivana Konvalinka, Damian Milton, Joanna R\u0105czaszek\u2010Leonardi, Vasudevi Reddy, Federico Rossano, David Schlangen, Johanna Seibt, Elizabeth Stokoe, Lucy Suchman, Cordula Vesper, Thalia Wheatley, Martina Wiltschko",
            "year": "2023",
            "abstract": "A fundamental fact about human minds is that they are never truly alone: all minds are steeped in situated interaction. That social interaction matters is recognized by any experimentalist who seeks to exclude its influence by studying individuals in isolation. On this view, interaction complicates cognition. Here, we explore the more radical stance that interaction co\u2010constitutes cognition: that we benefit from looking beyond single minds toward cognition as a process involving interacting minds. All around the cognitive sciences, there are approaches that put interaction center stage. Their diverse and pluralistic origins may obscure the fact that collectively, they harbor insights and methods that can respecify foundational assumptions and fuel novel interdisciplinary work. What might the cognitive sciences gain from stronger interactional foundations? This represents, we believe, one of the key questions for the future\u00a0\u2026",
            "url": "https://onlinelibrary.wiley.com/doi/pdf/10.1111/cogs.13230",
            "publicationVenue": "Cognitive science",
            "citation_count": 25
        }
    ],
    "Mona Diab": [
        {
            "title": "Proceedings of the 6th Workshop on Computational Approaches to Linguistic Code-Switching",
            "authors": "Genta Winata, Sudipta Kar, Marina Zhukova, Thamar Solorio, Mona Diab, Sunayana Sitaram, Monojit Choudhury, Kalika Bali",
            "year": "2023",
            "abstract": "Bienvenidos to the proceedings of the sixth edition of the workshop on computational approaches for linguistic code-switching (CALCS-2023)! Code-switching is a common phenomenon in the multilingual communities where multilingual speakers communicate by moving back and forth between the languages they speak when communicating with other multilingual speakers. This year the workshop is being held in Singapore on December 7th, 2023 at EMNLP.This workshop series brings together experts and practitioners that are currently working on different aspects of code-switching with a special focus on motivating tighter collaborations between speech and text researchers. We received 15 regular workshop submissions, of which we accepted 8 and 1 nonarchival. Our workshop also aims to motivate new research and energize the community to take on the challenges posed by code-switching data.",
            "url": "https://aclanthology.org/2023.calcs-1.0.pdf",
            "publicationVenue": "Proceedings of the 6th Workshop on Computational Approaches to Linguistic Code-Switching",
            "citation_count": 0
        },
        {
            "title": "Care4Lang at MEDIQA-Chat 2023: Fine-tuning Language Models for Classifying and Summarizing Clinical Dialogues",
            "authors": "Amal Alqahtani, Rana Salama, Mona Diab, Abdou Youssef",
            "year": "2023",
            "abstract": "Summarizing medical conversations is one of the tasks proposed by MEDIQA-Chat to promote research on automatic clinical note generation from doctor-patient conversations. In this paper, we present our submission to this task using fine-tuned language models, including T5, BART and BioGPT models. The fine-tuned models are evaluated using ensemble metrics including ROUGE, BERTScore andBLEURT. Among the fine-tuned models, Flan-T5 achieved the highest aggregated score for dialogue summarization.",
            "url": "https://aclanthology.org/2023.clinicalnlp-1.55.pdf",
            "publicationVenue": "Proceedings of the 5th Clinical Natural Language Processing Workshop",
            "citation_count": 1
        },
        {
            "title": "Evaluating multilingual speech translation under realistic conditions with resegmentation and terminology",
            "authors": "Elizabeth Salesky, Kareem Darwish, Mohamed Al-Badrashiny, Mona Diab, Jan Niehues",
            "year": "2023",
            "abstract": "We present the ACL 60/60 evaluation sets for multilingual translation of ACL 2022 technical presentations into 10 target languages. This dataset enables further research into multilingual speech translation under realistic recording conditions with unsegmented audio and domain-specific terminology, applying NLP tools to text and speech in the technical domain, and evaluating and improving model robustness to diverse speaker demographics.",
            "url": "https://aclanthology.org/2023.iwslt-1.2.pdf",
            "publicationVenue": "Proceedings of the 20th International Conference on Spoken Language Translation (IWSLT 2023)",
            "citation_count": 3
        },
        {
            "title": "ALERT: Adapt language models to reasoning tasks",
            "authors": "Ping Yu, Tianlu Wang, Olga Golovneva, Badr AlKhamissi, Siddharth Verma, Zhijing Jin, Gargi Ghosh, Mona Diab, Asli Celikyilmaz",
            "year": "2023",
            "abstract": "Recent advancements in large language models have enabled them to perform well on complex tasks that require step-by-step reasoning with few-shot learning. However, it is unclear whether these models are applying reasoning skills they have learnt during pre-training, or if they are simply memorizing their training corpus at finer granularity and have learnt to better understand their context. To address this question, we introduce {pasted macro \u2018OUR\u2019} model, a benchmark and suite of analyses for evaluating reasoning skills of language models.{pasted macro \u2018OUR\u2019} model enables comparing pre-trained and finetuned models on complex tasks that require reasoning skills to solve. Our benchmark provides a test bed to asses any language model on fine-grained reasoning skills, which spans over 20 datasets and covers 10 different reasoning skills. By using {pasted macro \u2018OUR\u2019} model we further investigate the role of finetuning. Our extensive empirical analysis shows that language models learn more reasoning skills such as textual entailment, abductive reasoning, and analogical reasoning during the finetuning stage compared to pretraining stage. However, we also find that when language models are finetuned they tend to overfit to the prompt template, which hurts the robustness of models causing generalization problems.",
            "url": "https://aclanthology.org/2023.acl-long.60.pdf",
            "publicationVenue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
            "citation_count": 2
        },
        {
            "title": "Arabic natural language processing for Qur\u2019anic research: A systematic review",
            "authors": "Muhammad Huzaifa Bashir, Aqil M Azmi, Haq Nawaz, Wajdi Zaghouani, Mona Diab, Ala Al-Fuqaha, Junaid Qadir",
            "year": "2023",
            "abstract": "The Qur\u2019an is a fourteen centuries old divine book in Arabic language that is read and followed by almost two billion Muslims globally as their sacred religious text. With the rise of Islam, the Arabic language gained popularity and became the lingua franca for large swaths of the old world. Devout Muslims read the Qur\u2019an daily seeking guidance and comfort. Though the Qur\u2019an, as a text, is short, there is a huge volume of supporting work filling tens of thousands of volumes, e.g., commentaries, exegesis, etc. Recently, there has been a renewed interest in such religious texts by non-specialists. Many of which were fueled by the recent advances in computational and natural language processing (NLP) techniques. These techniques help the development of tools that benefit common people to gain knowledge easily. This paper surveys the different efforts in the field of Qur\u2019anic NLP, serving as a synthesized\u00a0\u2026",
            "url": "https://link.springer.com/article/10.1007/s10462-022-10313-2",
            "publicationVenue": "Artificial Intelligence Review",
            "citation_count": 19
        },
        {
            "title": "Can Large Language Models Infer Causation from Correlation?",
            "authors": "Zhijing Jin, Jiarui Liu, Zhiheng Lyu, Spencer Poff, Mrinmaya Sachan, Rada Mihalcea, Mona Diab, Bernhard Sch\u00f6lkopf",
            "year": "2023",
            "abstract": "Causal inference is one of the hallmarks of human intelligence. While the field of CausalNLP has attracted much interest in the recent years, existing causal inference datasets in NLP primarily rely on discovering causality from empirical knowledge (e.g., commonsense knowledge). In this work, we propose the first benchmark dataset to test the pure causal inference skills of large language models (LLMs). Specifically, we formulate a novel task Corr2Cause, which takes a set of correlational statements and determines the causal relationship between the variables. We curate a large-scale dataset of more than 400K samples, on which we evaluate seventeen existing LLMs. Through our experiments, we identify a key shortcoming of LLMs in terms of their causal inference skills, and show that these models achieve almost close to random performance on the task. This shortcoming is somewhat mitigated when we try to re-purpose LLMs for this skill via finetuning, but we find that these models still fail to generalize -- they can only perform causal inference in in-distribution settings when variable names and textual expressions used in the queries are similar to those in the training set, but fail in out-of-distribution settings generated by perturbing these queries. Corr2Cause is a challenging task for LLMs, and would be helpful in guiding future research on improving LLMs' pure reasoning skills and generalizability. Our data is at https://huggingface.co/datasets/causalnlp/corr2cause. Our code is at https://github.com/causalNLP/corr2cause.",
            "url": "https://arxiv.org/pdf/2306.05836.pdf?trk=public_post_comment-text",
            "publicationVenue": "arXiv preprint arXiv:2306.05836",
            "citation_count": 28
        },
        {
            "title": "OPT-R: Exploring the Role of Explanations in Finetuning and Prompting for Reasoning Skills of Large Language Models",
            "authors": "Badr AlKhamissi, Siddharth Verma, Ping Yu, Zhijing Jin, Asli Celikyilmaz, Mona Diab",
            "year": "2023",
            "abstract": "In this paper, we conduct a thorough investigation into the reasoning capabilities of Large Language Models (LLMs), focusing specifically on the Open Pretrained Transformers (OPT) models as a representative of such models. Our study entails finetuning three different sizes of OPT on a carefully curated reasoning corpus, resulting in two sets of finetuned models: OPT-R, finetuned without explanations, and OPT-RE, finetuned with explanations. We then evaluate all models on 57 out-of-domain tasks drawn from the SUPER-NATURALINSTRUCTIONS benchmark, covering 26 distinct reasoning skills, utilizing three prompting techniques. Through a comprehensive grid of 27 configurations and 6,156 test evaluations, we investigate the dimensions of finetuning, prompting, and scale to understand the role of explanations on different reasoning skills. Our findings reveal that having explanations in the fewshot exemplar has no significant impact on the model's performance when the model is finetuned, while positively affecting the non-finetuned counterpart. Moreover, we observe a slight yet consistent increase in classification accuracy as we incorporate explanations during prompting and finetuning, respectively. Finally, we offer insights on which skills benefit the most from incorporating explanations during finetuning and prompting, such as Numerical (+20.4%) and Analogical (+13.9%) reasoning, as well as skills that exhibit negligible or negative effects.",
            "url": "https://arxiv.org/pdf/2305.12001",
            "publicationVenue": "arXiv preprint arXiv:2305.12001",
            "citation_count": 4
        },
        {
            "title": "Methods for measuring, updating, and visualizing factual beliefs in language models",
            "authors": "Peter Hase, Mona Diab, Asli Celikyilmaz, Xian Li, Zornitsa Kozareva, Veselin Stoyanov, Mohit Bansal, Srinivasan Iyer",
            "year": "2023",
            "abstract": "Language models can memorize a considerable amount of factual information during pretraining that can be elicited through prompting or finetuning models on tasks like question answering. In this paper, we discuss approaches to measuring model factual beliefs, updating incorrect factual beliefs in models, and visualizing graphical relationships between factual beliefs. Our main contributions include:(1) new metrics for evaluating belief-updating methods focusing on the logical consistency of beliefs,(2) a training objective for Sequential, Local, and Generalizing updates (SLAG) that improves the performance of existing hypernetwork approaches, and (3) the introduction of the belief graph, a new form of visualization for language models that shows relationships between stored model beliefs. Our experiments suggest that models show only limited consistency between factual beliefs, but update methods can both fix incorrect model beliefs and greatly improve their consistency. Although off-the-shelf optimizers are surprisingly strong belief-updating baselines, our learned optimizers can outperform them in more difficult settings than have been considered in past work.",
            "url": "https://aclanthology.org/2023.eacl-main.199.pdf",
            "publicationVenue": "Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics",
            "citation_count": 23
        },
        {
            "title": "Contextual natural language understanding for conversational agents",
            "authors": null,
            "year": "2023",
            "abstract": "Techniques are described for a contextual natural language understanding (cNLU) framework that is able to incorporate contextual signals of variable history length to perform joint intent classification (IC) and slot labeling (SL) tasks. A user utterance provided by a user within a multi-turn chat dialog between the user and a conversational agent is received. The user utterance and contextual information associated with one or more previous turns of the multi-turn chat dialog is provided to a machine learning (ML) model. An intent classification and one or more slot labels for the user utterance are then obtained from the ML model. The cNLU framework described herein thus uses, in addition to a current utterance itself, various contextual signals as input to a model to generate IC and SL predictions for each utterance of a multi-turn chat dialog.",
            "url": null,
            "publicationVenue": null,
            "citation_count": 2
        }
    ],
    "Fernando Diaz": [
        {
            "title": "AI Consent Futures: A Case Study on Voice Data Collection with Clinicians",
            "authors": "Lauren Wilcox, Robin Brewer, Fernando Diaz",
            "year": "2023",
            "abstract": "As new forms of data capture emerge to power new AI applications, questions abound about the ethical implications of these data collection practices. In this paper, we present clinicians' perspectives on the prospective benefits and harms of voice data collection during health consultations. Such data collection is being proposed as a means to power models to assist clinicians with medical data entry, administrative tasks, and consultation analysis. Yet, clinicians' attitudes and concerns are largely absent from the AI narratives surrounding these use cases, and the academic literature investigating them. Our qualitative interview study used the concept of an informed consent process as a type of design fiction, to support elicitation of clinicians' perspectives on voice data collection and use associated with a fictional, near-term AI assistant. Through reflexive thematic analysis of in-depth sessions with physicians, we\u00a0\u2026",
            "url": "https://dl.acm.org/doi/pdf/10.1145/3610107",
            "publicationVenue": "Proceedings of the ACM on Human-Computer Interaction",
            "citation_count": 3
        },
        {
            "title": "Group Membership Bias",
            "authors": "Ali Vardasbi, Maarten de Rijke, Fernando Diaz, Mostafa Dehghani",
            "year": "2023",
            "abstract": "When learning to rank from user interactions, search and recommendation systems must address biases in user behavior to provide a high-quality ranking. One type of bias that has recently been studied in the ranking literature is when sensitive attributes, such as gender, have an impact on a user's judgment about an item's utility. For example, in a search for an expertise area, some users may be biased towards clicking on male candidates over female candidates. We call this type of bias group membership bias or group bias for short. Increasingly, we seek rankings that not only have high utility but are also fair to individuals and sensitive groups. Merit-based fairness measures rely on the estimated merit or utility of the items. With group bias, the utility of the sensitive groups is under-estimated, hence, without correcting for this bias, a supposedly fair ranking is not truly fair. In this paper, first, we analyze the impact of group bias on ranking quality as well as two well-known merit-based fairness metrics and show that group bias can hurt both ranking and fairness. Then, we provide a correction method for group bias that is based on the assumption that the utility score of items in different groups comes from the same distribution. This assumption has two potential issues of sparsity and equality-instead-of-equity, which we use an amortized approach to solve. We show that our correction method can consistently compensate for the negative impact of group bias on ranking quality and fairness metrics.",
            "url": "https://arxiv.org/pdf/2308.02887",
            "publicationVenue": "arXiv preprint arXiv:2308.02887",
            "citation_count": 0
        },
        {
            "title": "Distributionally-informed recommender system evaluation",
            "authors": "Michael D Ekstrand, Ben Carterette, Fernando Diaz",
            "year": "2023",
            "abstract": "Current practice for evaluating recommender systems typically focuses on point estimates of user-oriented effectiveness metrics or business metrics, sometimes combined with additional metrics for considerations such as diversity and novelty. In this paper, we argue for the need for researchers and practitioners to attend more closely to variousdistributionsthat arise from a recommender system (or other information access system) and the sources of uncertainty that lead to these distributions. One immediate implication of our argument is that both researchers and practitioners must report and examine more thoroughly the distribution of utility between and within different stakeholder groups. However, distributions of various forms arise in many more aspects of the recommender systems experimental process, and distributional thinking has substantial ramifications for how we design, evaluate, and present\u00a0\u2026",
            "url": "https://dl.acm.org/doi/pdf/10.1145/3613455",
            "publicationVenue": "ACM Transactions on Recommender Systems",
            "citation_count": 5
        },
        {
            "title": "SIGIR 2023 Workshop on Retrieval Enhanced Machine Learning (REML@ SIGIR 2023)",
            "authors": "Michael Bendersky, Danqi Chen, Fernando Diaz, Hamed Zamani",
            "year": "2023",
            "abstract": "Most machine learning models are designed to be self-contained and encode both \"knowledge\" and \"reasoning\" in their parameters. However, such models cannot perform effectively for tasks that require knowledge grounding and tasks that deal with non-stationary data, such as news and social media. Besides, these models often require huge number of parameters to encode all the required knowledge. These issues can be addressed via augmentation with a retrieval model. This category of machine learning models, which is called Retrieval-enhanced machine learning (REML), has recently attracted considerable attention in multiple research communities. For instance, REML models have been studied in the context of open-domain question answering, fact verification, and dialogue systems and also in the context of generalization through memorization in language models and memory networks. We believe\u00a0\u2026",
            "url": "https://scholar.archive.org/work/kllosz5nfrfonipbjeopb4n42y/access/wayback/https://dl.acm.org/doi/pdf/10.1145/3539618.3591925",
            "publicationVenue": "Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval",
            "citation_count": 0
        },
        {
            "title": "Scaling Laws Do Not Scale",
            "authors": "Fernando Diaz, Michael Madaio",
            "year": "2023",
            "abstract": "Recent work has proposed a power law relationship, referred to as ``scaling laws,'' between the performance of artificial intelligence (AI) models and aspects of those models' design (e.g., dataset size). In other words, as the size of a dataset (or model parameters, etc) increases, the performance of a given model trained on that dataset will correspondingly increase. However, while compelling in the aggregate, this scaling law relationship overlooks the ways that metrics used to measure performance may be precarious and contested, or may not correspond with how different groups of people may perceive the quality of models' output. In this paper, we argue that as the size of datasets used to train large AI models grows, the number of distinct communities (including demographic groups) whose data is included in a given dataset is likely to grow, each of whom may have different values. As a result, there is an increased risk that communities represented in a dataset may have values or preferences not captured by (or in the worst case, at odds with) the metrics used to evaluate model performance for scaling laws. We end the paper with implications for AI scaling laws -- that models may not, in fact, continue to improve as the datasets get larger -- at least not for all people or communities impacted by those models.",
            "url": "https://arxiv.org/pdf/2307.03201",
            "publicationVenue": "arXiv preprint arXiv:2307.03201",
            "citation_count": 2
        },
        {
            "title": "Best-Case Retrieval Evaluation: Improving the Sensitivity of Reciprocal Rank with Lexicographic Precision",
            "authors": "Fernando Diaz",
            "year": "2023",
            "abstract": "Across a variety of ranking tasks, researchers use reciprocal rank to measure the effectiveness for users interested in exactly one relevant item. Despite its widespread use, evidence suggests that reciprocal rank is brittle when discriminating between systems. This brittleness, in turn, is compounded in modern evaluation settings where current, high-precision systems may be difficult to distinguish. We address the lack of sensitivity of reciprocal rank by introducing and connecting it to the concept of best-case retrieval, an evaluation method focusing on assessing the quality of a ranking for the most satisfied possible user across possible recall requirements. This perspective allows us to generalize reciprocal rank and define a new preference-based evaluation we call lexicographic precision or lexiprecision. By mathematical construction, we ensure that lexiprecision preserves differences detected by reciprocal rank, while empirically improving sensitivity and robustness across a broad set of retrieval and recommendation tasks.",
            "url": "https://arxiv.org/pdf/2306.07908",
            "publicationVenue": "arXiv preprint arXiv:2306.07908",
            "citation_count": 2
        },
        {
            "title": "Preference-Based Offline Evaluation",
            "authors": "Charles LA Clarke, Fernando Diaz, Negar Arabzadeh",
            "year": "2023",
            "abstract": "A core step in production model research and development involves the offline evaluation of a system before production deployment. Traditional offline evaluation of search, recommender, and other systems involves gathering item relevance labels from human editors. These labels can then be used to assess system performance using offline evaluation metrics. Unfortunately, this approach does not work when evaluating highly effective ranking systems, such as those emerging from the advances in machine learning. Recent work demonstrates that moving away from pointwise item and metric evaluation can be a more effective approach to the offline evaluation of systems. This tutorial, intended for both researchers and practitioners, reviews early work in preference-based evaluation and covers recent developments in detail.",
            "url": null,
            "publicationVenue": "Proceedings of the Sixteenth ACM International Conference on Web Search and Data Mining",
            "citation_count": 3
        },
        {
            "title": "Commonality in Recommender Systems: Evaluating Recommender Systems to Enhance Cultural Citizenship",
            "authors": "Andres Ferraro, Gustavo Ferreira, Fernando Diaz, Georgina Born",
            "year": "2023",
            "abstract": "Recommender systems have become the dominant means of curating cultural content, significantly influencing individual cultural experience. Since recommender systems tend to optimize for personalized user experience, they can overlook impacts on cultural experience in the aggregate. After demonstrating that existing metrics do not center culture, we introduce a new metric, commonality, that measures the degree to which recommendations familiarize a given user population with specified categories of cultural content. We developed commonality through an interdisciplinary dialogue between researchers in computer science and the social sciences and humanities. With reference to principles underpinning public service media systems in democratic societies, we identify universality of address and content diversity in the service of strengthening cultural citizenship as particularly relevant goals for recommender systems delivering cultural content. We develop commonality as a measure of recommender system alignment with the promotion of content toward a shared cultural experience across a population of users. We empirically compare the performance of recommendation algorithms using commonality with existing metrics, demonstrating that commonality captures a novel property of system behavior complementary to existing metrics. Alongside existing fairness and diversity metrics, commonality contributes to a growing body of scholarship developing `public good' rationales for machine learning systems.",
            "url": "https://arxiv.org/pdf/2302.11360",
            "publicationVenue": "arXiv preprint arXiv:2302.11360",
            "citation_count": 1
        },
        {
            "title": "Recall, Robustness, and Lexicographic Evaluation",
            "authors": "Fernando Diaz, Bhaskar Mitra",
            "year": "2023",
            "abstract": null,
            "url": null,
            "publicationVenue": "arXiv preprint arXiv:2302.11370",
            "citation_count": 0
        }
    ],
    "Scott E. Fahlman": [
        {
            "title": "Abstracts of Invited Talks Given at BICA* AI 2023",
            "authors": "Tingting Liu, Alexei V Samsonovich, Peter Boltuc, Scott E Fahlman, Sophie Hendrikse, Jan Treur, John Laird, Antonio Lieto, Paul Robertson, Ron Sun, Junichi Takeno",
            "year": "2023",
            "abstract": "This chapter comprises selected short and extended abstracts of invited talks and discussion panels that took place at the 2023 Annual International Conference on Brain-Inspired Cognitive Architectures for Artificial Intelligence, also known as the 14th Annual Meeting of the BICA Society (BICA*AI 2023), held in Ningbo, China during October 13\u201315, 2023. Abstracts included here were not accompanied by papers in this volume. The abstracts are arranged alphabetically by the first author\u2019s last name, as follows: (1) Boltuc, (2) Fahlman, (3) Hendrikse and Treur, (4) Laird, (5) Lieto, (6) Liu, (7) Robertson, (8) Samsonovich, (9) Sun, (10) Takeno. Section authors are listed again beneath the header of each section.",
            "url": null,
            "publicationVenue": "Biologically Inspired Cognitive Architectures Meeting",
            "citation_count": 0
        },
        {
            "title": "Score: A Rule Engine for the Scone Knowledge Base System",
            "authors": "Jeffrey Chen, Scott E Fahlman",
            "year": "2023",
            "abstract": "We present Score, a rule engine designed and implemented for the Scone knowledge base system. Scone is a knowledge base system designed for storing and manipulating rich representations of general knowledge in symbolic form. It represents knowledge in the form of nodes and links in a network structure, and it can perform basic inference about the relationships between different elements efficiently. On its own, Scone acts as a sort of \"smart memory\" that can interface with other software systems. One area of improvement for Scone is how useful it can be in supplying knowledge to an intelligent agent that can use the knowledge to perform actions and update the knowledge base with its observations. We augment the Scone system with a production rule engine that automatically performs simple inference based on existing and newly-added structures in Scone's knowledge base, potentially improving the capabilities of any planning systems built on top of Scone. Production rule systems consist of \"if-then\" production rules that try to match their predicates to existing knowledge and fire their actions when their predicates are satisfied. We propose two kinds of production rules, if-added and if-needed rules, that differ in how they are checked and fired to cover multiple use cases. We then implement methods to efficiently check and fire these rules in a large knowledge base. The new rule engine is not meant to be a complex stand-alone planner, so we discuss how it fits into the context of Scone and future work on planning systems.",
            "url": "https://arxiv.org/pdf/2305.04154",
            "publicationVenue": "arXiv preprint arXiv:2305.04154",
            "citation_count": 0
        }
    ],
    "Robert E Frederking": [],
    "Daniel Fried": [
        {
            "title": "AutoReply: Detecting Nonsense in Dialogue with Discriminative Replies",
            "authors": "Weiyan Shi, Emily Dinan, Adi Renduchintala, Daniel Fried, Athul Jacob, Zhou Yu, Mike Lewis",
            "year": "2023",
            "abstract": "We show that dialogue models can detect errors in their own messages, by calculating the likelihood of replies that are indicative of poor messages. For example, if an agent believes its partner is likely to respond \u201cI don\u2019t understand\u201d to a candidate message, that message may not make sense, so an alternative message should be chosen. We evaluate our approach on a dataset from the game Diplomacy, which contains long dialogues richly grounded in the game state, on which existing models make many errors. We first show that hand-crafted replies can be effective for the task of detecting nonsense in applications as complex as Diplomacy. We then design AutoReply, an algorithm to search for such discriminative replies automatically, given a small number of annotated dialogue examples. We find that AutoReply-generated replies outperform handcrafted replies and perform on par with supervised learning approaches.",
            "url": "https://aclanthology.org/2023.findings-emnlp.23.pdf",
            "publicationVenue": "Findings of the Association for Computational Linguistics: EMNLP 2023",
            "citation_count": 0
        },
        {
            "title": "Pragmatics in language grounding: Phenomena, tasks, and modeling approaches",
            "authors": "Daniel Fried, Nicholas Tomlin, Jennifer Hu, Roma Patel, Aida Nematzadeh",
            "year": "2023",
            "abstract": "People rely heavily on context to enrich meaning beyond what is literally said, enabling concise but effective communication. To interact successfully and naturally with people, user-facing artificial intelligence systems will require similar skills in pragmatics: relying on various types of context --- from shared linguistic goals and conventions, to the visual and embodied world --- to use language effectively.  We survey existing grounded settings and pragmatic modeling approaches and analyze how the task goals, environmental contexts, and communicative affordances in each work enrich linguistic meaning. We present recommendations for future grounded task design to naturally elicit pragmatic phenomena, and suggest directions that focus on a broader range of communicative contexts and affordances.",
            "url": "https://openreview.net/pdf?id=Cx5vVkpsOY",
            "publicationVenue": "The 2023 Conference on Empirical Methods in Natural Language Processing",
            "citation_count": 12
        },
        {
            "title": "Asking More Informative Questions for Grounded Retrieval",
            "authors": "Sedrick Keh, Justin T Chiu, Daniel Fried",
            "year": "2023",
            "abstract": "When a model is trying to gather information in an interactive setting, it benefits from asking informative questions. However, in the case of a grounded multi-turn image identification task, previous studies have been constrained to polar yes/no questions, limiting how much information the model can gain in a single turn. We present an approach that formulates more informative, open-ended questions. In doing so, we discover that off-the-shelf visual question answering (VQA) models often make presupposition errors, which standard information gain question selection methods fail to account for. To address this issue, we propose a method that can incorporate presupposition handling into both question selection and belief updates. Specifically, we use a two-stage process, where the model first filters out images which are irrelevant to a given question, then updates its beliefs about which image the user intends. Through self-play and human evaluations, we show that our method is successful in asking informative open-ended questions, increasing accuracy over the past state-of-the-art by 14%, while resulting in 48% more efficient games in human evaluations.",
            "url": "https://arxiv.org/pdf/2311.08584",
            "publicationVenue": "arXiv preprint arXiv:2311.08584",
            "citation_count": 0
        },
        {
            "title": "Generating Pragmatic Examples to Train Neural Program Synthesizers",
            "authors": "Saujas Vaduguru, Daniel Fried, Yewen Pu",
            "year": "2023",
            "abstract": "Programming-by-example is the task of synthesizing a program that is consistent with a set of user-provided input-output examples. As examples are often an under-specification of one's intent, a good synthesizer must choose the intended program from the many that are consistent with the given set of examples. Prior work frames program synthesis as a cooperative game between a listener (that synthesizes programs) and a speaker (a user choosing examples), and shows that models of computational pragmatic inference are effective in choosing the user intended programs. However, these models require counterfactual reasoning over a large set of programs and examples, which is infeasible in realistic program spaces. In this paper, we propose a novel way to amortize this search with neural networks. We sample pairs of programs and examples via self-play between listener and speaker models, and use pragmatic inference to choose informative training examples from this sample.We then use the informative dataset to train models to improve the synthesizer's ability to disambiguate user-provided examples without human supervision. We validate our method on the challenging task of synthesizing regular expressions from example strings, and find that our method (1) outperforms models trained without choosing pragmatic examples by 23% (a 51% relative increase) (2) matches the performance of supervised learning on a dataset of pragmatic examples provided by humans, despite using no human data in training.",
            "url": "https://arxiv.org/pdf/2311.05740",
            "publicationVenue": "arXiv preprint arXiv:2311.05740",
            "citation_count": 0
        },
        {
            "title": "Comparative Knowledge Distillation",
            "authors": "Alex Wilf, Alex Tianyi Xu, Paul Pu Liang, Alexander Obolenskiy, Daniel Fried, Louis-Philippe Morency",
            "year": "2023",
            "abstract": "In the era of large scale pretrained models, Knowledge Distillation (KD) serves an important role in transferring the wisdom of computationally heavy teacher models to lightweight, efficient student models while preserving performance. Traditional KD paradigms, however, assume readily available access to teacher models for frequent inference -- a notion increasingly at odds with the realities of costly, often proprietary, large scale models. Addressing this gap, our paper considers how to minimize the dependency on teacher model inferences in KD in a setting we term Few Teacher Inference Knowledge Distillation (FTI KD). We observe that prevalent KD techniques and state of the art data augmentation strategies fall short in this constrained setting. Drawing inspiration from educational principles that emphasize learning through comparison, we propose Comparative Knowledge Distillation (CKD), which encourages student models to understand the nuanced differences in a teacher model's interpretations of samples. Critically, CKD provides additional learning signals to the student without making additional teacher calls. We also extend the principle of CKD to groups of samples, enabling even more efficient learning from limited teacher calls. Empirical evaluation across varied experimental settings indicates that CKD consistently outperforms state of the art data augmentation and KD techniques.",
            "url": "https://arxiv.org/pdf/2311.02253",
            "publicationVenue": "arXiv preprint arXiv:2311.02253",
            "citation_count": 0
        },
        {
            "title": "Data Augmentation for Code Translation with Comparable Corpora and Multiple References",
            "authors": "Yiqing Xie, Atharva Naik, Daniel Fried, Carolyn Rose",
            "year": "2023",
            "abstract": "One major challenge of translating code between programming languages is that parallel training data is often limited. To overcome this challenge, we present two data augmentation techniques, one that builds comparable corpora (i.e., code pairs with similar functionality), and another that augments existing parallel data with multiple reference translations. Specifically, we build and analyze multiple types of comparable corpora, including programs generated from natural language documentation using a code generation model. Furthermore, to reduce overfitting to a single reference translation, we automatically generate additional translation references for available parallel data and filter the translations by unit tests, which increases variation in target translations. Experiments show that our data augmentation techniques significantly improve CodeT5 for translation between Java, Python, and C++ by an average of 7.5% Computational Accuracy (CA@1), which verifies the correctness of translations by execution. The code is available at https://github.com/Veronicium/CMTrans.",
            "url": "https://arxiv.org/pdf/2311.00317",
            "publicationVenue": "arXiv preprint arXiv:2311.00317",
            "citation_count": 1
        },
        {
            "title": "Symbolic Planning and Code Generation for Grounded Dialogue",
            "authors": "Justin T Chiu, Wenting Zhao, Derek Chen, Saujas Vaduguru, Alexander M Rush, Daniel Fried",
            "year": "2023",
            "abstract": "Large language models (LLMs) excel at processing and generating both text and code. However, LLMs have had limited applicability in grounded task-oriented dialogue as they are difficult to steer toward task objectives and fail to handle novel grounding. We present a modular and interpretable grounded dialogue system that addresses these shortcomings by composing LLMs with a symbolic planner and grounded code execution. Our system consists of a reader and planner: the reader leverages an LLM to convert partner utterances into executable code, calling functions that perform grounding. The translated code's output is stored to track dialogue state, while a symbolic planner determines the next appropriate response. We evaluate our system's performance on the demanding OneCommon dialogue task, involving collaborative reference resolution on abstract images of scattered dots. Our system substantially outperforms the previous state-of-the-art, including improving task success in human evaluations from 56% to 69% in the most challenging setting.",
            "url": "https://arxiv.org/pdf/2310.17140",
            "publicationVenue": "arXiv preprint arXiv:2310.17140",
            "citation_count": 0
        },
        {
            "title": "API-Assisted Code Generation for Question Answering on Varied Table Structures",
            "authors": "Yihan Cao, Shuyi Chen, Ryan Liu, Zhiruo Wang, Daniel Fried",
            "year": "2023",
            "abstract": "A persistent challenge to table question answering (TableQA) by generating executable programs has been adapting to varied table structures, typically requiring domain-specific logical forms. In response, this paper introduces a unified TableQA framework that: (1) provides a unified representation for structured tables as multi-index Pandas data frames, (2) uses Python as a powerful querying language, and (3) uses few-shot prompting to translate NL questions into Python programs, which are executable on Pandas data frames. Furthermore, to answer complex relational questions with extended program functionality and external knowledge, our framework allows customized APIs that Python programs can call. We experiment with four TableQA datasets that involve tables of different structures -- relational, multi-table, and hierarchical matrix shapes -- and achieve prominent improvements over past state-of-the-art systems. In ablation studies, we (1) show benefits from our multi-index representation and APIs over baselines that use only an LLM, and (2) demonstrate that our approach is modular and can incorporate additional APIs.",
            "url": "https://arxiv.org/pdf/2310.14687",
            "publicationVenue": "arXiv preprint arXiv:2310.14687",
            "citation_count": 2
        },
        {
            "title": "Sotopia: Interactive evaluation for social intelligence in language agents",
            "authors": "Xuhui Zhou, Hao Zhu, Leena Mathur, Ruohong Zhang, Haofei Yu, Zhengyang Qi, Louis-Philippe Morency, Yonatan Bisk, Daniel Fried, Graham Neubig, Maarten Sap",
            "year": "2023",
            "abstract": "Humans are social beings; we pursue social goals in our daily interactions, which is a crucial aspect of social intelligence. Yet, AI systems' abilities in this realm remain elusive. We present SOTOPIA, an open-ended environment to simulate complex social interactions between artificial agents and evaluate their social intelligence. In our environment, agents role-play and interact under a wide variety of scenarios; they coordinate, collaborate, exchange, and compete with each other to achieve complex social goals. We simulate the role-play interaction between LLM-based agents and humans within this task space and evaluate their performance with a holistic evaluation framework called SOTOPIA-Eval. With SOTOPIA, we find significant differences between these models in terms of their social intelligence, and we identify a subset of SOTOPIA scenarios, SOTOPIA-hard, that is generally challenging for all models. We find that on this subset, GPT-4 achieves a significantly lower goal completion rate than humans and struggles to exhibit social commonsense reasoning and strategic communication skills. These findings demonstrate SOTOPIA's promise as a general platform for research on evaluating and improving social intelligence in artificial agents.",
            "url": "https://arxiv.org/pdf/2310.11667",
            "publicationVenue": "arXiv preprint arXiv:2310.11667",
            "citation_count": 13
        },
        {
            "title": "Amortizing Pragmatic Program Synthesis with Rankings",
            "authors": "Yewen Pu, Saujas Vaduguru, Priyan Vaithilingam, Elena Glassman, Daniel Fried",
            "year": "2023",
            "abstract": "In program synthesis, an intelligent system takes in a set of user-generated examples and returns a program that is logically consistent with these examples. The usage of Rational Speech Acts (RSA) framework has been successful in building \\emph{pragmatic} program synthesizers that return programs which -- in addition to being logically consistent -- account for the fact that a user chooses their examples informatively. However, the computational burden of running the RSA algorithm has restricted the application of pragmatic program synthesis to domains with a small number of possible programs. This work presents a novel method of amortizing the RSA algorithm by leveraging a \\emph{global pragmatic ranking} -- a single, total ordering of all the hypotheses. We prove that for a pragmatic synthesizer that uses a single demonstration, our global ranking method exactly replicates RSA's ranked responses. We further empirically show that global rankings effectively approximate the full pragmatic synthesizer in an online, multi-demonstration setting. Experiments on two program synthesis domains using our pragmatic ranking method resulted in orders of magnitudes of speed ups compared to the RSA synthesizer, while outperforming the standard, non-pragmatic synthesizer.",
            "url": "https://arxiv.org/pdf/2309.03225",
            "publicationVenue": "arXiv preprint arXiv:2309.03225",
            "citation_count": 0
        },
        {
            "title": "Webarena: A realistic web environment for building autonomous agents",
            "authors": "Shuyan Zhou, Frank F Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Yonatan Bisk, Daniel Fried, Uri Alon, Graham Neubig",
            "year": "2023",
            "abstract": "With generative AI advances, the exciting potential for autonomous agents to manage daily tasks via natural language commands has emerged. However, cur rent agents are primarily created and tested in simplified synthetic environments, substantially limiting real-world scenario representation. In this paper, we build an environment for agent command and control that is highly realistic and reproducible. Specifically, we focus on agents that perform tasks on websites, and we create an environment with fully functional websites from four common domains: e-commerce, social forum discussions, collaborative software development, and content management. Our environment is enriched with tools (e.g., a map) and external knowledge bases (e.g., user manuals) to encourage human-like task-solving. Building upon our environment, we release a set of benchmark tasks focusing on evaluating the functional correctness of task completions. The tasks in our benchmark are diverse, long-horizon, and are designed to emulate tasks that humans routinely perform on the internet. We design and implement several autonomous agents, integrating recent techniques such as reasoning before acting. The results demonstrate that solving complex tasks is challenging: our best GPT-4-based agent only achieves an end-to-end task success rate of 10.59%. These results highlight the need for further development of robust agents, that current state-of-the-art LMs are far from perfect performance in these real-life tasks, and that WebArena can be used to measure such progress. Our code, data, environment reproduction resources, and video demonstrations are\u00a0\u2026",
            "url": "https://arxiv.org/pdf/2307.13854",
            "publicationVenue": "arXiv preprint arXiv:2307.13854",
            "citation_count": 59
        },
        {
            "title": "Coder reviewer reranking for code generation",
            "authors": "Tianyi Zhang, Tao Yu, Tatsunori Hashimoto, Mike Lewis, Wen-tau Yih, Daniel Fried, Sida Wang",
            "year": "2023",
            "abstract": "Sampling diverse programs from a code language model and reranking with model likelihood is a popular method for code generation but it is prone to preferring degenerate solutions. Inspired by collaborative programming, we propose Coder-Reviewer reranking. We augment Coder language models from past work, which generate programs given language instructions, with Reviewer models, which evaluate the likelihood of the instruction given the generated programs. We perform an extensive study across six datasets with eight models from three model families. Experimental results show that Coder-Reviewer reranking leads to consistent and significant improvement (up to 17% absolute accuracy gain) over reranking with the Coder model only. When combined with executability filtering, Coder-Reviewer reranking can often outperform the minimum Bayes risk method. Coder-Reviewer reranking is easy to implement by prompting, can generalize to different programming languages, and works well with off-the-shelf hyperparameters.",
            "url": "https://proceedings.mlr.press/v202/zhang23av/zhang23av.pdf",
            "publicationVenue": "International Conference on Machine Learning",
            "citation_count": 32
        },
        {
            "title": "DS-1000: A natural and reliable benchmark for data science code generation",
            "authors": "Yuhang Lai, Chengxi Li, Yiming Wang, Tianyi Zhang, Ruiqi Zhong, Luke Zettlemoyer, Wen-tau Yih, Daniel Fried, Sida Wang, Tao Yu",
            "year": "2023",
            "abstract": "We introduce DS-1000, a code generation benchmark with a thousand data science problems spanning seven Python libraries, such as Numpy and Pandas. Compared to prior works, DS-1000 incorporates three core features. First, our problems reflect diverse, realistic, and practical use cases since we collected them from StackOverflow. Second, our automatic evaluation is highly specific (reliable)\u2013across all Codex-002-predicted solutions that our evaluation accepts, only 1.8% of them are incorrect; we achieve this with multi-criteria metrics, checking both functional correctness by running test cases and surface-form constraints by restricting API usages or keywords. Finally, we proactively defend against memorization by slightly modifying our problems to be different from the original StackOverflow source; consequently, models cannot answer them correctly by memorizing the solutions from pre-training. The current best public system (Codex-002) achieves 43.3% accuracy, leaving ample room for improvement. We release our benchmark at https://ds1000-code-gen. github. io.",
            "url": "https://proceedings.mlr.press/v202/lai23b/lai23b.pdf",
            "publicationVenue": "International Conference on Machine Learning",
            "citation_count": 74
        },
        {
            "title": "Pragmatic Inference with a CLIP Listener for Contrastive Captioning",
            "authors": "Jiefu Ou, Benno Krojer, Daniel Fried",
            "year": "2023",
            "abstract": "We propose a simple yet effective and robust method for contrastive captioning: generating discriminative captions that distinguish target images from very similar alternative distractor images. Our approach is built on a pragmatic inference procedure that formulates captioning as a reference game between a speaker, which produces possible captions describing the target, and a listener, which selects the target given the caption. Unlike previous methods that derive both speaker and listener distributions from a single captioning model, we leverage an off-the-shelf CLIP model to parameterize the listener. Compared with captioner-only pragmatic models, our method benefits from rich vision language alignment representations from CLIP when reasoning over distractors. Like previous methods for discriminative captioning, our method uses a hyperparameter to control the tradeoff between the informativity (how likely captions are to allow a human listener to discriminate the target image) and the fluency of the captions. However, we find that our method is substantially more robust to the value of this hyperparameter than past methods, which allows us to automatically optimize the captions for informativity - outperforming past methods for discriminative captioning by 11% to 15% accuracy in human evaluations",
            "url": "https://arxiv.org/pdf/2306.08818",
            "publicationVenue": "arXiv preprint arXiv:2306.08818",
            "citation_count": 1
        },
        {
            "title": "Grounding Language Models to Images for Multimodal Inputs and Outputs",
            "authors": "Jing Yu Koh, Ruslan Salakhutdinov, Daniel Fried",
            "year": "2023",
            "abstract": "We propose an efficient method to ground pretrained text-only language models to the visual domain, enabling them to process arbitrarily interleaved image-and-text data, and generate text interleaved with retrieved images. Our method leverages the abilities of language models learnt from large scale text-only pretraining, such as in-context learning and free-form text generation. We keep the language model frozen, and finetune input and output linear layers to enable cross-modality interactions. This allows our model to process arbitrarily interleaved image-and-text inputs, and generate free-form text interleaved with retrieved images. We achieve strong zero-shot performance on grounded tasks such as contextual image retrieval and multimodal dialogue, and showcase compelling interactive abilities. Our approach works with any off-the-shelf language model and paves the way towards an effective, general solution for leveraging pretrained language models in visually grounded settings.",
            "url": "https://openreview.net/pdf?id=ElaajXDEKR",
            "publicationVenue": null,
            "citation_count": 86
        },
        {
            "title": "StarCoder: may the source be with you!",
            "authors": "Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, Qian Liu, Evgenii Zheltonozhskii, Terry Yue Zhuo, Thomas Wang, Olivier Dehaene, Mishig Davaadorj, Joel Lamy-Poirier, Jo\u00e3o Monteiro, Oleh Shliazhko, Nicolas Gontier, Nicholas Meade, Armel Zebaze, Ming-Ho Yee, Logesh Kumar Umapathi, Jian Zhu, Benjamin Lipkin, Muhtasham Oblokulov, Zhiruo Wang, Rudra Murthy, Jason Stillerman, Siva Sankalp Patel, Dmitry Abulkhanov, Marco Zocca, Manan Dey, Zhihan Zhang, Nour Fahmy, Urvashi Bhattacharyya, Wenhao Yu, Swayam Singh, Sasha Luccioni, Paulo Villegas, Maxim Kunakov, Fedor Zhdanov, Manuel Romero, Tony Lee, Nadav Timor, Jennifer Ding, Claire Schlesinger, Hailey Schoelkopf, Jan Ebert, Tri Dao, Mayank Mishra, Alex Gu, Jennifer Robinson, Carolyn Jane Anderson, Brendan Dolan-Gavitt, Danish Contractor, Siva Reddy, Daniel Fried, Dzmitry Bahdanau, Yacine Jernite, Carlos Mu\u00f1oz Ferrandis, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro von Werra, Harm de Vries",
            "year": "2023",
            "abstract": "The BigCode community, an open-scientific collaboration working on the responsible development of Large Language Models for Code (Code LLMs), introduces StarCoder and StarCoderBase: 15.5B parameter models with 8K context length, infilling capabilities and fast large-batch inference enabled by multi-query attention. StarCoderBase is trained on 1 trillion tokens sourced from The Stack, a large collection of permissively licensed GitHub repositories with inspection tools and an opt-out process. We fine-tuned StarCoderBase on 35B Python tokens, resulting in the creation of StarCoder. We perform the most comprehensive evaluation of Code LLMs to date and show that StarCoderBase outperforms every open Code LLM that supports multiple programming languages and matches or outperforms the OpenAI code-cushman-001 model. Furthermore, StarCoder outperforms every model that is fine-tuned on Python, can be prompted to achieve 40\\% pass@1 on HumanEval, and still retains its performance on other programming languages. We take several important steps towards a safe open-access model release, including an improved PII redaction pipeline and a novel attribution tracing tool, and make the StarCoder models publicly available under a more commercially viable version of the Open Responsible AI Model license.",
            "url": "https://arxiv.org/pdf/2305.06161",
            "publicationVenue": "arXiv preprint arXiv:2305.06161",
            "citation_count": 224
        },
        {
            "title": "SantaCoder: don't reach for the stars!",
            "authors": "Loubna Ben Allal, Raymond Li, Denis Kocetkov, Chenghao Mou, Christopher Akiki, Carlos Munoz Ferrandis, Niklas Muennighoff, Mayank Mishra, Alex Gu, Manan Dey, Logesh Kumar Umapathi, Carolyn Jane Anderson, Yangtian Zi, Joel Lamy Poirier, Hailey Schoelkopf, Sergey Troshin, Dmitry Abulkhanov, Manuel Romero, Michael Lappert, Francesco De Toni, Bernardo Garc\u00eda del R\u00edo, Qian Liu, Shamik Bose, Urvashi Bhattacharyya, Terry Yue Zhuo, Ian Yu, Paulo Villegas, Marco Zocca, Sourab Mangrulkar, David Lansky, Huu Nguyen, Danish Contractor, Luis Villa, Jia Li, Dzmitry Bahdanau, Yacine Jernite, Sean Hughes, Daniel Fried, Arjun Guha, Harm de Vries, Leandro von Werra",
            "year": "2023",
            "abstract": "The BigCode project is an open-scientific collaboration working on the responsible development of large language models for code. This tech report describes the progress of the collaboration until December 2022, outlining the current state of the Personally Identifiable Information (PII) redaction pipeline, the experiments conducted to de-risk the model architecture, and the experiments investigating better preprocessing methods for the training data. We train 1.1B parameter models on the Java, JavaScript, and Python subsets of The Stack and evaluate them on the MultiPL-E text-to-code benchmark. We find that more aggressive filtering of near-duplicates can further boost performance and, surprisingly, that selecting files from repositories with 5+ GitHub stars deteriorates performance significantly. Our best model outperforms previous open-source multilingual code generation models (InCoder-6.7B and CodeGen-Multi-2.7B) in both left-to-right generation and infilling on the Java, JavaScript, and Python portions of MultiPL-E, despite being a substantially smaller model. All models are released under an OpenRAIL license at https://hf.co/bigcode.",
            "url": "https://arxiv.org/pdf/2301.03988",
            "publicationVenue": "arXiv preprint arXiv:2301.03988",
            "citation_count": 80
        }
    ],
    "Alex Hauptmann": [
        {
            "title": "Documentnet: Bridging the data gap in document pre-training",
            "authors": "Lijun Yu, Jin Miao, Xiaoyu Sun, Jiayi Chen, Alexander G Hauptmann, Hanjun Dai, Wei Wei",
            "year": "2023",
            "abstract": "Document understanding tasks, in particular, Visually-rich Document Entity Retrieval (VDER), have gained significant attention in recent years thanks to their broad applications in enterprise AI. However, publicly available data have been scarce for these tasks due to strict privacy constraints and high annotation costs. To make things worse, the non-overlapping entity spaces from different datasets hinder the knowledge transfer between document types. In this paper, we propose a method to collect massive-scale and weakly labeled data from the web to benefit the training of VDER models. The collected dataset, named DocumentNet, does not depend on specific document types or entity sets, making it universally applicable to all VDER tasks. The current DocumentNet consists of 30M documents spanning nearly 400 document types organized in a four-level ontology. Experiments on a set of broadly adopted VDER tasks show significant improvements when DocumentNet is incorporated into the pre-training for both classic and few-shot learning settings. With the recent emergence of large language models (LLMs), DocumentNet provides a large data source to extend their multimodal capabilities for VDER.",
            "url": "https://aclanthology.org/2023.emnlp-industry.66.pdf",
            "publicationVenue": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: Industry Track",
            "citation_count": 1
        },
        {
            "title": "Language Model Beats Diffusion--Tokenizer is Key to Visual Generation",
            "authors": "Lijun Yu, Jose Lezama, Nitesh B Gundavarapu, Luca Versari, Kihyuk Sohn, David Minnen, Yong Cheng, Agrim Gupta, Xiuye Gu, Alexander G Hauptmann, Boqing Gong, Ming-Hsuan Yang, Irfan Essa, David A Ross, Lu Jiang",
            "year": "2023",
            "abstract": "While Large Language Models (LLMs) are the dominant models for generative tasks in language, they do not perform as well as diffusion models on image and video generation. To effectively use LLMs for visual generation, one crucial component is the visual tokenizer that maps pixel-space inputs to discrete tokens appropriate for LLM learning. In this paper, we introduce MAGVIT-v2, a video tokenizer designed to generate concise and expressive tokens for both videos and images using a common token vocabulary. Equipped with this new tokenizer, we show that LLMs outperform diffusion models on standard image and video generation benchmarks including ImageNet and Kinetics. In addition, we demonstrate that our tokenizer surpasses the previously top-performing video tokenizer on two more tasks: (1) video compression comparable to the next-generation video codec (VCC) according to human evaluations, and (2) learning effective representations for action recognition tasks.",
            "url": "https://arxiv.org/pdf/2310.05737",
            "publicationVenue": "arXiv preprint arXiv:2310.05737",
            "citation_count": 14
        },
        {
            "title": "Zero-Shot and Few-Shot Stance Detection on Varied Topics via Conditional Generation",
            "authors": "Haoyang Wen, Alexander G Hauptmann",
            "year": "2023",
            "abstract": "Zero-shot and few-shot stance detection identify the polarity of text with regard to a certain target when we have only limited or no training resources for the target. Previous work generally formulates the problem into a classification setting, ignoring the potential use of label text. In this paper, we instead utilize a conditional generation framework and formulate the problem as denoising from partially-filled templates, which can better utilize the semantics among input, label, and target texts. We further propose to jointly train an auxiliary task, target prediction, and to incorporate manually constructed incorrect samples with unlikelihood training to improve the representations for both target and label texts. We also verify the effectiveness of target-related Wikipedia knowledge with the generation framework. Experiments show that our proposed method significantly outperforms several strong baselines on VAST, and achieves new state-of-the-art performance.",
            "url": "https://aclanthology.org/2023.acl-short.127.pdf",
            "publicationVenue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
            "citation_count": 2
        },
        {
            "title": "Towards open-domain twitter user profile inference",
            "authors": "Haoyang Wen, Zhenxin Xiao, Eduard Hovy, Alexander G Hauptmann",
            "year": "2023",
            "abstract": "Twitter user profile inference utilizes information from Twitter to predict user attributes (eg, occupation, location), which is controversial because of its usefulness for downstream applications and its potential to reveal users\u2019 privacy. Therefore, it is important for researchers to determine the extent of profiling in a safe environment to facilitate proper use and make the public aware of the potential risks. Contrary to existing approaches on limited attributes, we explore open-domain Twitter user profile inference. We conduct a case study where we collect publicly available WikiData public figure profiles and use diverse WikiData predicates for profile inference. After removing sensitive attributes, our data contains over 150K public figure profiles from WikiData, over 50 different attribute predicates, and over 700K attribute values. We further propose a prompt-based generation method, which can infer values that are implicitly mentioned in the Twitter information. Experimental results show that the generation-based approach can infer more comprehensive user profiles than baseline extraction-based methods, but limitations still remain to be applied for real-world use. We also enclose a detailed ethical statement for our data, potential benefits and risks from this work, and our efforts to mitigate the risks.",
            "url": "https://aclanthology.org/2023.findings-acl.198.pdf",
            "publicationVenue": "Findings of the Association for Computational Linguistics: ACL 2023",
            "citation_count": 1
        },
        {
            "title": "Robust Automatic Detection of Traffic Activity",
            "authors": "Alexander Hauptmann, Lijun Yu, Wenhe Liu, Yijun Qian, Zhiqi Cheng, Liangke Gui",
            "year": "2023",
            "abstract": "The accurate detection and prediction of actions by multiple traffic participants such as pedestrians, vehicles, cyclists and others is a critical prerequisite for enabling self driving vehicles to make autonomous decisions. Current approaches to teach an autonomous vehicle how to drive use reinforcement learning which is essentially relies on already collected situations as examples relying purely on visual similarity without any understanding of the semantics of the situation and therefore no ability to reason about other similar situations that may have different appearance. This can be overcome by methods that provide situation awareness to the vehicle. The idea is to enable semantically meaningful representations of road scenarios which include the physical layout of the scene, the various participants prior and current activities. The ability to abstract this semantic representation and apply it to multiple scenes that are conceptually similar allows much more robust decision-making strategies by autonomous vehicles. Essentially this allows endowing autonomous vehicles with a reasoning process.",
            "url": "https://rosap.ntl.bts.gov/view/dot/68085",
            "publicationVenue": null,
            "citation_count": 2
        },
        {
            "title": "Training Vision-Language Transformers from Captions",
            "authors": "Liangke Gui, Yingshan Chang, Qiuyuan Huang, Subhojit Som, Alexander G Hauptmann, Jianfeng Gao, Yonatan Bisk",
            "year": "2023",
            "abstract": "Vision-Language Transformers can be learned without low-level human labels (e.g. class labels, bounding boxes, etc). Existing work, whether explicitly utilizing bounding boxes (Chen et al., 2020b; Tan & Bansal, 2019; Lu et al., 2019) or patches (Kim et al., 2021), assumes that the visual backbone must first be trained on ImageNet (Russakovsky et al., 2015) class prediction before being integrated into a multimodal linguistic pipeline. We show that this is not necessary and introduce a new model Vision-Language from Captions (VLC) built on top of Masked Auto-Encoders (He et al., 2022) that does not require this supervision. We seek to provide general advice on multimodal pretraining by examining the roles of (a) unimodal initialization, (b) unimodal architectural components and (c) data annotation in the pretraining corpus. Our extensive and carefully controlled studies suggest that none of the above factors is absolutely important in achieving versatile vision-language representations. We conclude our analysis with suggestions on the choices of initialization, architectural components, and annotation formats targeting a better balance between data efficiency and representation quality.",
            "url": "https://openreview.net/pdf?id=xLnbSpozWS",
            "publicationVenue": "Transactions on Machine Learning Research",
            "citation_count": 0
        },
        {
            "title": "Document Entity Retrieval with Massive and Noisy Pre-training",
            "authors": "Lijun Yu, Jin Miao, Xiaoyu Sun, Jiayi Chen, Alexander G Hauptmann, Hanjun Dai, Wei Wei",
            "year": "2023",
            "abstract": "Visually-Rich Document Entity Retrieval (VDER) is a type of machine learning task that aims at recovering text spans in the documents for each of the entities in question. VDER has gained significant attention in recent years thanks to its broad applications in enterprise AI. Unfortunately, as document images often contain personally identifiable information (PII), publicly available data have been scarce, not only because of privacy constraints but also the costs of acquiring annotations. To make things worse, each dataset would often define its own sets of entities, and the non-overlapping entity spaces between datasets make it difficult to transfer knowledge between documents. In this paper, we propose a method to collect massive-scale, noisy, and weakly labeled data from the web to benefit the training of VDER models. Such a method will generate a huge amount of document image data to compensate for the lack of training data in many VDER settings. Moreover, the collected dataset named DocuNet would not need to be dependent on specific document types or entity sets, making it universally applicable to all VDER tasks. Empowered by DocuNet, we present a lightweight multimodal architecture named UniFormer, which can learn a unified representation from text, layout, and image crops without needing extra visual pertaining. We experiment with our methods on popular VDER models in various settings and show the improvements when this massive dataset is incorporated with UniFormer on both classic entity retrieval and few-shot learning settings.",
            "url": "https://arxiv.org/pdf/2306.08937",
            "publicationVenue": "arXiv preprint arXiv:2306.08937",
            "citation_count": 0
        },
        {
            "title": "Leveraging body pose estimation for gesture recognition in human-robot interaction using synthetic data",
            "authors": "Xiaoyu Zhu, Celso M de Melo, Alexander Hauptmann",
            "year": "2023",
            "abstract": "Effectively recognizing human gestures from variant viewpoints plays a fundamental role in the successful collaboration between humans and robots. Deep learning approaches have achieved promising performance in gesture recognition. However, they are usually data-hungry and require large-scale labeled data, which are not usually accessible in a practical setting. Synthetic data, on the other hand, can be easily obtained from simulators with fine-grained annotations and variant modalities. Existing state-of-the-art approaches have shown promising results using synthetic data, but there is still a large performance gap between the models trained on synthetic data and real data. To learn domain-invariant feature representations, we propose a novel approach which jointly takes RGB videos and 3D meshes as inputs to perform robust action recognition. We empirically validate our model on the RoCoG-v2 dataset\u00a0\u2026",
            "url": null,
            "publicationVenue": "Synthetic Data for Artificial Intelligence and Machine Learning: Tools, Techniques, and Applications",
            "citation_count": 0
        },
        {
            "title": "Data processing system for classifying keyed data representing inhaler device operation",
            "authors": null,
            "year": "2023",
            "abstract": "A data processing system digitally processes data feeds of inhaler device operation. The data feed represents operation of an inhaler device. The system indexes the live data feed with a key value representing the inhaler device for which the live data feed is obtained. For a particular key value indexed in the in-memory data storage, the system queries, a data feed representing physical operation of an inhaler device, segments the live data feed for that particular key value into a plurality of data samples, process at least a portion of the data samples to classify each of the processed data samples; outputs a prompt specifying whether operation of the inhaler device is within a threshold range of operation. Audio data, temperature data, image data, and ranging data can be processed to classify operation of the inhaler device and the order of operations of the inhaler device.",
            "url": "https://patentimages.storage.googleapis.com/e8/db/84/488663e8579aa8/US20200381111A1.pdf",
            "publicationVenue": null,
            "citation_count": 0
        },
        {
            "title": "ChartReader: A Unified Framework for Chart Derendering and Comprehension without Heuristic Rules",
            "authors": "Zhi-Qi Cheng, Qi Dai, Siyao Li, Jingdong Sun, Teruko Mitamura, Alexander G Hauptmann",
            "year": "2023",
            "abstract": "Charts are a powerful tool for visually conveying complex data, but their comprehension poses a challenge due to the diverse chart types and intricate components. Existing chart comprehension methods suffer from either heuristic rules or an over-reliance on OCR systems, resulting in suboptimal performance. To address these issues, we present ChartReader, a unified framework that seamlessly integrates chart derendering and comprehension tasks. Our approach includes a transformer-based chart component detection module and an extended pre-trained vision-language model for chart-to-X tasks. By learning the rules of charts automatically from annotated datasets, our approach eliminates the need for manual rule-making, reducing effort and enhancing accuracy.~We also introduce a data variable replacement technique and extend the input and position embeddings of the pre-trained model for cross-task training. We evaluate ChartReader on Chart-to-Table, ChartQA, and Chart-to-Text tasks, demonstrating its superiority over existing methods. Our proposed framework can significantly reduce the manual effort involved in chart analysis, providing a step towards a universal chart understanding model. Moreover, our approach offers opportunities for plug-and-play integration with mainstream LLMs such as T5 and TaPas, extending their capability to chart comprehension tasks. The code is available at https://github.com/zhiqic/ChartReader.",
            "url": "https://arxiv.org/pdf/2304.02173",
            "publicationVenue": "arXiv preprint arXiv:2304.02173",
            "citation_count": 2
        },
        {
            "title": "Breaking the limits of text-conditioned 3d motion synthesis with elaborative descriptions",
            "authors": "Yijun Qian, Jack Urbanek, Alexander G Hauptmann, Jungdam Won",
            "year": "2023",
            "abstract": "Given its wide applications, there is increasing focus on generating 3D human motions from textual descriptions. Differing from the majority of previous works, which regard actions as single entities and can only generate short sequences for simple motions, we propose EMS, an elaborative motion synthesis model conditioned on detailed natural language descriptions. It generates natural and smooth motion sequences for long and complicated actions by factorizing them into groups of atomic actions. Meanwhile, it understands atomic-action level attributes (eg, motion direction, speed, and body parts) and enables users to generate sequences of unseen complex actions from unique sequences of known atomic actions with independent attribute settings and timings applied. We evaluate our method on the KIT Motion-Language and BABEL benchmarks, where it outperforms all previous state-of-the-art with noticeable margins.",
            "url": "https://openaccess.thecvf.com/content/ICCV2023/papers/Qian_Breaking_The_Limits_of_Text-conditioned_3D_Motion_Synthesis_with_Elaborative_ICCV_2023_paper.pdf",
            "publicationVenue": "Proceedings of the IEEE/CVF International Conference on Computer Vision",
            "citation_count": 4
        },
        {
            "title": "Magvit: Masked generative video transformer",
            "authors": "Lijun Yu, Yong Cheng, Kihyuk Sohn, Jose Lezama, Han Zhang, Huiwen Chang, Alexander G Hauptmann, Ming-Hsuan Yang, Yuan Hao, Irfan Essa, Lu Jiang",
            "year": "2023",
            "abstract": "We introduce the MAsked Generative VIdeo Transformer, MAGVIT, to tackle various video synthesis tasks with a single model. We introduce a 3D tokenizer to quantize a video into spatial-temporal visual tokens and propose an embedding method for masked video token modeling to facilitate multi-task learning. We conduct extensive experiments to demonstrate the quality, efficiency, and flexibility of MAGVIT. Our experiments show that (i) MAGVIT performs favorably against state-of-the-art approaches and establishes the best-published FVD on three video generation benchmarks, including the challenging Kinetics-600.(ii) MAGVIT outperforms existing methods in inference time by two orders of magnitude against diffusion models and by 60x against autoregressive models.(iii) A single MAGVIT model supports ten diverse generation tasks and generalizes across videos from different visual domains. The source code and trained models will be released to the public at https://magvit. cs. cmu. edu.",
            "url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Yu_MAGVIT_Masked_Generative_Video_Transformer_CVPR_2023_paper.pdf",
            "publicationVenue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "citation_count": 54
        },
        {
            "title": "Stmt: A spatial-temporal mesh transformer for mocap-based action recognition",
            "authors": "Xiaoyu Zhu, Po-Yao Huang, Junwei Liang, Celso M de Melo, Alexander G Hauptmann",
            "year": "2023",
            "abstract": "We study the problem of human action recognition using motion capture (MoCap) sequences. Unlike existing techniques that take multiple manual steps to derive standardized skeleton representations as model input, we propose a novel Spatial-Temporal Mesh Transformer (STMT) to directly model the mesh sequences. The model uses a hierarchical transformer with intra-frame off-set attention and inter-frame self-attention. The attention mechanism allows the model to freely attend between any two vertex patches to learn non-local relationships in the spatial-temporal domain. Masked vertex modeling and future frame prediction are used as two self-supervised tasks to fully activate the bi-directional and auto-regressive attention in our hierarchical transformer. The proposed method achieves state-of-the-art performance compared to skeleton-based and point-cloud-based models on common MoCap benchmarks. Code is available at https://github. com/zgzxy001/STMT.",
            "url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Zhu_STMT_A_Spatial-Temporal_Mesh_Transformer_for_MoCap-Based_Action_Recognition_CVPR_2023_paper.pdf",
            "publicationVenue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "citation_count": 7
        }
    ],
    "Daphne Ippolito": [
        {
            "title": "Scalable extraction of training data from (production) language models",
            "authors": "Milad Nasr, Nicholas Carlini, Jonathan Hayase, Matthew Jagielski, A Feder Cooper, Daphne Ippolito, Christopher A Choquette-Choo, Eric Wallace, Florian Tram\u00e8r, Katherine Lee",
            "year": "2023",
            "abstract": "This paper studies extractable memorization: training data that an adversary can efficiently extract by querying a machine learning model without prior knowledge of the training dataset. We show an adversary can extract gigabytes of training data from open-source language models like Pythia or GPT-Neo, semi-open models like LLaMA or Falcon, and closed models like ChatGPT. Existing techniques from the literature suffice to attack unaligned models; in order to attack the aligned ChatGPT, we develop a new divergence attack that causes the model to diverge from its chatbot-style generations and emit training data at a rate 150x higher than when behaving properly. Our methods show practical attacks can recover far more data than previously thought, and reveal that current alignment techniques do not eliminate memorization.",
            "url": "https://arxiv.org/pdf/2311.17035.pdf?trk=public_post_comment-text",
            "publicationVenue": "arXiv preprint arXiv:2311.17035",
            "citation_count": 36
        },
        {
            "title": "Report of the 1st Workshop on Generative AI and Law",
            "authors": "A Feder Cooper, Katherine Lee, James Grimmelmann, Daphne Ippolito, Christopher Callison-Burch, Christopher A Choquette-Choo, Niloofar Mireshghallah, Miles Brundage, David Mimno, Madiha Zahrah Choksi, Jack M Balkin, Nicholas Carlini, Christopher De Sa, Jonathan Frankle, Deep Ganguli, Bryant Gipson, Andres Guadamuz, Swee Leng Harris, Abigail Z Jacobs, Elizabeth Joh, Gautam Kamath, Mark Lemley, Cass Matthews, Christine McLeavey, Corynne McSherry, Milad Nasr, Paul Ohm, Adam Roberts, Tom Rubin, Pamela Samuelson, Ludwig Schubert, Kristen Vaccaro, Luis Villa, Felix Wu, Elana Zeide",
            "year": "2023",
            "abstract": "This report presents the takeaways of the inaugural Workshop on Generative AI and Law (GenLaw), held in July 2023. A cross-disciplinary group of practitioners and scholars from computer science and law convened to discuss the technical, doctrinal, and policy challenges presented by law for Generative AI, and by Generative AI for law, with an emphasis on US law in particular. We begin the report with a high-level statement about why Generative AI is both immensely significant and immensely challenging for law. To meet these challenges, we conclude that there is an essential need for 1) a shared knowledge base that provides a common conceptual language for experts across disciplines; 2) clarification of the distinctive technical capabilities of generative-AI systems, as compared and contrasted to other computer and AI systems; 3) a logical taxonomy of the legal issues these systems raise; and, 4) a concrete\u00a0\u2026",
            "url": null,
            "publicationVenue": "arXiv e-prints",
            "citation_count": 1
        },
        {
            "title": "Reverse-Engineering Decoding Strategies Given Blackbox Access to a Language Generation System",
            "authors": "Daphne Ippolito, Nicholas Carlini, Katherine Lee, Milad Nasr, Yun William Yu",
            "year": "2023",
            "abstract": "Neural language models are increasingly deployed into APIs and websites that allow a user to pass in a prompt and receive generated text. Many of these systems do not reveal generation parameters. In this paper, we present methods to reverse-engineer the decoding method used to generate text (i.e., top-or nucleus sampling). Our ability to discover which decoding strategy was used has implications for detecting generated text. Additionally, the process of discovering the decoding strategy can reveal biases caused by selecting decoding settings which severely truncate a model's predicted distributions. We perform our attack on several families of open-source language models, as well as on production systems (e.g., ChatGPT).",
            "url": "https://arxiv.org/pdf/2309.04858",
            "publicationVenue": "arXiv preprint arXiv:2309.04858",
            "citation_count": 1
        },
        {
            "title": "Prompts should not be seen as secrets: Systematically measuring prompt extraction attack success",
            "authors": "Yiming Zhang, Daphne Ippolito",
            "year": "2023",
            "abstract": "The generations of large language models are commonly controlled through prompting techniques, where a user's query to the model is prefixed with a prompt that aims to guide the model's behaviour on the query. The prompts used by companies to guide their models are often treated as secrets, to be hidden from the user making the query. They have even been treated as commodities to be bought and sold. However, there has been anecdotal evidence showing that the prompts can be extracted by a user even when they are kept secret. In this paper, we present a framework for systematically measuring the success of prompt extraction attacks. In experiments with multiple sources of prompts and multiple underlying language models, we find that simple text-based attacks can in fact reveal prompts with high probability.",
            "url": "https://arxiv.org/html/2307.06865v2",
            "publicationVenue": "arXiv preprint arXiv:2307.06865",
            "citation_count": 16
        },
        {
            "title": "AI and Law: The Next Generation",
            "authors": "Katherine Lee, A Feder Cooper, James Grimmelmann, Daphne Ippolito",
            "year": "2023",
            "abstract": "We are in a moment of seemingly nonstop excitement (and seemingly nonstop lawsuits) about the future of AI-assisted content creation, and the questions such creation raises about data ownership, privacy, the future of work, and how technology shapes individual and collective rights. When considering questions about rights, it is important to think not only about novel technical developments, but also novel issues such development presents for the law--in particular, copyright law. Systems like ChatGPT and Stable Diffusion exhibit impressive capabilities; however, they have also been shown to regurgitate training data examples in their outputs, bringing about concerns regarding infringement of intellectual property rights.",
            "url": "https://www.researchgate.net/profile/A-Cooper-2/publication/372251056_AI_and_Law_The_Next_Generation_An_explainer_series/links/64ad12b7b9ed6874a51152ec/AI-and-Law-The-Next-Generation-An-explainer-series.pdf",
            "publicationVenue": "Available at SSRN",
            "citation_count": 1
        },
        {
            "title": "Real or fake text?: Investigating human ability to detect boundaries between human-written and machine-generated text",
            "authors": "Liam Dugan, Daphne Ippolito, Arun Kirubarajan, Sherry Shi, Chris Callison-Burch",
            "year": "2023",
            "abstract": "As text generated by large language models proliferates, it becomes vital to understand how humans engage with such text, and whether or not they are able to detect when the text they are reading did not originate with a human writer. Prior work on human detection of generated text focuses on the case where an entire passage is either human-written or machine-generated. In this paper, we study a more realistic setting where text begins as human-written and transitions to being generated by state-of-the-art neural language models. We show that, while annotators often struggle at this task, there is substantial variance in annotator skill and that given proper incentives, annotators can improve at this task over time. Furthermore, we conduct a detailed comparison study and analyze how a variety of variables (model size, decoding strategy, fine-tuning, prompt genre, etc.) affect human detection performance. Finally, we collect error annotations from our participants and use them to show that certain textual genres influence models to make different types of errors and that certain sentence-level features correlate highly with annotator selection. We release the RoFT dataset: a collection of over 21,000 human annotations paired with error classifications to encourage future work in human detection and evaluation of generated text.",
            "url": "https://ojs.aaai.org/index.php/AAAI/article/view/26501/26273",
            "publicationVenue": "Proceedings of the AAAI Conference on Artificial Intelligence",
            "citation_count": 21
        },
        {
            "title": "A Pretrainer's Guide to Training Data: Measuring the Effects of Data Age, Domain Coverage, Quality, & Toxicity",
            "authors": "Shayne Longpre, Gregory Yauney, Emily Reif, Katherine Lee, Adam Roberts, Barret Zoph, Denny Zhou, Jason Wei, Kevin Robinson, David Mimno, Daphne Ippolito",
            "year": "2023",
            "abstract": "Pretraining is the preliminary and fundamental step in developing capable language models (LM). Despite this, pretraining data design is critically under-documented and often guided by empirically unsupported intuitions. To address this, we pretrain 28 1.5B parameter decoder-only models, training on data curated (1) at different times, (2) with varying toxicity and quality filters, and (3) with different domain compositions. First, we quantify the effect of pretraining data age. A temporal shift between evaluation data and pretraining data leads to performance degradation, which is not overcome by finetuning. Second, we explore the effect of quality and toxicity filters, showing a trade-off between performance on standard benchmarks and risk of toxic generations. Our findings indicate there does not exist a one-size-fits-all solution to filtering training data. We also find that the effects of different types of filtering are not predictable from text domain characteristics. Lastly, we empirically validate that the inclusion of heterogeneous data sources, like books and web, is broadly beneficial and warrants greater prioritization. These findings constitute the largest set of experiments to validate, quantify, and expose many undocumented intuitions about text pretraining, which we hope will help support more informed data-centric decisions in LM development.",
            "url": "https://arxiv.org/pdf/2305.13169",
            "publicationVenue": "arXiv preprint arXiv:2305.13169",
            "citation_count": 35
        },
        {
            "title": "Preventing generation of verbatim memorization in language models gives a false sense of privacy",
            "authors": "Daphne Ippolito, Florian Tram\u00e8r, Milad Nasr, Chiyuan Zhang, Matthew Jagielski, Katherine Lee, Christopher A Choquette-Choo, Nicholas Carlini",
            "year": "2023",
            "abstract": "Studying data memorization in neural language models helps us understand the risks (e.g., to privacy or copyright) associated with models regurgitating training data, and aids in the evaluation of potential countermeasures. Many prior works -- and some recently deployed defenses -- focus on \"verbatim memorization\", defined as a model generation that exactly matches a substring from the training set. We argue that verbatim memorization definitions are too restrictive and fail to capture more subtle forms of memorization. Specifically, we design and implement an efficient defense based on Bloom filters that perfectly prevents all verbatim memorization. And yet, we demonstrate that this \"perfect\" filter does not prevent the leakage of training data. Indeed, it is easily circumvented by plausible and minimally modified \"style-transfer\" prompts -- and in some cases even the non-modified original prompts -- to extract memorized information. For example, instructing the model to output ALL-CAPITAL texts bypasses memorization checks based on verbatim matching. We conclude by discussing potential alternative definitions and why defining memorization is a difficult yet crucial open question for neural language models.",
            "url": "https://www.research-collection.ethz.ch/bitstream/handle/20.500.11850/642948/2/2023.inlg-main.3.pdf",
            "publicationVenue": "Proceedings of the 16th International Natural Language Generation Conference",
            "citation_count": 42
        },
        {
            "title": "Extracting training data from diffusion models",
            "authors": "Nicolas Carlini, Jamie Hayes, Milad Nasr, Matthew Jagielski, Vikash Sehwag, Florian Tramer, Borja Balle, Daphne Ippolito, Eric Wallace",
            "year": "2023",
            "abstract": "Image diffusion models such as DALL-E 2, Imagen, and Stable Diffusion have attracted significant attention due to their ability to generate high-quality synthetic images. In this work, we show that diffusion models memorize individual images from their training data and emit them at generation time. With a generate-and-filter pipeline, we extract over a thousand training examples from state-of-the-art models, ranging from photographs of individual people to trademarked company logos. We also train hundreds of diffusion models in various settings to analyze how different modeling and data decisions affect privacy. Overall, our results show that diffusion models are much less private than prior generative models such as GANs, and that mitigating these vulnerabilities may require new advances in privacy-preserving training.",
            "url": "https://www.usenix.org/system/files/usenixsecurity23-carlini.pdf",
            "publicationVenue": "32nd USENIX Security Symposium (USENIX Security 23)",
            "citation_count": 232
        },
        {
            "title": "Understanding the Limitations of Using Large Language Models for Text Generation",
            "authors": "Daphne Ippolito",
            "year": "2023",
            "abstract": "State-of-the-art neural language models are capable of generating incredibly fluent English text. This success provides opportunities for novel forms of interaction, where human writers work collaboratively with a natural-language generation system toward a set of goals. However, it also poses several challenges. Evaluating and comparing the skill of different open-ended text generation systems is challenging, and generated text can have negative societal impact if it proliferates and is not detectable by humans. In this dissertation, I introduce a detection-based evaluation task that can be used to compare different language models and generative configuations. By both asking humans to complete this task and training automatic classifier to complete it, I investigate how the tradeoff between generating high-quality and generating diverse text impacts detectability. Through subsequent large-scale user studies, I show\u00a0\u2026",
            "url": "https://www.cis.upenn.edu/~ccb/publications/dissertations/daphne-ippolito-thesis.pdf",
            "publicationVenue": null,
            "citation_count": 0
        },
        {
            "title": "Palm: Scaling language modeling with pathways",
            "authors": "Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, Noah Fiedel",
            "year": "2023",
            "abstract": "Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning, which drastically reduces the number of task-specific training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model (PaLM). We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML system which enables highly efficient training across multiple TPU Pods. We demonstrate continued benefits of scaling by achieving state-of-the-art few-shot learning results on hundreds of language understanding and generation benchmarks. On a number of these tasks, PaLM 540B achieves breakthrough performance, outperforming the finetuned state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released BIG-bench benchmark. A significant number of BIG-bench tasks showed discontinuous improvements from model scale, meaning that performance steeply increased as we scaled to our largest model. PaLM also has strong capabilities in multilingual tasks and source code generation, which we demonstrate on a wide array of benchmarks. We additionally provide a comprehensive analysis on bias and toxicity, and study the extent of training data memorization with respect to model scale. Finally, we discuss the ethical considerations related to large language models and discuss potential mitigation strategies.",
            "url": "https://www.jmlr.org/papers/volume24/22-1144/22-1144.pdf",
            "publicationVenue": "Journal of Machine Learning Research",
            "citation_count": 2930
        }
    ],
    "Lori Levin": [
        {
            "title": "SigMoreFun submission to the SIGMORPHON shared task on interlinear glossing",
            "authors": "Taiqi He, Lindia Tjuatja, Nathaniel Robinson, Shinji Watanabe, David R Mortensen, Graham Neubig, Lori Levin",
            "year": "2023",
            "abstract": "In our submission to the SIGMORPHON 2023 Shared Task on interlinear glossing (IGT), we explore approaches to data augmentation and modeling across seven low-resource languages. For data augmentation, we explore two approaches: creating artificial data from the provided training data and utilizing existing IGT resources in other languages. On the modeling side, we test an enhanced version of the provided token classification baseline as well as a pretrained multilingual seq2seq model. Additionally, we apply post-correction using a dictionary for Gitksan, the language with the smallest amount of data. We find that our token classification models are the best performing, with the highest word-level accuracy for Arapaho and highest morpheme-level accuracy for Gitksan out of all submissions. We also show that data augmentation is an effective strategy, though applying artificial data pretraining has very different effects across both models tested.",
            "url": "https://aclanthology.org/2023.sigmorphon-1.22.pdf",
            "publicationVenue": "Proceedings of the 20th SIGMORPHON workshop on Computational Research in Phonetics, Phonology, and Morphology",
            "citation_count": 2
        },
        {
            "title": "Generalized Glossing Guidelines: An Explicit, Human-and Machine-Readable, Item-and-Process Convention for Morphological Annotation",
            "authors": "David R Mortensen, Ela Gulsen, Taiqi He, Nathaniel Robinson, Jonathan Amith, Lindia Tjuatja, Lori Levin",
            "year": "2023",
            "abstract": "Interlinear glossing provides a vital type of morphosyntactic annotation, both for linguists and language revitalists, and numerous conventions exist for representing it formally and computationally. Some of these formats are human readable; others are machine readable. Some are easy to edit with general-purpose tools. Few represent non-concatentative processes like infixation, reduplication, mutation, truncation, and tonal overwriting in a consistent and formally rigorous way (on par with affixation). We propose an annotation convention\u00e2\u20ac\u201d Generalized Glossing Guidelines (GGG) that combines all of these positive properties using an Item-and-Process (IP) framework. We describe the format, demonstrate its linguistic adequacy, and compare it with two other interlinear glossed text annotation schemes.",
            "url": "https://aclanthology.org/2023.sigmorphon-1.7.pdf",
            "publicationVenue": "Proceedings of the 20th SIGMORPHON workshop on Computational Research in Phonetics, Phonology, and Morphology",
            "citation_count": 1
        },
        {
            "title": "Syntax and Semantics Meet in the\" Middle\": Probing the Syntax-Semantics Interface of LMs Through Agentivity",
            "authors": "Lindia Tjuatja, Emmy Liu, Lori Levin, Graham Neubig",
            "year": "2023",
            "abstract": "Recent advances in large language models have prompted researchers to examine their abilities across a variety of linguistic tasks, but little has been done to investigate how models handle the interactions in meaning across words and larger syntactic forms -- i.e. phenomena at the intersection of syntax and semantics. We present the semantic notion of agentivity as a case study for probing such interactions. We created a novel evaluation dataset by utilitizing the unique linguistic properties of a subset of optionally transitive English verbs. This dataset was used to prompt varying sizes of three model classes to see if they are sensitive to agentivity at the lexical level, and if they can appropriately employ these word-level priors given a specific syntactic context. Overall, GPT-3 text-davinci-003 performs extremely well across all experiments, outperforming all other models tested by far. In fact, the results are even better correlated with human judgements than both syntactic and semantic corpus statistics. This suggests that LMs may potentially serve as more useful tools for linguistic annotation, theory testing, and discovery than select corpora for certain tasks.",
            "url": "https://arxiv.org/pdf/2305.18185",
            "publicationVenue": "arXiv preprint arXiv:2305.18185",
            "citation_count": 1
        },
        {
            "title": "Proceedings of the 21st International Workshop on Treebanks and Linguistic Theories (TLT, GURT/SyntaxFest 2023)",
            "authors": "Daniel Dakota, Kilian Evang, Sandra K\u00fcbler, Lori Levin",
            "year": "2023",
            "abstract": "The 21st International Workshop on Treebanks and Linguistic Theories (TLT 2023) follows an annual series that started in 2002 in Sozopol, Bulgaria. TLT addresses all aspects of treebank design, development, and use.\u201cTreebank\u201d is taken in a broad sense, comprising any spoken, signed, or written data augmented with computationally processable annotations of linguistic structure at various levels. For the first time, TLT is part of GURT2023, an annual linguistics conference held at Georgetown University, which this year co-locates four related but independent events:",
            "url": "https://aclanthology.org/2023.tlt-1.0.pdf",
            "publicationVenue": "Proceedings of the 21st International Workshop on Treebanks and Linguistic Theories (TLT, GURT/SyntaxFest 2023)",
            "citation_count": 0
        },
        {
            "title": "Construction grammar provides unique insight into neural language models",
            "authors": "Leonie Weissweiler, Taiqi He, Naoki Otani, David R Mortensen, Lori Levin, Hinrich Sch\u00fctze",
            "year": "2023",
            "abstract": "Construction Grammar (CxG) has recently been used as the basis for probing studies that have investigated the performance of large pretrained language models (PLMs) with respect to the structure and meaning of constructions. In this position paper, we make suggestions for the continuation and augmentation of this line of research. We look at probing methodology that was not designed with CxG in mind, as well as probing methodology that was designed for specific constructions. We analyse selected previous work in detail, and provide our view of the most important challenges and research questions that this promising new field faces.",
            "url": "https://arxiv.org/pdf/2302.02178",
            "publicationVenue": "arXiv preprint arXiv:2302.02178",
            "citation_count": 11
        }
    ],
    "Lei Li": [
        {
            "title": "Image segmentation method and apparatus, and device, and storage medium",
            "authors": null,
            "year": "2023",
            "abstract": "Provided are an image segmentation method and apparatus, a device, and a storage medium. The image segmentation method includes: fusing a visual feature corresponding to an original image with a text feature corresponding to a description language to obtain a multimodal feature, where the description language is used for specifying a target object to be segmented in the original image; determining a visual region of the target object according to an image corresponding to the multimodal feature and recording an image corresponding to the visual region as a response heat map; and determining a segmentation result of the target object according to the image corresponding to the multimodal feature and the response heat map.",
            "url": null,
            "publicationVenue": null,
            "citation_count": 0
        },
        {
            "title": "Point cloud segmentation method and apparatus, device, and storage medium",
            "authors": null,
            "year": "2023",
            "abstract": "Provided are a point cloud segmentation method and apparatus, a device, and a storage medium. The point cloud segmentation method includes: acquiring a to-be-processed point cloud; obtaining, in a gridding scenario space to which respective point in the point cloud belongs, a target grid corresponding to the respective point through a pre-trained neural network, wherein the pre-trained neural network is obtained by training a sample point cloud and a sample target grid corresponding to the sample point cloud in a sample gridding scenario space; and outputting a point cloud corresponding to a respective instance according to an instance category corresponding to the target grid, wherein the same target grid has the same instance category.",
            "url": null,
            "publicationVenue": null,
            "citation_count": 0
        },
        {
            "title": "Learning from mistakes via cooperative study assistant for large language models",
            "authors": "Danqing Wang, Lei Li",
            "year": "2023",
            "abstract": "Large language models (LLMs) have demonstrated their potential to refine their generation based on their own feedback. However, the feedback from LLM itself is often inaccurate, thereby limiting its benefits. In this paper, we propose Study Assistant for Large LAnguage Model (SALAM), a novel framework with an auxiliary agent to assist the main LLM in learning from mistakes through interactive cooperation. In the gathering phase, the student assistant agent probes the main LLM, analyzes its errors, and collects the interaction in a mistake memory. During the examination phase, the study assistant provides guidelines by retrieving relevant cases to help the main LLM anticipate and avoid similar errors. We first investigate the effectiveness of a general study assistant and then customize it to provide LLM-specific guidance through imitation learning from successful guidance experiences. Our experiments on three LLMs using two challenging frameworks demonstrate that SALAM can significantly boost LLMs by an accuracy margin of up to 6.6 on BBH and 12.6 on BBQ.",
            "url": "https://aclanthology.org/2023.emnlp-main.659.pdf",
            "publicationVenue": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
            "citation_count": 6
        },
        {
            "title": "AutoPlan: Automatic Planning of Interactive Decision-Making Tasks With Large Language Models",
            "authors": "Siqi Ouyang, Lei Li",
            "year": "2023",
            "abstract": "Recent large language models (LLMs) are promising for making decisions in grounded environments. However, LLMs frequently fail in complex decision-making tasks due to the misalignment between the pre-trained knowledge in LLMs and the actual rules in the environment. Existing methods require either costly gradient computation or lengthy in-context demonstrations. In this paper, we propose AutoPlan, an approach to guide LLM-based agents to accomplish interactive decision-making tasks. AutoPlan augments the LLM prompt with a task-solving plan and optimizes it through iterative experience collection and reflection. Our experiments show that AutoPlan, though using no in-context demonstrations, achieves success rates on par with the baselines using human-written demonstrations on ALFWorld and even outperforms them by 8% on HotpotQA. The code is available at https://github.com/owaski/AutoPlan.",
            "url": "https://openreview.net/pdf?id=pxscU6TidP",
            "publicationVenue": "The 2023 Conference on Empirical Methods in Natural Language Processing",
            "citation_count": 1
        },
        {
            "title": "Bi-Directional Goal-Conditioning on Single Value Function for State Space Search Problems",
            "authors": "Vihaan Akshaay Rajendiran, Yu-Xiang Wang, Lei Li",
            "year": "2023",
            "abstract": "State space search problems have a binary (found/not found) reward system. In our work, we assume the ability to sample goal states and use the same to define a forward taskand a backward taskderived from the original state space search task to ensure more useful and learnable samples. Similar to Hindsight Relabelling, we define 'Foresight Relabelling' for reverse trajectories. We also use the agent's ability (from the policy function) to evaluate the reachability of intermediate states and use these states as goals for new sub-tasks. We group these tasks and sample generation strategies and make a single policy function (DQN) using goal-conditioning to learn all these different tasks and call it 'SRE-DQN\u2019 (Scrambler-Resolver-Explorer). Finally, we demonstrate the advantages of bi-directional goal-conditioning and knowledge of the goal state by evaluating our framework on classical goal-reaching tasks, and comparing with existing concepts extended to our bi-directional setting.",
            "url": "https://openreview.net/pdf?id=HvtpbQkpK1",
            "publicationVenue": "NeurIPS 2023 Workshop on Goal-Conditioned Reinforcement Learning",
            "citation_count": 0
        },
        {
            "title": "Pinpoint, not criticize: Refining large language models via fine-grained actionable feedback",
            "authors": "Wenda Xu, Daniel Deutsch, Mara Finkelstein, Juraj Juraska, Biao Zhang, Zhongtao Liu, William Yang Wang, Lei Li, Markus Freitag",
            "year": "2023",
            "abstract": "Recent improvements in text generation have leveraged human feedback to improve the quality of the generated output. However, human feedback is not always available, especially during inference. In this work, we propose an inference time optimization method FITO to use fine-grained actionable feedback in the form of error type, error location and severity level that are predicted by a learned error pinpoint model for iterative refinement. FITO starts with an initial output, then iteratively incorporates the feedback via a refinement model that generates an improved output conditioned on the feedback. Given the uncertainty of consistent refined samples at iterative steps, we formulate iterative refinement into a local search problem and develop a simulated annealing based algorithm that balances exploration of the search space and optimization for output quality. We conduct experiments on three text generation tasks, including machine translation, long-form question answering (QA) and topical summarization. We observe 0.8 and 0.7 MetricX gain on Chinese-English and English-German translation, 4.5 and 1.8 ROUGE-L gain at long form QA and topic summarization respectively, with a single iteration of refinement. With our simulated annealing algorithm, we see further quality improvements, including up to 1.7 MetricX improvements over the baseline approach.",
            "url": "https://arxiv.org/pdf/2311.09336",
            "publicationVenue": "arXiv preprint arXiv:2311.09336",
            "citation_count": 1
        },
        {
            "title": "How Multilingual is Multilingual LLM?",
            "authors": "Fei Yuan, Shuai Yuan, Zhiyong Wu, Lei Li",
            "year": "2023",
            "abstract": "Large Language Models (LLMs), trained predominantly on extensive English data, often exhibit limitations when applied to other languages. Current research is primarily focused on enhancing the multilingual capabilities of these models by employing various tuning strategies. Despite their effectiveness in certain languages, the understanding of the multilingual abilities of LLMs remains incomplete. This study endeavors to evaluate the multilingual capacity of LLMs by conducting an exhaustive analysis across 101 languages, and classifies languages with similar characteristics into four distinct quadrants. By delving into each quadrant, we shed light on the rationale behind their categorization and offer actionable guidelines for tuning these languages. Extensive experiments reveal that existing LLMs possess multilingual capabilities that surpass our expectations, and we can significantly improve the multilingual performance of LLMs by focusing on these distinct attributes present in each quadrant.",
            "url": "https://arxiv.org/pdf/2311.09071",
            "publicationVenue": "arXiv preprint arXiv:2311.09071",
            "citation_count": 1
        },
        {
            "title": "Syntax Error-Free and Generalizable Tool Use for LLMs via Finite-State Decoding",
            "authors": "Kexun Zhang, Hongqiao Chen, Lei Li, William Wang",
            "year": "2023",
            "abstract": "Large language models (LLMs) have shown promising capabilities in using external tools to solve complex problems. However, existing approaches either involve fine-tuning on tool demonstrations, which do not generalize to new tools without additional training, or providing tool documentation in context, limiting the number of tools. Both approaches often generate syntactically invalid tool calls. In this paper, we propose ToolDec, a finite-state machine-guided decoding algorithm for tool-augmented LLMs. ToolDec eliminates tool-related errors for any tool-augmented LLMs by ensuring valid tool names and type-conforming arguments. Furthermore, ToolDec enables LLM to effectively select tools using only the information contained in their names, with no need for fine-tuning or in-context documentation. We evaluated multiple prior methods and their ToolDec-enhanced versions on a variety of tasks involving tools like math functions, knowledge graph relations, and complex real-world RESTful APIs. Our experiments show that ToolDec reduces syntactic errors to zero, consequently achieving significantly better performance and as much as a 2x speedup. We also show that ToolDec achieves superior generalization performance on unseen tools, performing up to 8x better than the baselines.",
            "url": "https://arxiv.org/pdf/2310.07075",
            "publicationVenue": "arXiv preprint arXiv:2310.07075",
            "citation_count": 2
        },
        {
            "title": "Functional Geometry Guided Protein Sequence and Backbone Structure Co-Design",
            "authors": "Zhenqiao Song, Yunlong Zhao, Wenxian Shi, Yang Yang, Lei Li",
            "year": "2023",
            "abstract": "Proteins are macromolecules responsible for essential functions in almost all living organisms. Designing reasonable proteins with desired functions is crucial. A protein's sequence and structure are strongly correlated and they together determine its function. In this paper, we propose NAEPro, a model to jointly design Protein sequence and structure based on automatically detected functional sites. NAEPro is powered by an interleaving network of attention and equivariant layers, which can capture global correlation in a whole sequence and local influence from nearest amino acids in three dimensional (3D) space. Such an architecture facilitates effective yet economic message passing at two levels. We evaluate our model and several strong baselines on two protein datasets,-lactamase and myoglobin. Experimental results show that our model consistently achieves the highest amino acid recovery rate, TM-score, and the lowest RMSD among all competitors. These findings prove the capability of our model to design protein sequences and structures that closely resemble their natural counterparts. Furthermore, in-depth analysis further confirms our model's ability to generate highly effective proteins capable of binding to their target metallocofactors. We provide code, data and models in Github.",
            "url": "https://arxiv.org/html/2310.04343v3",
            "publicationVenue": "arXiv preprint arXiv:2310.04343",
            "citation_count": 0
        },
        {
            "title": "Learning personalized story evaluation",
            "authors": "Danqing Wang, Kevin Yang, Hanlin Zhu, Xiaomeng Yang, Andrew Cohen, Lei Li, Yuandong Tian",
            "year": "2023",
            "abstract": "While large language models (LLMs) have shown impressive results for more objective tasks such as QA and retrieval, it remains nontrivial to evaluate their performance on open-ended text generation for reasons including (1) data contamination; (2) multi-dimensional evaluation criteria; and (3) subjectiveness stemming from reviewers' personal preferences. To address such issues, we propose to model personalization in an uncontaminated open-ended generation assessment. We create two new datasets Per-MPST and Per-DOC for personalized story evaluation, by re-purposing existing datasets with proper anonymization and new personalized labels. We further develop a personalized story evaluation model PERSE to infer reviewer preferences and provide a personalized evaluation. Specifically, given a few exemplary reviews from a particular reviewer, PERSE predicts either a detailed review or fine-grained comparison in several aspects (such as interestingness and surprise) for that reviewer on a new text input. Experimental results show that PERSE outperforms GPT-4 by 15.8% on Kendall correlation of story ratings, and by 13.7% on pairwise preference prediction accuracy. Both datasets and code will be released at https://github.com/dqwang122/PerSE.",
            "url": "https://arxiv.org/pdf/2310.03304",
            "publicationVenue": "arXiv preprint arXiv:2310.03304",
            "citation_count": 3
        },
        {
            "title": "Joint design of protein sequence and structure based on motifs",
            "authors": "Zhenqiao Song, Yunlong Zhao, Yufei Song, Wenxian Shi, Yang Yang, Lei Li",
            "year": "2023",
            "abstract": "Designing novel proteins with desired functions is crucial in biology and chemistry. However, most existing work focus on protein sequence design, leaving protein sequence and structure co-design underexplored. In this paper, we propose GeoPro, a method to design protein backbone structure and sequence jointly. Our motivation is that protein sequence and its backbone structure constrain each other, and thus joint design of both can not only avoid nonfolding and misfolding but also produce more diverse candidates with desired functions. To this end, GeoPro is powered by an equivariant encoder for three-dimensional (3D) backbone structure and a protein sequence decoder guided by 3D geometry. Experimental results on two biologically significant metalloprotein datasets, including-lactamases and myoglobins, show that our proposed GeoPro outperforms several strong baselines on most metrics. Remarkably, our method discovers novel-lactamases and myoglobins which are not present in protein data bank (PDB) and UniProt. These proteins exhibit stable folding and active site environments reminiscent of those of natural proteins, demonstrating their excellent potential to be biologically functional.",
            "url": "https://arxiv.org/pdf/2310.02546",
            "publicationVenue": "arXiv preprint arXiv:2310.02546",
            "citation_count": 1
        },
        {
            "title": "Extrapolating large language models to non-english by aligning languages",
            "authors": "Wenhao Zhu, Yunzhe Lv, Qingxiu Dong, Fei Yuan, Jingjing Xu, Shujian Huang, Lingpeng Kong, Jiajun Chen, Lei Li",
            "year": "2023",
            "abstract": null,
            "url": null,
            "publicationVenue": "arXiv preprint arXiv:2308.04948",
            "citation_count": 7
        },
        {
            "title": "Generating Global Factual and Counterfactual Explainer for Molecule under Domain Constraints",
            "authors": "Danqing Wang, Antonis Antoniades, Ambuj Singh, Lei Li",
            "year": "2023",
            "abstract": "Graph neural networks (GNNs) are powerful tools for handling graph-structured data but often lack transparency. This paper aims to generate interpretable global explanations for GNN predictions, focusing on real-world scenarios like chemical molecules. We develop an approach that produces both factual and counterfactual explanations while incorporating domain constraints, ensuring validity and interpretability for domain experts. Our contributions include creating global explanations, integrating domain constraints, and improving random walk in global explanations using fragment-based editing. We demonstrate the effectiveness of our approach on AIDS and Mutagenicity datasets, providing a comprehensive understanding of GNNs and aiding domain experts in evaluating generated explanations.",
            "url": "https://openreview.net/pdf?id=qElXYQqxQh",
            "publicationVenue": "ICML 3rd Workshop on Interpretable Machine Learning in Healthcare (IMLH)",
            "citation_count": 0
        },
        {
            "title": "PlayGround Low Resource Machine Translation System for the 2023 AmericasNLP Shared Task",
            "authors": "Tianrui Gu, Kaie Chen, Siqi Ouyang, Lei Li",
            "year": "2023",
            "abstract": "This paper presents PlayGround\u2019s submission to the AmericasNLP 2023 shared task on machine translation (MT) into indigenous languages. We finetuned NLLB-600M, a multilingual MT model pre-trained on Flores-200, on 10 low-resource language directions and examined the effectiveness of weight averaging and back translation. Our experiments showed that weight averaging, on average, led to a 0.0169 improvement in the ChrF++ score. Additionally, we found that back translation resulted in a 0.008 improvement in the ChrF++ score.",
            "url": "https://aclanthology.org/2023.americasnlp-1.19.pdf",
            "publicationVenue": "Proceedings of the Workshop on Natural Language Processing for Indigenous Languages of the Americas (AmericasNLP)",
            "citation_count": 1
        },
        {
            "title": "SESCORE2: Learning text generation evaluation via synthesizing realistic mistakes",
            "authors": "Wenda Xu, Xian Qian, Mingxuan Wang, Lei Li, William Yang Wang",
            "year": "2023",
            "abstract": "Is it possible to train a general metric for evaluating text generation quality without human-annotated ratings? Existing learned metrics either perform unsatisfactory across text generation tasks or require human ratings for training on specific tasks. In this paper, we propose SEScore2, a self-supervised approach for training a model-based metric for text generation evaluation. The key concept is to synthesize realistic model mistakes by perturbing sentences retrieved from a corpus. We evaluate SEScore2 and previous methods on four text generation tasks across three languages. SEScore2 outperforms all prior unsupervised metrics on four text generation evaluation benchmarks, with an average Kendall improvement of 0.158. Surprisingly, SEScore2 even outperforms the supervised BLEURT and COMET on multiple text generation tasks.",
            "url": "https://aclanthology.org/2023.acl-long.283.pdf",
            "publicationVenue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
            "citation_count": 7
        },
        {
            "title": "Lego-MT: Learning Detachable Models for Massively Multilingual Machine Translation",
            "authors": "Fei Yuan, Yinquan Lu, Wenhao Zhu, Lingpeng Kong, Lei Li, Yu Qiao, Jingjing Xu",
            "year": "2023",
            "abstract": "Multilingual neural machine translation (MNMT) aims to build a unified model for many language directions. Existing monolithic models for MNMT encounter two challenges: parameter interference among languages and inefficient inference for large models. In this paper, we revisit the classic multi-way structures and develop a detachable model by assigning each language (or group of languages) to an individual branch that supports plug-and-play training and inference. To address the needs of learning representations for all languages in a unified space, we propose a novel efficient training recipe, upon which we build an effective detachable model, Lego-MT. For a fair comparison, we collect data from OPUS and build a translation benchmark covering 433 languages and 1.3 B parallel data. Experiments show that Lego-MT with 1.2 B parameters brings an average gain of 3.2 spBLEU. It even outperforms M2M-100 with 12B parameters. The proposed training recipe brings a 28.2\u00d7 speedup over the conventional multi-way training method. code and data repo: https://github. com/CONE-MT/Lego-MT. git.",
            "url": "https://aclanthology.org/2023.findings-acl.731.pdf",
            "publicationVenue": "Findings of the Association for Computational Linguistics: ACL 2023",
            "citation_count": 3
        },
        {
            "title": "Provable robust watermarking for ai-generated text",
            "authors": "Xuandong Zhao, Prabhanjan Ananth, Lei Li, Yu-Xiang Wang",
            "year": "2023",
            "abstract": "As AI-generated text increasingly resembles human-written content, the ability to detect machine-generated text becomes crucial. To address this challenge, we present GPTWatermark, a robust and high-quality solution designed to ascertain whether a piece of text originates from a specific model. Our approach extends existing watermarking strategies and employs a fixed group design to enhance robustness against editing and paraphrasing attacks. We show that our watermarked language model enjoys strong provable guarantees on generation quality, correctness in detection, and security against evasion attacks. Experimental results on various large language models (LLMs) and diverse datasets demonstrate that our method achieves superior detection accuracy and comparable generation quality in perplexity, thus promoting the responsible use of LLMs.",
            "url": "https://arxiv.org/pdf/2306.17439",
            "publicationVenue": "arXiv preprint arXiv:2306.17439",
            "citation_count": 33
        },
        {
            "title": "Converge to the truth: Factual error correction via iterative constrained editing",
            "authors": "Jiangjie Chen, Rui Xu, Wenxuan Zeng, Changzhi Sun, Lei Li, Yanghua Xiao",
            "year": "2023",
            "abstract": "Given a possibly false claim sentence, how can we automatically correct it with minimal editing? Existing methods either require a large number of pairs of false and corrected claims for supervised training or do not handle well errors spanning over multiple tokens within an utterance. In this paper, we propose VENCE, a novel method for factual error correction (FEC) with minimal edits. VENCE formulates the FEC problem as iterative sampling editing actions with respect to a target density function. We carefully design the target function with predicted truthfulness scores from an offline trained fact verification model. VENCE samples the most probable editing positions based on back-calculated gradients of the truthfulness score concerning input tokens and the editing actions using a distantly-supervised language model (T5). Experiments on a public dataset show that VENCE improves the well-adopted SARI metric by 5.3 (or a relative improvement of 11.8%) over the previous best distantly-supervised methods.",
            "url": "https://ojs.aaai.org/index.php/AAAI/article/view/26485/26257",
            "publicationVenue": "Proceedings of the AAAI Conference on Artificial Intelligence",
            "citation_count": 6
        },
        {
            "title": "Generative Autoencoders as Watermark Attackers: Analyses of Vulnerabilities and Threats",
            "authors": "Xuandong Zhao, Kexun Zhang, Yu-Xiang Wang, Lei Li",
            "year": "2023",
            "abstract": "Invisible watermarks safeguard images' copyrights by embedding hidden messages detectable by owners. It also prevents people from misusing images, especially those generated by AI models. Malicious adversaries can violate these rights by removing the watermarks. In order to remove watermarks without damaging the visual quality, the adversary needs to erase them while retaining the essential information in the image. This is analogous to the encoding and decoding process of generative autoencoders, especially variational autoencoders (VAEs) and diffusion models. We propose a framework using generative autoencoders to remove invisible watermarks and test it using VAEs and diffusions. Our results reveal that, even without specific training, off-the-shelf Stable Diffusion effectively removes most watermarks, surpassing all current attackers. The result underscores the vulnerabilities in existing watermarking schemes and calls for more robust methods for copyright protection.",
            "url": "https://arxiv.org/pdf/2306.01953",
            "publicationVenue": "arXiv preprint arXiv:2306.01953",
            "citation_count": 3
        },
        {
            "title": "A survey for in-context learning",
            "authors": "Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, Lei Li, Zhifang Sui",
            "year": "2023",
            "abstract": "With the increasing ability of large language models (LLMs), in-context learning (ICL) has become a new paradigm for natural language processing (NLP), where LLMs make predictions only based on contexts augmented with a few training examples. It has been a new trend exploring ICL to evaluate and extrapolate the ability of LLMs. In this paper, we aim to survey and summarize the progress, challenges, and future work in ICL. We first present a formal definition of ICL and clarify its correlation to related studies. Then, we organize and discuss advanced techniques of ICL, including training strategies, prompting strategies, and so on. Finally, we present the challenges of ICL and provide potential directions for further research. We hope our work can encourage more research on uncovering how ICL works and improving ICL in future work.",
            "url": "https://arxiv.org/pdf/2301.00234",
            "publicationVenue": "arXiv preprint arXiv:2301.00234",
            "citation_count": 454
        },
        {
            "title": "Vocabulary generation for neural machine translation",
            "authors": null,
            "year": "2023",
            "abstract": "Implementations of the present disclosure relate to methods, devices, and computer program products for generating a destination vocabulary from a source vocabulary. In a method, a group of candidate vocabularies are determined from the source vocabulary based on a corpus, a size of a candidate vocabulary in the group of candidate vocabularies being different from a size of the source vocabulary. A group of marginal scores are obtained for the group of candidate vocabularies, respectively, a marginal score in the group of marginal scores being obtained for the candidate vocabulary based on a corpus entropy of the candidate vocabulary and a size of the candidate vocabulary. The destination vocabulary is selected from the group of candidate vocabularies based on the group of marginal scores. With these implementations, both of the corpus entropy and the vocabulary size are considered in the vocabulary\u00a0\u2026",
            "url": null,
            "publicationVenue": null,
            "citation_count": 0
        },
        {
            "title": "Instructscore: Towards explainable text generation evaluation with automatic feedback",
            "authors": "Wenda Xu, Danqing Wang, Liangming Pan, Zhenqiao Song, Markus Freitag, William Yang Wang, Lei Li",
            "year": "2023",
            "abstract": "The field of automatic evaluation of text generation made tremendous progress in the last few years. In particular, since the advent of neural metrics, like COMET, BLEURT, and SEScore2, the newest generation of metrics show a high correlation with human judgment. Unfortunately, quality scores generated with neural metrics are not interpretable, and it is unclear which part of the generation output is criticized by the metrics. To address this limitation, we present INSTRUCTSCORE, an open-source, explainable evaluation metric for text generation. By harnessing both explicit human instruction and the implicit knowledge of GPT4, we fine-tune a LLAMA model to create an evaluative metric that can produce a diagnostic report aligned with human judgment. We evaluate INSTRUCTSCORE on the WMT22 Zh-En translation task, where our 7B model surpasses other LLM-based baselines, including those based on 175B GPT3. Impressively, our INSTRUCTSCORE, even without direct supervision from human-rated data, achieves performance levels on par with state-of-the-art metrics like COMET22, which was fine-tuned on human ratings.",
            "url": "https://arxiv.org/pdf/2305.14282",
            "publicationVenue": "arXiv preprint arXiv:2305.14282",
            "citation_count": 18
        },
        {
            "title": "Extrapolating Multilingual Understanding Models as Multilingual Generators",
            "authors": "Bohong Wu, Fei Yuan, Hai Zhao, Lei Li, Jingjing Xu",
            "year": "2023",
            "abstract": "Multilingual understanding models (or encoder-based), pre-trained via masked language modeling, have achieved promising results on many language understanding tasks (e.g., mBERT). However, these non-autoregressive (NAR) models still struggle to generate high-quality texts compared with autoregressive (AR) models. Considering that encoder-based models have the advantage of efficient generation and self-correction abilities, this paper explores methods to empower multilingual understanding models the generation abilities to get a unified model. Specifically, we start from a multilingual encoder (XLM-R) and propose a \\textbf{S}emantic-\\textbf{G}uided \\textbf{A}lignment-then-Denoising (SGA) approach to adapt an encoder to a multilingual generator with a small number of new parameters. Experiments show that the proposed approach is an effective adaption method, outperforming widely-used initialization-based methods with gains of 9.4 BLEU on machine translation, 8.1 Rouge-L on question generation, and 5.5 METEOR on story generation on XLM-R. On the other hand, we observe that XLM-R is still inferior to mBART in supervised settings despite better results on zero-shot settings, indicating that more exploration is required to make understanding models strong generators.",
            "url": "https://arxiv.org/pdf/2305.13140",
            "publicationVenue": "arXiv preprint arXiv:2305.13140",
            "citation_count": 1
        },
        {
            "title": "Say what you mean! large language models speak too positively about negative commonsense knowledge",
            "authors": "Jiangjie Chen, Wei Shi, Ziquan Fu, Sijie Cheng, Lei Li, Yanghua Xiao",
            "year": "2023",
            "abstract": "Large language models (LLMs) have been widely studied for their ability to store and utilize positive knowledge. However, negative knowledge, such as \"lions don't live in the ocean\", is also ubiquitous in the world but rarely mentioned explicitly in the text. What do LLMs know about negative knowledge? This work examines the ability of LLMs to negative commonsense knowledge. We design a constrained keywords-to-sentence generation task (CG) and a Boolean question-answering task (QA) to probe LLMs. Our experiments reveal that LLMs frequently fail to generate valid sentences grounded in negative commonsense knowledge, yet they can correctly answer polar yes-or-no questions. We term this phenomenon the belief conflict of LLMs. Our further analysis shows that statistical shortcuts and negation reporting bias from language modeling pre-training cause this conflict.",
            "url": "https://arxiv.org/pdf/2305.05976",
            "publicationVenue": "arXiv preprint arXiv:2305.05976",
            "citation_count": 18
        },
        {
            "title": "Importance Weighted Expectation-Maximization for Protein Sequence Design",
            "authors": "Zhenqiao Song, Lei Li",
            "year": "2023",
            "abstract": "Designing protein sequences with desired biological function is crucial in biology and chemistry. Recent machine learning methods use a surrogate sequence-function model to replace the expensive wet-lab validation. How can we efficiently generate diverse and novel protein sequences with high fitness? In this paper, we propose IsEM-Pro, an approach to generate protein sequences towards a given fitness criterion. At its core, IsEM-Pro is a latent generative model, augmented by combinatorial structure features from a separately learned Markov random fields (MRFs). We develop an Monte Carlo Expectation-Maximization method (MCEM) to learn the model. During inference, sampling from its latent space enhances diversity while its MRFs features guide the exploration in high fitness regions. Experiments on eight protein sequence design tasks show that our IsEM-Pro outperforms the previous best methods by at least 55% on average fitness score and generates more diverse and novel protein sequences.",
            "url": "https://arxiv.org/pdf/2305.00386",
            "publicationVenue": "arXiv preprint arXiv:2305.00386",
            "citation_count": 7
        },
        {
            "title": "Multilingual machine translation with large language models: Empirical results and analysis",
            "authors": "Wenhao Zhu, Hongyi Liu, Qingxiu Dong, Jingjing Xu, Lingpeng Kong, Jiajun Chen, Lei Li, Shujian Huang",
            "year": "2023",
            "abstract": "Large language models (LLMs) have demonstrated remarkable potential in handling multilingual machine translation (MMT). In this paper, we systematically investigate the advantages and challenges of LLMs for MMT by answering two questions: 1) How well do LLMs perform in translating a massive number of languages? 2) Which factors affect LLMs' performance in translation? We evaluate popular LLMs, including XGLM, OPT, BLOOMZ, and ChatGPT, on 102 languages. Our empirical results show that even the best model ChatGPT still lags behind the supervised baseline NLLB in 83.33% of translation directions. Through further analysis, we discover that LLMs exhibit new working patterns when used for MMT. First, prompt semantics can surprisingly be ignored when given in-context exemplars, where LLMs still show strong performance even with unreasonable prompts. Second, cross-lingual exemplars can provide better task instruction for low-resource translation than exemplars in the same language pairs. Third, we observe the overestimated performance of BLOOMZ on dataset Flores-101, indicating the potential risk when using public datasets for evaluation.",
            "url": "https://arxiv.org/pdf/2304.04675",
            "publicationVenue": "arXiv preprint arXiv:2304.04675",
            "citation_count": 83
        },
        {
            "title": "Method and device for comparing media features",
            "authors": null,
            "year": "2023",
            "abstract": "The disclosure is related to a method and device for comparing media features, the method comprising: obtaining first media feature sequences of a first media object and second media feature sequences of a second media object, the first media feature sequence comprises a plurality of first media feature units arranged in sequence, and the second media feature sequence comprises a plurality of second media feature units arranged in sequence; determining unit similarities between the first media feature units and the second media feature units; determining a similarity matrix between the first media feature sequences and the second media feature sequences according to the unit similarities; determining a similarity of the first media object and the second media object according to the similarity matrix.",
            "url": "https://patentimages.storage.googleapis.com/bb/c7/c5/ba8e1d257617b2/US20210042569A1.pdf",
            "publicationVenue": null,
            "citation_count": 0
        },
        {
            "title": "Speech translation method electronic device and computer-readable storage medium using SEQ2SEQ for determining alternative translated speech segments",
            "authors": null,
            "year": "2023",
            "abstract": "Provided are a speech translation method and apparatus, an electronic device and a storage medium. The method includes: acquiring a source speech corresponding to a to-be-translated language; acquiring a specified target language; inputting the source speech and indication information matched with the target language into a pre-trained speech translation model, where the speech translation model is configured to translate a language in a first language set into a language in a second language set, the first language set includes a plurality of languages, the first language set includes the to-be-translated language, the second language set includes a plurality of languages, and the second language set includes the target language; and acquiring a translated speech corresponding to the target language and output by the speech translation model; where the to-be-translated language is different from the\u00a0\u2026",
            "url": "https://patentimages.storage.googleapis.com/ed/97/32/88947182ad0aa8/US20210271826A1.pdf",
            "publicationVenue": null,
            "citation_count": 2
        },
        {
            "title": "Document translation method and apparatus, storage medium, and electronic device",
            "authors": null,
            "year": "2023",
            "abstract": "A document translation method includes: displaying a source text display region, a translated text region, and an editing region, wherein textual content in a document to be translated is displayed in the source text display region, and reference translated text for the textual content is displayed in the translated text region; and providing a translated text recommendation from the reference translated text according to input from a user within the editing region. The method further includes: displaying the translation recommendation in the editing area as a translation result, if a confirmation operation for the translation recommendation is detected; and receiving a translation inputted by the user that is different from the translation recommendation and displaying the translation inputted by the user in the editing area as the translation result, if a non-confirmation operation for the translation recommendation is detected.",
            "url": "https://patentimages.storage.googleapis.com/a0/6c/5c/2ce5e0e80923a4/US20220374617A1.pdf",
            "publicationVenue": null,
            "citation_count": 0
        },
        {
            "title": "Protecting language generation models via invisible watermarking",
            "authors": "Xuandong Zhao, Yu-Xiang Wang, Lei Li",
            "year": "2023",
            "abstract": "Language generation models have been an increasingly powerful enabler for many applications. Many such models offer free or affordable API access, which makes them potentially vulnerable to model extraction attacks through distillation. To protect intellectual property (IP) and ensure fair use of these models, various techniques such as lexical watermarking and synonym replacement have been proposed. However, these methods can be nullified by obvious countermeasures such as \"synonym randomization\". To address this issue, we propose GINSEW, a novel method to protect text generation models from being stolen through distillation. The key idea of our method is to inject secret signals into the probability vector of the decoding steps for each target token. We can then detect the secret message by probing a suspect model to tell if it is distilled from the protected one. Experimental results show that GINSEW can effectively identify instances of IP infringement with minimal impact on the generation quality of protected APIs. Our method demonstrates an absolute improvement of 19 to 29 points on mean average precision (mAP) in detecting suspects compared to previous methods against watermark removal attacks.",
            "url": "https://arxiv.org/pdf/2302.03162",
            "publicationVenue": "arXiv preprint arXiv:2302.03162",
            "citation_count": 34
        },
        {
            "title": "ReDi: Efficient Learning-Free Diffusion Inference via Trajectory Retrieval",
            "authors": "Kexun Zhang, Xianjun Yang, William Yang Wang, Lei Li",
            "year": "2023",
            "abstract": "Diffusion models show promising generation capability for a variety of data. Despite their high generation quality, the inference for diffusion models is still time-consuming due to the numerous sampling iterations required. To accelerate the inference, we propose ReDi, a simple yet learning-free Retrieval-based Diffusion sampling framework. From a precomputed knowledge base, ReDi retrieves a trajectory similar to the partially generated trajectory at an early stage of generation, skips a large portion of intermediate steps, and continues sampling from a later step in the retrieved trajectory. We theoretically prove that the generation performance of ReDi is guaranteed. Our experiments demonstrate that ReDi improves the model inference efficiency by 2x speedup. Furthermore, ReDi is able to generalize well in zero-shot cross-domain image generation such as image stylization.",
            "url": "https://arxiv.org/pdf/2302.02285",
            "publicationVenue": "arXiv preprint arXiv:2302.02285",
            "citation_count": 0
        }
    ],
    "Teruko Mitamura": [
        {
            "title": "Overview of the NTCIR-17 QA Lab-PoliInfo-4 Task",
            "authors": "Yasuhiro Ogawa, Yasutomo Kimura, Hideyuki Shibuki, Hokuto Ototake, Yuzu Uchida, Keiichi Takamaru, Kazuma Kadowaki, Tomoyoshi Akiba, Minoru Sasaki, Akio Kobayashi, Masaharu Yoshioka, Tatsunori Mori, Kenji Araki, Teruko Mitamura",
            "year": "2023",
            "abstract": "The goal of the NTCIR-17 QA Lab-PoliInfo-4 task is to develop real-world complex question answering (QA) techniques using Japanese political information such as local assembly minutes and newsletters. QA Lab-PoliInfo-4 consists of four subtasks: Question Answering-2, Answer Verification, Stance Classification-2, and Minutes-to-Budget Linking. In this paper, we present the data used and the results of QA Lab-PoliInfo-4\u2019s formal run.",
            "url": "http://research.nii.ac.jp/ntcir/workshop/OnlineProceedings17/pdf/ntcir/01-NTCIR17-OV-QALAB-OgawaY.pdf",
            "publicationVenue": "Proceedings of",
            "citation_count": 1
        },
        {
            "title": "Language-agnostic transformers and assessing ChatGPT-based query rewriting for multilingual document-grounded QA",
            "authors": "Srinivas Gowriraj, Soham Dinesh Tiwari, Mitali Potnis, Srijan Bansal, Teruko Mitamura, Eric Nyberg",
            "year": "2023",
            "abstract": "The DialDoc 2023 shared task has expanded the document-grounded dialogue task to encompass multiple languages, despite having limited annotated data. This paper assesses the effectiveness of both language-agnostic and language-aware paradigms for multilingual pre-trained transformer models in a bi-encoder-based dense passage retriever (DPR), concluding that the language-agnostic approach is superior. Additionally, the study investigates the impact of query rewriting techniques using large language models, such as ChatGPT, on multilingual, document-grounded question-answering systems. The experiments conducted demonstrate that, for the examples examined, query rewriting does not enhance performance compared to the original queries. This failure is due to topic switching in final dialogue turns and irrelevant topics being considered for query rewriting.",
            "url": "https://aclanthology.org/2023.dialdoc-1.11.pdf",
            "publicationVenue": "Proceedings of the Third DialDoc Workshop on Document-grounded Dialogue and Conversational Question Answering",
            "citation_count": 2
        },
        {
            "title": "ChartReader: A Unified Framework for Chart Derendering and Comprehension without Heuristic Rules",
            "authors": "Zhi-Qi Cheng, Qi Dai, Siyao Li, Jingdong Sun, Teruko Mitamura, Alexander G Hauptmann",
            "year": "2023",
            "abstract": "Charts are a powerful tool for visually conveying complex data, but their comprehension poses a challenge due to the diverse chart types and intricate components. Existing chart comprehension methods suffer from either heuristic rules or an over-reliance on OCR systems, resulting in suboptimal performance. To address these issues, we present ChartReader, a unified framework that seamlessly integrates chart derendering and comprehension tasks. Our approach includes a transformer-based chart component detection module and an extended pre-trained vision-language model for chart-to-X tasks. By learning the rules of charts automatically from annotated datasets, our approach eliminates the need for manual rule-making, reducing effort and enhancing accuracy.~We also introduce a data variable replacement technique and extend the input and position embeddings of the pre-trained model for cross-task training. We evaluate ChartReader on Chart-to-Table, ChartQA, and Chart-to-Text tasks, demonstrating its superiority over existing methods. Our proposed framework can significantly reduce the manual effort involved in chart analysis, providing a step towards a universal chart understanding model. Moreover, our approach offers opportunities for plug-and-play integration with mainstream LLMs such as T5 and TaPas, extending their capability to chart comprehension tasks. The code is available at https://github.com/zhiqic/ChartReader.",
            "url": "https://arxiv.org/pdf/2304.02173",
            "publicationVenue": "arXiv preprint arXiv:2304.02173",
            "citation_count": 2
        },
        {
            "title": "Hierarchical Event Grounding",
            "authors": "Jiefu Ou, Adithya Pratapa, Rishubh Gupta, Teruko Mitamura",
            "year": "2023",
            "abstract": "Event grounding aims at linking mention references in text corpora to events from a knowledge base (KB). Previous work on this task focused primarily on linking to a single KB event, thereby overlooking the hierarchical aspects of events. Events in documents are typically described at various levels of spatio-temporal granularity (Glavas et al. 2014). These hierarchical relations are utilized in downstream tasks of narrative understanding and schema construction. In this work, we present an extension to the event grounding task that requires tackling hierarchical event structures from the KB. Our proposed task involves linking a mention reference to a set of event labels from a subevent hierarchy in the KB. We propose a retrieval methodology that leverages event hierarchy through an auxiliary hierarchical loss (Murty et al. 2018). On an automatically created multilingual dataset from Wikipedia and Wikidata, our experiments demonstrate the effectiveness of the hierarchical loss against retrieve and re-rank baselines (Wu et al. 2020; Pratapa, Gupta, and Mitamura 2022). Furthermore, we demonstrate the systems' ability to aid hierarchical discovery among unseen events.",
            "url": "https://arxiv.org/pdf/2302.04197",
            "publicationVenue": "arXiv preprint arXiv:2302.04197",
            "citation_count": 3
        },
        {
            "title": "Biomedical Question Answering with Transformer Ensembles",
            "authors": "Teruko Mitamura",
            "year": "2023",
            "abstract": "Recent advancements in natural language processing, specifically transformers, have shown great promise in improving the performance of question-answering systems. However, we observe that a single transformer model may not achieve sufficient accuracy and reliability to meet the stringent requirements of biomedical question answering. Based on our participation in the BioASQ Challenge, we present a comprehensive approach for biomedical question answering using transformers, integrating an end-to-end data processing pipeline with the UMLS Metamap and different ensembling techniques. Our findings suggest that transformer ensembles achieve significant performance improvements when compared to individual models.",
            "url": "https://ceur-ws.org/Vol-3497/paper-014.pdf",
            "publicationVenue": null,
            "citation_count": 0
        }
    ],
    "Louis-Philippe Morency": [
        {
            "title": "Mixture of Multimodal Interaction Experts",
            "authors": "Haofei Yu, Paul Pu Liang, Ruslan Salakhutdinov, Louis-Philippe Morency",
            "year": "2023",
            "abstract": "Multimodal machine learning, which studies the information and interactions across various input modalities, has made significant advancements in understanding the relationship between images and descriptive text. Yet, this is just a portion of the potential multimodal interactions in the real world, such as sarcasm in conflicting utterance and gestures. Notably, the current methods for capturing this shared information often don't extend well to these more nuanced interactions. Current models, in fact, show particular weaknesses with disagreement and synergistic interactions, sometimes performing as low as 50\\% in binary classification. In this paper, we address this problem via a new approach called mixture of multimodal interaction experts. This method automatically classifies datapoints from unlabeled multimodal dataset by their intereaction types, then employs specialized models for each specific interaction. Based on our experiments, this approach has improved performance on these challenging interactions to more than 10%, leading to an overall increase of 2% for tasks like sarcasm prediction. As a result, not only does interaction quantification provide new insights for dataset analysis, but also simple approaches to obtain state-of-the-art performance.",
            "url": "https://openreview.net/pdf?id=WuFcps7SNL",
            "publicationVenue": "UniReps: the First Workshop on Unifying Representations in Neural Models",
            "citation_count": 0
        },
        {
            "title": "Think Twice: Perspective-Taking Improves Large Language Models' Theory-of-Mind Capabilities",
            "authors": "Alex Wilf, Sihyun Shawn Lee, Paul Pu Liang, Louis-Philippe Morency",
            "year": "2023",
            "abstract": "Human interactions are deeply rooted in the interplay of thoughts, beliefs, and desires made possible by Theory of Mind (ToM): our cognitive ability to understand the mental states of ourselves and others. Although ToM may come naturally to us, emulating it presents a challenge to even the most advanced Large Language Models (LLMs). Recent improvements to LLMs' reasoning capabilities from simple yet effective prompting techniques such as Chain-of-Thought have seen limited applicability to ToM. In this paper, we turn to the prominent cognitive science theory \"Simulation Theory\" to bridge this gap. We introduce SimToM, a novel two-stage prompting framework inspired by Simulation Theory's notion of perspective-taking. To implement this idea on current ToM benchmarks, SimToM first filters context based on what the character in question knows before answering a question about their mental state. Our approach, which requires no additional training and minimal prompt-tuning, shows substantial improvement over existing methods, and our analysis reveals the importance of perspective-taking to Theory-of-Mind capabilities. Our findings suggest perspective-taking as a promising direction for future research into improving LLMs' ToM capabilities.",
            "url": "https://arxiv.org/pdf/2311.10227",
            "publicationVenue": "arXiv preprint arXiv:2311.10227",
            "citation_count": 2
        },
        {
            "title": "MMOE: Mixture of Multimodal Interaction Experts",
            "authors": "Haofei Yu, Paul Pu Liang, Ruslan Salakhutdinov, Louis-Philippe Morency",
            "year": "2023",
            "abstract": "Multimodal machine learning, which studies the information and interactions across various input modalities, has made significant advancements in understanding the relationship between images and descriptive text. However, this is just a portion of the potential multimodal interactions seen in the real world and does not include new interactions between conflicting utterances and gestures in predicting sarcasm, for example. Notably, the current methods for capturing shared information often do not extend well to these more nuanced interactions, sometimes performing as low as 50% in binary classification. In this paper, we address this problem via a new approach called MMOE, which stands for a mixture of multimodal interaction experts. Our method automatically classifies data points from unlabeled multimodal datasets by their interaction type and employs specialized models for each specific interaction. Based on our experiments, this approach improves performance on these challenging interactions by more than 10%, leading to an overall increase of 2% for tasks like sarcasm prediction. As a result, interaction quantification provides new insights for dataset analysis and yields simple approaches that obtain state-of-the-art performance.",
            "url": "https://arxiv.org/pdf/2311.09580",
            "publicationVenue": "arXiv preprint arXiv:2311.09580",
            "citation_count": 0
        },
        {
            "title": "MultiIoT: Towards Large-scale Multisensory Learning for the Internet of Things",
            "authors": "Shentong Mo, Paul Pu Liang, Russ Salakhutdinov, Louis-Philippe Morency",
            "year": "2023",
            "abstract": "The Internet of Things (IoT), the network integrating billions of smart physical devices embedded with sensors, software, and communication technologies for the purpose of connecting and exchanging data with other devices and systems, is a critical and rapidly expanding component of our modern world. The IoT ecosystem provides a rich source of real-world modalities such as motion, thermal, geolocation, imaging, depth, sensors, video, and audio for prediction tasks involving the pose, gaze, activities, and gestures of humans as well as the touch, contact, pose, 3D of physical objects. Machine learning presents a rich opportunity to automatically process IoT data at scale, enabling efficient inference for impact in understanding human wellbeing, controlling physical devices, and interconnecting smart cities. To develop machine learning technologies for IoT, this paper proposes MultiIoT, the most expansive IoT benchmark to date, encompassing over 1.15 million samples from 12 modalities and 8 tasks. MultiIoT introduces unique challenges involving (1) learning from many sensory modalities, (2) fine-grained interactions across long temporal ranges, and (3) extreme heterogeneity due to unique structure and noise topologies in real-world sensors. We also release a set of strong modeling baselines, spanning modality and task-specific methods to multisensory and multitask models to encourage future research in multisensory representation learning for IoT.",
            "url": "https://arxiv.org/pdf/2311.06217",
            "publicationVenue": "arXiv preprint arXiv:2311.06217",
            "citation_count": 0
        },
        {
            "title": "Comparative Knowledge Distillation",
            "authors": "Alex Wilf, Alex Tianyi Xu, Paul Pu Liang, Alexander Obolenskiy, Daniel Fried, Louis-Philippe Morency",
            "year": "2023",
            "abstract": "In the era of large scale pretrained models, Knowledge Distillation (KD) serves an important role in transferring the wisdom of computationally heavy teacher models to lightweight, efficient student models while preserving performance. Traditional KD paradigms, however, assume readily available access to teacher models for frequent inference -- a notion increasingly at odds with the realities of costly, often proprietary, large scale models. Addressing this gap, our paper considers how to minimize the dependency on teacher model inferences in KD in a setting we term Few Teacher Inference Knowledge Distillation (FTI KD). We observe that prevalent KD techniques and state of the art data augmentation strategies fall short in this constrained setting. Drawing inspiration from educational principles that emphasize learning through comparison, we propose Comparative Knowledge Distillation (CKD), which encourages student models to understand the nuanced differences in a teacher model's interpretations of samples. Critically, CKD provides additional learning signals to the student without making additional teacher calls. We also extend the principle of CKD to groups of samples, enabling even more efficient learning from limited teacher calls. Empirical evaluation across varied experimental settings indicates that CKD consistently outperforms state of the art data augmentation and KD techniques.",
            "url": "https://arxiv.org/pdf/2311.02253",
            "publicationVenue": "arXiv preprint arXiv:2311.02253",
            "citation_count": 0
        },
        {
            "title": "Text-transport: Toward learning causal effects of natural language",
            "authors": "Victoria Lin, Louis-Philippe Morency, Eli Ben-Michael",
            "year": "2023",
            "abstract": "As language technologies gain prominence in real-world settings, it is important to understand how changes to language affect reader perceptions. This can be formalized as the causal effect of varying a linguistic attribute (e.g., sentiment) on a reader's response to the text. In this paper, we introduce Text-Transport, a method for estimation of causal effects from natural language under any text distribution. Current approaches for valid causal effect estimation require strong assumptions about the data, meaning the data from which one can estimate valid causal effects often is not representative of the actual target domain of interest. To address this issue, we leverage the notion of distribution shift to describe an estimator that transports causal effects between domains, bypassing the need for strong assumptions in the target domain. We derive statistical guarantees on the uncertainty of this estimator, and we report empirical results and analyses that support the validity of Text-Transport across data settings. Finally, we use Text-Transport to study a realistic setting--hate speech on social media--in which causal effects do shift significantly between text domains, demonstrating the necessity of transport when conducting causal inference on natural language.",
            "url": "https://arxiv.org/pdf/2310.20697",
            "publicationVenue": "arXiv preprint arXiv:2310.20697",
            "citation_count": 2
        },
        {
            "title": "Sotopia: Interactive evaluation for social intelligence in language agents",
            "authors": "Xuhui Zhou, Hao Zhu, Leena Mathur, Ruohong Zhang, Haofei Yu, Zhengyang Qi, Louis-Philippe Morency, Yonatan Bisk, Daniel Fried, Graham Neubig, Maarten Sap",
            "year": "2023",
            "abstract": "Humans are social beings; we pursue social goals in our daily interactions, which is a crucial aspect of social intelligence. Yet, AI systems' abilities in this realm remain elusive. We present SOTOPIA, an open-ended environment to simulate complex social interactions between artificial agents and evaluate their social intelligence. In our environment, agents role-play and interact under a wide variety of scenarios; they coordinate, collaborate, exchange, and compete with each other to achieve complex social goals. We simulate the role-play interaction between LLM-based agents and humans within this task space and evaluate their performance with a holistic evaluation framework called SOTOPIA-Eval. With SOTOPIA, we find significant differences between these models in terms of their social intelligence, and we identify a subset of SOTOPIA scenarios, SOTOPIA-hard, that is generally challenging for all models. We find that on this subset, GPT-4 achieves a significantly lower goal completion rate than humans and struggles to exhibit social commonsense reasoning and strategic communication skills. These findings demonstrate SOTOPIA's promise as a general platform for research on evaluating and improving social intelligence in artificial agents.",
            "url": "https://arxiv.org/pdf/2310.11667",
            "publicationVenue": "arXiv preprint arXiv:2310.11667",
            "citation_count": 13
        },
        {
            "title": "Quantifying Interactions in Semi-supervised Multimodal Learning: Guarantees and Applications",
            "authors": "Paul Pu Liang, Chun Kai Ling, Yun Cheng, Alexander Obolenskiy, Yudong Liu, Rohan Pandey, Alex Wilf, Louis-Philippe Morency, Russ Salakhutdinov",
            "year": "2023",
            "abstract": "In many machine learning systems that jointly learn from multiple modalities, a core research question is to understand the nature of multimodal interactions: how modalities combine to provide new task-relevant information that was not present in either alone. We study this challenge of interaction quantification in a semi-supervised setting with only labeled unimodal data and naturally co-occurring multimodal data (e.g., unlabeled images and captions, video and corresponding audio) but when labeling them is time-consuming. Using a precise information-theoretic definition of interactions, our key contribution is the derivation of lower and upper bounds to quantify the amount of multimodal interactions in this semi-supervised setting. We propose two lower bounds: one based on the shared information between modalities and the other based on disagreement between separately trained unimodal classifiers, and derive an upper bound through connections to approximate algorithms for min-entropy couplings. We validate these estimated bounds and show how they accurately track true interactions. Finally, we show how these theoretical results can be used to estimate multimodal model performance, guide data collection, and select appropriate multimodal models for various tasks.",
            "url": "https://openreview.net/pdf?id=BrjLHbqiYs",
            "publicationVenue": "The Twelfth International Conference on Learning Representations",
            "citation_count": 0
        },
        {
            "title": "ICMI'23: Proceedings of the 25th International Conference on Multimodal Interaction",
            "authors": "Elisabeth Andre, Mohamed Chetouani, Dominique Vaufreydaz, Gale Lucas, Tanja Schultz, Louis-Philippe Morency, Alessandro Vinciarelli",
            "year": "2023",
            "abstract": null,
            "url": null,
            "publicationVenue": null,
            "citation_count": 0
        },
        {
            "title": "Companion Publication of the 25th International Conference on Multimodal Interaction",
            "authors": "Elisabeth Andre, Mohamed Chetouani, Dominique Vaufreydaz, Gale Lucas, Tanja Schultz, Louis-Philippe Morency, Alessandro Vinciarelli",
            "year": "2023",
            "abstract": null,
            "url": null,
            "publicationVenue": null,
            "citation_count": 0
        },
        {
            "title": "Tutorial on Multimodal Machine Learning: Principles, Challenges, and Open Questions",
            "authors": "Paul Pu Liang, Louis-Philippe Morency",
            "year": "2023",
            "abstract": "Multimodal machine learning is a vibrant multi-disciplinary research field that aims to design computer agents capable of understanding, reasoning, and learning through integrating multiple communicative modalities, including linguistic, acoustic, visual, tactile, and physiological messages. With the recent interest in video understanding, embodied autonomous agents, text-to-image generation, and multisensor fusion in healthcare and robotics, multimodality has brought unique computational and theoretical challenges to the machine learning community given the heterogeneity of data sources and the interconnections often found between modalities. However, the breadth of progress in multimodal research has made it difficult to identify the common themes and open questions in the field. By synthesizing a broad range of application domains and theoretical frameworks from both historical and recent perspectives\u00a0\u2026",
            "url": null,
            "publicationVenue": "Companion Publication of the 25th International Conference on Multimodal Interaction",
            "citation_count": 0
        },
        {
            "title": "SHAP-based Prediction of Mother's History of Depression to Understand the Influence on Child Behavior",
            "authors": "Maneesh Bilalpur, Saurabh Hinduja, Laura Cariola, Lisa Sheeber, Nicholas Allen, Louis-Philippe Morency, Jeffrey F Cohn",
            "year": "2023",
            "abstract": "Depression strongly impacts parents\u2019 behavior. Does parents\u2019 depression strongly affect the behavior of their children as well? To investigate this question, we compared dyadic interactions between 73 depressed and 75 non-depressed mothers and their adolescent child. Families were of low income and 84% were white. Child behavior was measured from audio-video recordings using manual annotation of verbal and nonverbal behavior by expert coders and by multimodal computational measures of facial expression, face and head dynamics, prosody, speech behavior, and linguistics. For both sets of measures, we used Support Vector Machines. For computational measures, we investigated the relative contribution of single versus multiple modalities using a novel approach to SHapley Additive exPlanations (SHAP). Computational measures outperformed manual ratings by human experts. Among individual\u00a0\u2026",
            "url": "https://dl.acm.org/doi/pdf/10.1145/3577190.3614136",
            "publicationVenue": "Proceedings of the 25th International Conference on Multimodal Interaction",
            "citation_count": 0
        },
        {
            "title": "Representation Learning for Interpersonal and Multimodal Behavior Dynamics: A Multiview Extension of Latent Change Score Models",
            "authors": "Alexandria K Vail, Jeffrey M Girard, Lauren M Bylsma, Jay Fournier, Holly A Swartz, Jeffrey F Cohn, Louis-Philippe Morency",
            "year": "2023",
            "abstract": "Characterizing the dynamics of behavior across multiple modalities and individuals is a vital component of computational behavior analysis. This is especially important in certain applications, such as psychotherapy, where individualized tracking of behavior patterns can provide valuable information about the patient\u2019s mental state. Conventional methods that rely on aggregate statistics and correlational metrics may not always suffice, as they are often unable to capture causal relationships or evaluate the true probability of identified patterns. To address these challenges, we present a novel approach to learning multimodal and interpersonal representations of behavior dynamics during one-on-one interaction. Our approach is enabled by the introduction of a multiview extension of latent change score models, which facilitates the concurrent capture of both inter-modal and interpersonal behavior dynamics and the\u00a0\u2026",
            "url": "https://dl.acm.org/doi/pdf/10.1145/3577190.3614118",
            "publicationVenue": "Proceedings of the 25th International Conference on Multimodal Interaction",
            "citation_count": 1
        },
        {
            "title": "Neural mixed effects for nonlinear personalized predictions",
            "authors": "Torsten W\u00f6rtwein, Nicholas B Allen, Lisa B Sheeber, Randy P Auerbach, Jeffrey F Cohn, Louis-Philippe Morency",
            "year": "2023",
            "abstract": "Personalized prediction is a machine learning approach that predicts a person\u2019s future observations based on their past labeled observations and is typically used for sequential tasks, e.g., to predict daily mood ratings. When making personalized predictions, a model can combine two types of trends: (a) trends shared across people, i.e., person-generic trends, such as being happier on weekends, and (b) unique trends for each person, i.e., person-specific trends, such as a stressful weekly meeting. Mixed effect models are popular statistical models to study both trends by combining person-generic and person-specific parameters. Though linear mixed effect models are gaining popularity in machine learning by integrating them with neural networks, these integrations are currently limited to linear person-specific parameters: ruling out nonlinear person-specific trends. In this paper, we propose Neural Mixed Effect\u00a0\u2026",
            "url": "https://dl.acm.org/doi/pdf/10.1145/3577190.3614115",
            "publicationVenue": "Proceedings of the 25th International Conference on Multimodal Interaction",
            "citation_count": 2
        },
        {
            "title": "Expanding the Role of Affective Phenomena in Multimodal Interaction Research",
            "authors": "Leena Mathur, Maja Mataric, Louis-Philippe Morency",
            "year": "2023",
            "abstract": "In recent decades, the field of affective computing has made substantial progress in advancing the ability of AI systems to recognize and express affective phenomena, such as affect and emotions, during human-human and human-machine interactions. This paper describes our examination of research at the intersection of multimodal interaction and affective computing, with the objective of observing trends and identifying understudied areas. We examined over 16,000 papers from selected conferences in multimodal interaction, affective computing, and natural language processing: ACM International Conference on Multimodal Interaction, AAAC International Conference on Affective Computing and Intelligent Interaction, Annual Meeting of the Association for Computational Linguistics, and Conference on Empirical Methods in Natural Language Processing. We identified 910 affect-related papers and present our\u00a0\u2026",
            "url": "https://dl.acm.org/doi/pdf/10.1145/3577190.3614171",
            "publicationVenue": "Proceedings of the 25th International Conference on Multimodal Interaction",
            "citation_count": 0
        },
        {
            "title": "Reconstructing the neurodynamics of face perception during real world vision in humans using intracranial EEG recordings",
            "authors": "Arish Alreja, Michael J Ward, Jhair A Colan, Qianli Ma, R Mark Richardson, Louis-Phillipe Morency, Avniel S Ghuman",
            "year": "2023",
            "abstract": "We use face perception to see and understand people around us during natural behavior in the real world. Here, we take advantage of the unique opportunity afforded by intracranial recordings in two epilepsy patients to assess the neural basis of face perception during natural, unscripted interactions in real world settings with friends, family, and experimenters. With eye tracking glasses, we captured what subjects saw, time locked to the corresponding neural activity, on a fixation-by-fixation basis for hours during these interactions. We restricted the analysis to face fixations annotated using a combination of manual annotations and computer vision. After training a bidirectional Canonical Component Analysis (CCA) model on training fixations, we sought to reconstruct an image of the face people were seeing based on the corresponding pattern of neural activity, and reconstruct an image of the neural activity based\u00a0\u2026",
            "url": null,
            "publicationVenue": "Journal of Vision",
            "citation_count": 0
        },
        {
            "title": "A new paradigm for investigating real-world social behavior and its neural underpinnings",
            "authors": "Arish Alreja, Michael J Ward, Qianli Ma, Brian E Russ, Stephan Bickel, Nelleke C Van Wouwe, Jorge A Gonz\u00e1lez-Mart\u00ednez, Joseph S Neimat, Taylor J Abel, Anto Bagi\u0107, Lisa S Parker, R Mark Richardson, Charles E Schroeder, Louis\u2013Philippe Morency, Avniel Singh Ghuman",
            "year": "2023",
            "abstract": "Eye tracking and other behavioral measurements collected from patient-participants in their hospital rooms afford a unique opportunity to study natural behavior for basic and clinical translational research. We describe an immersive social and behavioral paradigm implemented in patients undergoing evaluation for surgical treatment of epilepsy, with electrodes implanted in the brain to determine the source of their seizures. Our studies entail collecting eye tracking with other behavioral and psychophysiological measurements from patient-participants during unscripted behavior, including social interactions with clinical staff, friends, and family in the hospital room. This approach affords a unique opportunity to study the neurobiology of natural social behavior, though it requires carefully addressing distinct logistical, technical, and ethical challenges. Collecting neurophysiological data synchronized to behavioral and\u00a0\u2026",
            "url": "https://link.springer.com/article/10.3758/s13428-022-01882-9",
            "publicationVenue": "Behavior research methods",
            "citation_count": 1
        },
        {
            "title": "Multimodal Learning Without Labeled Multimodal Data: Guarantees and Applications",
            "authors": "Paul Pu Liang, Chun Kai Ling, Yun Cheng, Alex Obolenskiy, Yudong Liu, Rohan Pandey, Alex Wilf, Louis-Philippe Morency, Ruslan Salakhutdinov",
            "year": "2023",
            "abstract": "In many machine learning systems that jointly learn from multiple modalities, a core research question is to understand the nature of multimodal interactions: the emergence of new task-relevant information during learning from both modalities that was not present in either alone. We study this challenge of interaction quantification in a semi-supervised setting with only labeled unimodal data and naturally co-occurring multimodal data (e.g., unlabeled images and captions, video and corresponding audio) but when labeling them is time-consuming. Using a precise information-theoretic definition of interactions, our key contributions are the derivations of lower and upper bounds to quantify the amount of multimodal interactions in this semi-supervised setting. We propose two lower bounds based on the amount of shared information between modalities and the disagreement between separately trained unimodal classifiers, and derive an upper bound through connections to approximate algorithms for min-entropy couplings. We validate these estimated bounds and show how they accurately track true interactions. Finally, two semi-supervised multimodal applications are explored based on these theoretical results: (1) analyzing the relationship between multimodal performance and estimated interactions, and (2) self-supervised learning that embraces disagreement between modalities beyond agreement as is typically done.",
            "url": "https://arxiv.org/pdf/2306.04539",
            "publicationVenue": "arXiv preprint arXiv:2306.04539",
            "citation_count": 3
        },
        {
            "title": "Multimodal Fusion Interactions: A Study of Human and Automatic Quantification",
            "authors": "Paul Pu Liang, Yun Cheng, Ruslan Salakhutdinov, Louis-Philippe Morency",
            "year": "2023",
            "abstract": "Multimodal fusion of multiple heterogeneous and interconnected signals is a fundamental challenge in almost all multimodal problems and applications. In order to perform multimodal fusion, we need to understand the types of interactions that modalities can exhibit: how each modality individually provides information useful for a task and how this information changes in the presence of other modalities. In this paper, we perform a comparative study of how human annotators can be leveraged to annotate two categorizations of multimodal interactions: (1) partial labels, where different randomly assigned annotators annotate the label given the first, second, and both modalities, and (2) counterfactual labels, where the same annotator is tasked to annotate the label given the first modality before giving them the second modality and asking them to explicitly reason about how their answer changes, before proposing an alternative taxonomy based on (3) information decomposition, where annotators annotate the degrees of redundancy: the extent to which modalities individually and together give the same predictions on the task, uniqueness: the extent to which one modality enables a task prediction that the other does not, and synergy: the extent to which only both modalities enable one to make a prediction about the task that one would not otherwise make using either modality individually. Through extensive experiments and annotations, we highlight several opportunities and limitations of each approach and propose a method to automatically convert annotations of partial and counterfactual labels to information decomposition, yielding an accurate\u00a0\u2026",
            "url": "https://arxiv.org/pdf/2306.04125",
            "publicationVenue": "arXiv preprint arXiv:2306.04125",
            "citation_count": 1
        },
        {
            "title": "Language Models Get a Gender Makeover: Mitigating Gender Bias with Few-Shot Data Interventions",
            "authors": "Himanshu Thakur, Atishay Jain, Praneetha Vaddamanu, Paul Pu Liang, Louis-Philippe Morency",
            "year": "2023",
            "abstract": "Societal biases present in pre-trained large language models are a critical issue as these models have been shown to propagate biases in countless downstream applications, rendering them unfair towards specific groups of people. Since large-scale retraining of these models from scratch is both time and compute-expensive, a variety of approaches have been previously proposed that de-bias a pre-trained model. While the majority of current state-of-the-art debiasing methods focus on changes to the training regime, in this paper, we propose data intervention strategies as a powerful yet simple technique to reduce gender bias in pre-trained models. Specifically, we empirically show that by fine-tuning a pre-trained model on only 10 de-biased (intervened) training examples, the tendency to favor any gender is significantly reduced. Since our proposed method only needs a few training examples, our few-shot debiasing approach is highly feasible and practical. Through extensive experimentation, we show that our debiasing technique performs better than competitive state-of-the-art baselines with minimal loss in language modeling ability.",
            "url": "https://arxiv.org/pdf/2306.04597",
            "publicationVenue": "arXiv preprint arXiv:2306.04597",
            "citation_count": 8
        },
        {
            "title": "SenteCon: Leveraging Lexicons to Learn Human-Interpretable Language Representations",
            "authors": "Victoria Lin, Louis-Philippe Morency",
            "year": "2023",
            "abstract": "Although deep language representations have become the dominant form of language featurization in recent years, in many settings it is important to understand a model's decision-making process. This necessitates not only an interpretable model but also interpretable features. In particular, language must be featurized in a way that is interpretable while still characterizing the original text well. We present SenteCon, a method for introducing human interpretability in deep language representations. Given a passage of text, SenteCon encodes the text as a layer of interpretable categories in which each dimension corresponds to the relevance of a specific category. Our empirical evaluations indicate that encoding language with SenteCon provides high-level interpretability at little to no cost to predictive performance on downstream tasks. Moreover, we find that SenteCon outperforms existing interpretable language representations with respect to both its downstream performance and its agreement with human characterizations of the text.",
            "url": "https://arxiv.org/pdf/2305.14728",
            "publicationVenue": "arXiv preprint arXiv:2305.14728",
            "citation_count": 0
        },
        {
            "title": "Difference-Masking: Choosing What to Mask in Continued Pretraining",
            "authors": "Alex Wilf, Syeda Nahida Akter, Leena Mathur, Paul Pu Liang, Sheryl Mathew, Mengrou Shou, Eric Nyberg, Louis-Philippe Morency",
            "year": "2023",
            "abstract": "Self-supervised learning (SSL) and the objective of masking-and-predicting in particular have led to promising SSL performance on a variety of downstream tasks. However, while most approaches randomly mask tokens, there is strong intuition from the field of education that deciding what to mask can substantially improve learning outcomes. We introduce Difference-Masking, an approach that automatically chooses what to mask during continued pretraining by considering what makes an unlabelled target domain different from the pretraining domain. Empirically, we find that Difference-Masking outperforms baselines on continued pretraining settings across four diverse language and multimodal video tasks. The cross-task applicability of Difference-Masking supports the effectiveness of our framework for SSL pretraining in language, vision, and other domains.",
            "url": "https://arxiv.org/pdf/2305.14577",
            "publicationVenue": "arXiv preprint arXiv:2305.14577",
            "citation_count": 0
        },
        {
            "title": "Counterfactual Augmentation for Multimodal Learning Under Presentation Bias",
            "authors": "Victoria Lin, Louis-Philippe Morency, Dimitrios Dimitriadis, Srinagesh Sharma",
            "year": "2023",
            "abstract": "In real-world machine learning systems, labels are often derived from user behaviors that the system wishes to encourage. Over time, new models must be trained as new training examples and features become available. However, feedback loops between users and models can bias future user behavior, inducing a presentation bias in the labels that compromises the ability to train new models. In this paper, we propose counterfactual augmentation, a novel causal method for correcting presentation bias using generated counterfactual labels. Our empirical evaluations demonstrate that counterfactual augmentation yields better downstream performance compared to both uncorrected models and existing bias-correction methods. Model analyses further indicate that the generated counterfactuals align closely with true counterfactuals in an oracle setting.",
            "url": "https://arxiv.org/pdf/2305.14083",
            "publicationVenue": "arXiv preprint arXiv:2305.14083",
            "citation_count": 0
        },
        {
            "title": "Intensive longitudinal assessment of adolescents to predict suicidal thoughts and behaviors",
            "authors": "Randy P Auerbach, Ranqing Lan, Hanga Galfalvy, Kira L Alqueza, Jeffrey F Cohn, Ryann N Crowley, Katherine Durham, Karla J Joyce, Lauren E Kahn, Rahil A Kamath, Louis-Philippe Morency, Giovanna Porta, Apoorva Srinivasan, Jamie Zelazny, David A Brent, Nicholas B Allen",
            "year": "2023",
            "abstract": "ObjectiveSuicide is a leading cause of death among adolescents. However, there are no clinical tools to detect proximal risk for suicide.MethodParticipants included 13- to 18-year-old adolescents (N\u00a0= 103) reporting a current depressive, anxiety, and/or substance use disorder who owned a smartphone; 62% reported current suicidal ideation, with 25% indicating a past-year attempt. At baseline, participants were administered clinical interviews to assess lifetime disorders and suicidal thoughts and behaviors (STBs). Self-reports assessing symptoms and suicide risk factors also were obtained. In addition, the Effortless Assessment of Risk States (EARS) app was installed on adolescent smartphones to acquire daily mood and weekly suicidal ideation severity during the 6-month follow-up period. Adolescents completed STB and psychiatric service use interviews at the 1-, 3-, and 6-month follow-up assessments\u00a0\u2026",
            "url": null,
            "publicationVenue": "Journal of the American Academy of Child & Adolescent Psychiatry",
            "citation_count": 3
        },
        {
            "title": "MultiViz: Towards User-Centric Visualizations and Interpretations of Multimodal Models",
            "authors": "Paul Pu Liang, Yiwei Lyu, Gunjan Chhablani, Nihal Jain, Zihao Deng, Xingbo Wang, Louis-Philippe Morency, Ruslan Salakhutdinov",
            "year": "2023",
            "abstract": "The nature of human and computer interactions are inherently multimodal, which has led to substantial interest in building interpretable, interactive, and reliable multimodal interfaces. However, modern multimodal models and interfaces are typically black-box neural networks, which makes it challenging to understand their internal mechanics. How can we visualize their internal workings in order to empower stakeholders to visualize model behavior, perform model debugging, and promote trust in these models? Our paper proposes MultiViz, a method for analyzing the behavior of multimodal models via 4 stages: (1) unimodal importance, (2) cross-modal interactions, (3) multimodal representations and (4) multimodal prediction. MultiViz includes modular visualization tools for each stage before combining outputs from all stages through an interactive and human-in-the-loop API. Through user studies with 21\u00a0\u2026",
            "url": null,
            "publicationVenue": "Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems",
            "citation_count": 0
        },
        {
            "title": "Quantifying & modeling feature interactions: An information decomposition framework",
            "authors": "Paul Pu Liang, Yun Cheng, Xiang Fan, Chun Kai Ling, Suzanne Nie, Richard Chen, Zihao Deng, Faisal Mahmood, Ruslan Salakhutdinov, Louis-Philippe Morency",
            "year": "2023",
            "abstract": "The recent explosion of interest in multimodal applications has resulted in a wide selection of datasets and methods for representing and integrating information from different signals. Despite these empirical advances, there remain fundamental research questions: how can we quantify the nature of interactions that exist among input features? Subsequently, how can we capture these interactions using suitable data-driven methods? To answer this question, we propose an information-theoretic approach to quantify the degree of redundancy, uniqueness, and synergy across input features, which we term the PID statistics of a multimodal distribution. Using 2 newly proposed estimators that scale to high-dimensional distributions, we demonstrate their usefulness in quantifying the interactions within multimodal datasets, the nature of interactions captured by multimodal models, and principled approaches for model selection. We conduct extensive experiments on both synthetic datasets where the PID statistics are known and on large-scale multimodal benchmarks where PID estimation was previously impossible. Finally, to demonstrate the real-world applicability of our approach, we present three case studies in pathology, mood prediction, and robotic perception where our framework accurately recommends strong multimodal models for each application.",
            "url": "https://arxiv.org/pdf/2302.12247",
            "publicationVenue": "arXiv preprint arXiv:2302.12247",
            "citation_count": 16
        },
        {
            "title": "Proceedings of the 1st International Workshop on Socially Intelligent human-like Virtual Agents (SIVA 2023)",
            "authors": "Nicolas Obin, Catherine Pelachaud, Louis-Philippe Morency, Rachael Jack, Ryo Ishii",
            "year": "2023",
            "abstract": "SIVA\u201923 - Socially Interactive Human-like Virtual Agents From expressive and context-aware multimodal generation of digital humans to understanding the social cognition of real humans  Due to the rapid growth of virtual, augmented, and hybrid reality together with spectacular advances in artificial intelligence, the ultra-realistic generation and animation of digital humans with human-like behaviors is becoming a massive topic of interest. This complex endeavor requires modeling several elements of human behavior including the natural coordination of multimodal behaviors including text, speech, face, and body, plus the contextualization of behavior in response to interlocutors of different cultures and motivations. Thus, challenges in this topic are two folds\u2014the generation and animation of coherent multimodal behaviors, and modeling the expressivity and contextualization of the virtual agent with respect to human behavior, plus understanding and modeling virtual agent behavior adaptation to increase human\u2019s engagement. The aim of this workshop is to connect traditionally distinct communities (e.g., speech, vision, cognitive neurosciences, social psychology) to elaborate and discuss the future of human interaction with human-like virtual agents. We expect contributions from the fields of signal processing, speech and vision, machine learning and artificial intelligence, perceptual studies, and cognitive and neuroscience. Topics will range from multimodal generative modeling of virtual agent behaviors, and speech-to-face and posture 2D and 3D animation, to original research topics including style, expressivity, and context-aware animation of\u00a0\u2026",
            "url": null,
            "publicationVenue": null,
            "citation_count": 0
        },
        {
            "title": "Multimodal Feature Selection for Detecting Mothers' Depression in Dyadic Interactions with their Adolescent Offspring",
            "authors": "Maneesh Bilalpur, Saurabh Hinduja, Laura A Cariola, Lisa B Sheeber, Nick Alien, L\u00e1szl\u00f3 A Jeni, Louis-Philippe Morency, Jeffrey F Cohn",
            "year": "2023",
            "abstract": "Depression is the most common psychological disorder, a leading cause of disability world-wide, and a major contributor to inter-generational transmission of psychopathol-ogy within families. To contribute to our understanding of depression within families and to inform modality selection and feature reduction, it is critical to identify interpretable features in developmentally appropriate contexts. Mothers with and without depression were studied. Depression was defined as history of treatment for depression and elevations in current or recent symptoms. We explored two multimodal feature selection strategies in dyadic interaction tasks of mothers with their adolescent children for depression detection. Modalities included face and head dynamics, facial action units, speech-related behavior, and verbal features. The initial feature space was vast and inter-correlated (collinear). To reduce dimension-ality and gain\u00a0\u2026",
            "url": "https://www.jeffcohn.net/wp-content/uploads/2022/10/FG2023_camera_ready.pdf",
            "publicationVenue": "2023 IEEE 17th International Conference on Automatic Face and Gesture Recognition (FG)",
            "citation_count": 5
        },
        {
            "title": "Face-to-Face Contrastive Learning for Social Intelligence Question-Answering",
            "authors": "Alex Wilf, Martin Q Ma, Paul Pu Liang, Amir Zadeh, Louis-Philippe Morency",
            "year": "2023",
            "abstract": "Creating artificial social intelligence \u2013 algorithms that can understand the nuances of multi-person interactions \u2013 is an exciting and emerging challenge in processing facial expressions and gestures from multimodal videos. Recent multimodal methods have set the state of the art on many tasks, but have difficulty modeling the complex face-to-face conversational dynamics across speaking turns in social interaction, particularly in a self-supervised setup. In this paper, we propose Face-to-Face Contrastive Learning (F2F-CL), a graph neural network designed to model social interactions using factorization nodes to contextualize the multimodal face-to-face interaction along the boundaries of the speaking turn. With the F2F-CL model, we propose to perform contrastive learning between the factorization nodes of different speaking turns within the same video. We experimentally evaluate our method on the challenging\u00a0\u2026",
            "url": "https://arxiv.org/pdf/2208.01036",
            "publicationVenue": "2023 IEEE 17th International Conference on Automatic Face and Gesture Recognition (FG)",
            "citation_count": 3
        },
        {
            "title": "Continual Learning for Personalized Co-Speech Gesture Generation",
            "authors": "Chaitanya Ahuja, Pratik Joshi, Ryo Ishii, Louis-Philippe Morency",
            "year": "2023",
            "abstract": "Co-speech gestures are a key channel of human communication, making them important for personalized chat agents to generate. In the past, gesture generation models assumed that data for each speaker is available all at once, and in large amounts. However in practical scenarios, speaker data comes sequentially and in small amounts as the agent personalizes with more speakers, akin to a continual learning paradigm. While more recent works have shown progress in adapting to low-resource data, they catastrophically forget the gesture styles of initial speakers they were trained on. Also, prior generative continual learning works are not multimodal, making this space less studied. In this paper, we explore this new paradigm and propose C-DiffGAN: an approach that continually learns new speaker gesture styles with only a few minutes of per-speaker data, while retaining previously learnt styles. Inspired by prior continual learning works, C-DiffGAN encourages knowledge retention by 1) generating reminiscences of previous low-resource speaker data, then 2) crossmodally aligning to them to mitigate catastrophic forgetting. We quantitatively demonstrate improved performance and reduced forgetting over strong baselines through standard continual learning measures, reinforced by a qualitative user study that shows that our method produces more natural, style-preserving gestures. Code and videos can be found at https://chahuja. com/cdiffgan",
            "url": "http://openaccess.thecvf.com/content/ICCV2023/papers/Ahuja_Continual_Learning_for_Personalized_Co-speech_Gesture_Generation_ICCV_2023_paper.pdf",
            "publicationVenue": "Proceedings of the IEEE/CVF International Conference on Computer Vision",
            "citation_count": 0
        },
        {
            "title": "Lecture Presentations Multimodal Dataset: Towards Understanding Multimodality in Educational Videos",
            "authors": "Dong Won Lee, Chaitanya Ahuja, Paul Pu Liang, Sanika Natu, Louis-Philippe Morency",
            "year": "2023",
            "abstract": "Many educational videos use slide presentations, a sequence of visual pages that contain text and figures accompanied by spoken language, which are constructed and presented carefully in order to optimally transfer knowledge to students. Previous studies in multimedia and psychology attribute the effectiveness of lecture presentations to their multimodal nature. As a step toward developing vision-language models to aid in student learning as intelligent teacher assistants, we introduce the Lecture Presentations Multimodal (LPM) Dataset as a large-scale benchmark testing the capabilities of vision-and-language models in multimodal understanding of educational videos. Our dataset contains aligned slides and spoken language, for 180+ hours of video and 9000+ slides, with 10 lecturers from various subjects (eg, computer science, dentistry, biology). We introduce three research tasks,(1) figure-to-text retrieval,(2) text-to-figure retrieval, and (3) generation of slide explanations, which are grounded in multimedia learning and psychology principles to test a vision-language model's understanding of multimodal content. We provide manual annotations to help implement these tasks and establish baselines on them. Comparing baselines and human student performances, we find that state-of-the-art vision-language models (zero-shot and fine-tuned) struggle in (1) weak crossmodal alignment between slides and spoken text,(2) learning novel visual mediums,(3) technical language, and (4) long-range sequences. We introduce PolyViLT, a novel multimodal transformer trained with a multi-instance learning loss that is more effective than current\u00a0\u2026",
            "url": "https://openaccess.thecvf.com/content/ICCV2023/papers/Lee_Lecture_Presentations_Multimodal_Dataset_Towards_Understanding_Multimodality_in_Educational_Videos_ICCV_2023_paper.pdf",
            "publicationVenue": "Proceedings of the IEEE/CVF International Conference on Computer Vision",
            "citation_count": 0
        },
        {
            "title": "MULTIZOO & MULTIBENCH: A Standardized Toolkit for Multimodal Deep Learning",
            "authors": "Paul Pu Liang, Yiwei Lyu, Xiang Fan, Arav Agarwal, Yun Cheng, Louis-Philippe Morency, Ruslan Salakhutdinov",
            "year": "2023",
            "abstract": "Learning multimodal representations involves integrating information from multiple heterogeneous sources of data. In order to accelerate progress towards understudied modalities and tasks while ensuring real-world robustness, we release MULTIZOO, a public toolkit consisting of standardized implementations of> 20 core multimodal algorithms and MULTIBENCH, a large-scale benchmark spanning 15 datasets, 10 modalities, 20 prediction tasks, and 6 research areas. Together, these provide an automated end-to-end machine learning pipeline that simplifies and standardizes data loading, experimental setup, and model evaluation. To enable holistic evaluation, we offer a comprehensive methodology to assess (1) generalization,(2) time and space complexity, and (3) modality robustness. MULTIBENCH paves the way towards a better understanding of the capabilities and limitations of multimodal models, while ensuring ease of use, accessibility, and reproducibility. Our toolkits are publicly available, will be regularly updated, and welcome inputs from the community 1. Code: https://github. com/pliang279/MultiBench Documentation: https://multibench. readthedocs. io/en/latest/",
            "url": "https://www.jmlr.org/papers/volume24/22-1021/22-1021.pdf",
            "publicationVenue": "Journal of Machine Learning Research",
            "citation_count": 2
        },
        {
            "title": "Understanding Masked Autoencoders via Hierarchical Latent Variable Models",
            "authors": "Lingjing Kong, Martin Q Ma, Guangyi Chen, Eric P Xing, Yuejie Chi, Louis-Philippe Morency, Kun Zhang",
            "year": "2023",
            "abstract": "Masked autoencoder (MAE), a simple and effective self-supervised learning framework based on the reconstruction of masked image regions, has recently achieved prominent success in a variety of vision tasks. Despite the emergence of intriguing empirical observations on MAE, a theoretically principled understanding is still lacking. In this work, we formally characterize and justify existing empirical insights and provide theoretical guarantees of MAE. We formulate the underlying data-generating process as a hierarchical latent variable model, and show that under reasonable assumptions, MAE provably identifies a set of latent variables in the hierarchical model, explaining why MAE can extract high-level information from pixels. Further, we show how key hyperparameters in MAE (the masking ratio and the patch size) determine which true latent variables to be recovered, therefore influencing the level of semantic information in the representation. Specifically, extremely large or small masking ratios inevitably lead to low-level representations. Our theory offers coherent explanations of existing empirical observations and provides insights for potential empirical improvements and fundamental limitations of the masked-reconstruction paradigm. We conduct extensive experiments to validate our theoretical insights.",
            "url": "http://openaccess.thecvf.com/content/CVPR2023/papers/Kong_Understanding_Masked_Autoencoders_via_Hierarchical_Latent_Variable_Models_CVPR_2023_paper.pdf",
            "publicationVenue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "citation_count": 8
        }
    ],
    "David Mortensen": [
        {
            "title": "Calibrated seq2seq models for efficient and generalizable ultra-fine entity typing",
            "authors": "Yanlin Feng, Adithya Pratapa, David R Mortensen",
            "year": "2023",
            "abstract": "Ultra-fine entity typing plays a crucial role in information extraction by predicting fine-grained semantic types for entity mentions in text. However, this task poses significant challenges due to the massive number of entity types in the output space. The current state-of-the-art approaches, based on standard multi-label classifiers or cross-encoder models, suffer from poor generalization performance or inefficient inference. In this paper, we present CASENT, a seq2seq model designed for ultra-fine entity typing that predicts ultra-fine types with calibrated confidence scores. Our model takes an entity mention as input and employs constrained beam search to generate multiple types autoregressively. The raw sequence probabilities associated with the predicted types are then transformed into confidence scores using a novel calibration method. We conduct extensive experiments on the UFET dataset which contains over 10k types. Our method outperforms the previous state-of-the-art in terms of F1 score and calibration error, while achieving an inference speedup of over 50 times. Additionally, we demonstrate the generalization capabilities of our model by evaluating it in zero-shot and few-shot settings on five specialized domain entity typing datasets that are unseen during training. Remarkably, our model outperforms large language models with 10 times more parameters in the zero-shot setting, and when fine-tuned on 50 examples, it significantly outperforms ChatGPT on all datasets. Our code, models and demo are available at https://github.com/yanlinf/CASENT.",
            "url": "https://arxiv.org/pdf/2311.00835",
            "publicationVenue": "arXiv preprint arXiv:2311.00835",
            "citation_count": 1
        },
        {
            "title": "Counting the Bugs in ChatGPT's Wugs: A Multilingual Investigation into the Morphological Capabilities of a Large Language Model",
            "authors": "Leonie Weissweiler, Valentin Hofmann, Anjali Kantharuban, Anna Cai, Ritam Dutt, Amey Hengle, Anubha Kabra, Atharva Kulkarni, Abhishek Vijayakumar, Haofei Yu, Hinrich Sch\u00fctze, Kemal Oflazer, David R Mortensen",
            "year": "2023",
            "abstract": "Large language models (LLMs) have recently reached an impressive level of linguistic capability, prompting comparisons with human language skills. However, there have been relatively few systematic inquiries into the linguistic capabilities of the latest generation of LLMs, and those studies that do exist (i) ignore the remarkable ability of humans to generalize, (ii) focus only on English, and (iii) investigate syntax or semantics and overlook other capabilities that lie at the heart of human language, like morphology. Here, we close these gaps by conducting the first rigorous analysis of the morphological capabilities of ChatGPT in four typologically varied languages (specifically, English, German, Tamil, and Turkish). We apply a version of Berko's (1958) wug test to ChatGPT, using novel, uncontaminated datasets for the four examined languages. We find that ChatGPT massively underperforms purpose-built systems, particularly in English. Overall, our results -- through the lens of morphology -- cast a new light on the linguistic capabilities of ChatGPT, suggesting that claims of human-like language skills are premature and misleading.",
            "url": "https://arxiv.org/pdf/2310.15113",
            "publicationVenue": "arXiv preprint arXiv:2310.15113",
            "citation_count": 1
        },
        {
            "title": "ChatGPT MT: Competitive for high-(but not low-) resource languages",
            "authors": "Nathaniel R Robinson, Perez Ogayo, David R Mortensen, Graham Neubig",
            "year": "2023",
            "abstract": "Large language models (LLMs) implicitly learn to perform a range of language tasks, including machine translation (MT). Previous studies explore aspects of LLMs' MT capabilities. However, there exist a wide variety of languages for which recent LLM MT performance has never before been evaluated. Without published experimental evidence on the matter, it is difficult for speakers of the world's diverse languages to know how and whether they can use LLMs for their languages. We present the first experimental evidence for an expansive set of 204 languages, along with MT cost analysis, using the FLORES-200 benchmark. Trends reveal that GPT models approach or exceed traditional MT model performance for some high-resource languages (HRLs) but consistently lag for low-resource languages (LRLs), under-performing traditional MT for 84.1% of languages we covered. Our analysis reveals that a language's resource level is the most important feature in determining ChatGPT's relative ability to translate it, and suggests that ChatGPT is especially disadvantaged for LRLs and African languages.",
            "url": "https://arxiv.org/pdf/2309.07423",
            "publicationVenue": "arXiv preprint arXiv:2309.07423",
            "citation_count": 13
        },
        {
            "title": "A Review of the Applications of Deep Learning-Based Emergent Communication",
            "authors": "Brendon Boldt, David R Mortensen",
            "year": "2023",
            "abstract": "Emergent communication, or emergent language, is the field of research which studies how human language-like communication systems emerge de novo in deep multi-agent reinforcement learning environments. The possibilities of replicating the emergence of a complex behavior like language have strong intuitive appeal, yet it is necessary to complement this with clear notions of how such research can be applicable to other fields of science, technology, and engineering. This paper comprehensively reviews the applications of emergent communication research across machine learning, natural language processing, linguistics, and cognitive science. Each application is illustrated with a description of its scope, an explication of emergent communication's unique role in addressing it, a summary of the extant literature working towards the application, and brief recommendations for near-term research directions.",
            "url": "https://openreview.net/pdf?id=jesKcQxQ7j",
            "publicationVenue": "Transactions on Machine Learning Research",
            "citation_count": 0
        },
        {
            "title": "Multilingual TTS Accent Impressions for Accented ASR",
            "authors": "Georgios Karakasidis, Nathaniel Robinson, Yaroslav Getman, Atieno Ogayo, Ragheb Al-Ghezi, Ananya Ayasi, Shinji Watanabe, David R Mortensen, Mikko Kurimo",
            "year": "2023",
            "abstract": "Automatic Speech Recognition (ASR) for high-resource languages like English is often considered a solved problem. However, most high-resource ASR systems favor socioeconomically advantaged dialects. In the case of English, this leaves behind many L2 speakers and speakers of low-resource accents (a majority of English speakers). One way to mitigate this is to fine-tune a pre-trained English ASR model for a desired low-resource accent. However, collecting transcribed accented audio is costly and time-consuming. In this work, we present a method to produce synthetic L2-English speech via pre-trained text-to-speech (TTS) in an L1 language (target accent). This can be produced at a much larger scale and lower cost than authentic speech collection. We present initial experiments applying this augmentation method. Our results suggest that success of TTS augmentation relies on access to more than one\u00a0\u2026",
            "url": null,
            "publicationVenue": "International Conference on Text, Speech, and Dialogue",
            "citation_count": 0
        },
        {
            "title": "Transformed protoform reconstruction",
            "authors": "Young Min Kim, Kalvin Chang, Chenxuan Cui, David Mortensen",
            "year": "2023",
            "abstract": "Protoform reconstruction is the task of inferring what morphemes or words appeared like in the ancestral languages of a set of daughter languages. Meloni et al. (2021) achieved the state-of-the-art on Latin protoform reconstruction with an RNN-based encoder-decoder with attention model. We update their model with the state-of-the-art seq2seq model: the Transformer. Our model outperforms their model on a suite of different metrics on two different datasets: their Romance data of 8,000 cognates spanning 5 languages and a Chinese dataset (Hou 2004) of 800+ cognates spanning 39 varieties. We also probe our model for potential phylogenetic signal contained in the model. Our code is publicly available at https://github.com/cmu-llab/acl-2023.",
            "url": "https://arxiv.org/pdf/2307.01896",
            "publicationVenue": "arXiv preprint arXiv:2307.01896",
            "citation_count": 3
        },
        {
            "title": "SigMoreFun submission to the SIGMORPHON shared task on interlinear glossing",
            "authors": "Taiqi He, Lindia Tjuatja, Nathaniel Robinson, Shinji Watanabe, David R Mortensen, Graham Neubig, Lori Levin",
            "year": "2023",
            "abstract": "In our submission to the SIGMORPHON 2023 Shared Task on interlinear glossing (IGT), we explore approaches to data augmentation and modeling across seven low-resource languages. For data augmentation, we explore two approaches: creating artificial data from the provided training data and utilizing existing IGT resources in other languages. On the modeling side, we test an enhanced version of the provided token classification baseline as well as a pretrained multilingual seq2seq model. Additionally, we apply post-correction using a dictionary for Gitksan, the language with the smallest amount of data. We find that our token classification models are the best performing, with the highest word-level accuracy for Arapaho and highest morpheme-level accuracy for Gitksan out of all submissions. We also show that data augmentation is an effective strategy, though applying artificial data pretraining has very different effects across both models tested.",
            "url": "https://aclanthology.org/2023.sigmorphon-1.22.pdf",
            "publicationVenue": "Proceedings of the 20th SIGMORPHON workshop on Computational Research in Phonetics, Phonology, and Morphology",
            "citation_count": 2
        },
        {
            "title": "Generalized Glossing Guidelines: An Explicit, Human-and Machine-Readable, Item-and-Process Convention for Morphological Annotation",
            "authors": "David R Mortensen, Ela Gulsen, Taiqi He, Nathaniel Robinson, Jonathan Amith, Lindia Tjuatja, Lori Levin",
            "year": "2023",
            "abstract": "Interlinear glossing provides a vital type of morphosyntactic annotation, both for linguists and language revitalists, and numerous conventions exist for representing it formally and computationally. Some of these formats are human readable; others are machine readable. Some are easy to edit with general-purpose tools. Few represent non-concatentative processes like infixation, reduplication, mutation, truncation, and tonal overwriting in a consistent and formally rigorous way (on par with affixation). We propose an annotation convention\u00e2\u20ac\u201d Generalized Glossing Guidelines (GGG) that combines all of these positive properties using an Item-and-Process (IP) framework. We describe the format, demonstrate its linguistic adequacy, and compare it with two other interlinear glossed text annotation schemes.",
            "url": "https://aclanthology.org/2023.sigmorphon-1.7.pdf",
            "publicationVenue": "Proceedings of the 20th SIGMORPHON workshop on Computational Research in Phonetics, Phonology, and Morphology",
            "citation_count": 1
        },
        {
            "title": "Morphotactics, affix ordering, the Mirror Principle, and the Relevance Principle",
            "authors": "David R Mortensen",
            "year": "2023",
            "abstract": "Scope (how broad a swath an operations applies to) is central to morphotactics, as is SUBCATEGORIZATION (selectivity on the part of operations about what they will apply to). If thing A has scope over thing B in grammar, the morphological equivalent of B will occur closer to the root than A. From this following the prediction that inflection should almost always occur farther from the root than derivation. This is because inflection tells one about the whole word, whereas derivation applies to a narrower scope. Consider the following example:(1) establish-ment-ary-ian-ismThe suffix-ment must attach to a verb and has scope only over establish. It makes a noun that means, roughly, the abstract concept corresponding to the patient of verb. The suffix-ary, on the other hand, attaches to nouns, so it must attach to establish-ment as a whole and have scope over it. It yields adjectives.-ian takes an adjective and yields a nouns (signifying a person related to the property associated with the adjective). These are sometimes understood in terms of a tree-structure of items which the grammar can manipulate in various ways (as with the inflectional structures that motivated the Mirror Principle as seen in the next section). However, to capture a The Mirror Principle, in its original formu-",
            "url": "https://dmort27.github.io/subwordmodeling/lectures/05-morphotactics.pdf",
            "publicationVenue": null,
            "citation_count": 0
        },
        {
            "title": "Inflection, Derivation, and Compounding",
            "authors": "David R Mortensen",
            "year": "2023",
            "abstract": "The prototypical morphological operation adds some formal material (some phonemes) to a base and, in doing so, creates a new word with a different meaning:(1) a. drink b. drinker (2) a. truck b. truckerThe kind of operation is called DERIVATION. But other morphology is different. In some cases, the operation does not create a new word (but only creates a form of the word that fits a particular grammatical context). For example, the word DOG may be either dog (singular) or dogs (plural). This is called INFLECTION. In still other cases, rather than adding an affix to a base, a new word is formed by concatenating two words or stems, as in catfish or",
            "url": "https://dmort27.github.io/subwordmodeling/lectures/04-inflection_derivation.pdf",
            "publicationVenue": null,
            "citation_count": 0
        },
        {
            "title": "Do All Languages Cost the Same? Tokenization in the Era of Commercial Language Models",
            "authors": "Orevaoghene Ahia, Sachin Kumar, Hila Gonen, Jungo Kasai, David R Mortensen, Noah A Smith, Yulia Tsvetkov",
            "year": "2023",
            "abstract": "Language models have graduated from being research prototypes to commercialized products offered as web APIs, and recent works have highlighted the multilingual capabilities of these products. The API vendors charge their users based on usage, more specifically on the number of ``tokens'' processed or generated by the underlying language models. What constitutes a token, however, is training data and model dependent with a large variance in the number of tokens required to convey the same information in different languages. In this work, we analyze the effect of this non-uniformity on the fairness of an API's pricing policy across languages. We conduct a systematic analysis of the cost and utility of OpenAI's language model API on multilingual benchmarks in 22 typologically diverse languages. We show evidence that speakers of a large number of the supported languages are overcharged while obtaining poorer results. These speakers tend to also come from regions where the APIs are less affordable to begin with. Through these analyses, we aim to increase transparency around language model APIs' pricing policies and encourage the vendors to make them more equitable.",
            "url": "https://arxiv.org/pdf/2305.13707",
            "publicationVenue": "arXiv preprint arXiv:2305.13707",
            "citation_count": 9
        },
        {
            "title": "Productivity and Generalization",
            "authors": "David R Mortensen",
            "year": "2023",
            "abstract": "In 1958, Jean Berko Gleason reported what would become perhaps the most important experiment in the history of PSYCHOLINGUISTICS: the Wug Test1. In this experiment, small English-speaking children were asked to 1 Jean Berko. The child\u2019s learning of English morphology. Word, 14 (2-3): 150\u2013177, 1958",
            "url": "https://dmort27.github.io/subwordmodeling/lectures/03-productivity.pdf",
            "publicationVenue": null,
            "citation_count": 0
        },
        {
            "title": "Large-scale computerized forward reconstruction yields new perspectives in french diachronic phonology",
            "authors": "Clayton Marr, David Mortensen",
            "year": "2023",
            "abstract": "Traditionally, historical phonologists have relied on tedious manual derivations to sequence the sound changes that have shaped the phonological evolution of languages. However, humans are prone to errors, and cannot track thousands of parallel derivations in any efficient manner. We demonstrate computerized forward reconstruction (CFR), deriving each etymon in parallel, as a task with metrics to optimize, and as a tool which drastically facilitates inquiry. To this end we present DiaSim, an application which simulates \u201ccascades\u201d of diachronic developments over a language\u2019s lexicon and provides various diagnostics for \u201cdebugging\u201d those cascades. We test our method on a Latin-to-French reflex prediction task, using a newly compiled, publicly available dataset FLLex consisting of 1368 paired Latin and Modern French forms. We also introduce a second dataset, FLLAPS, which maps 310 reflexes from Latin\u00a0\u2026",
            "url": "https://www.academia.edu/download/99345855/aug29preprint_Large_Scale_Computerized_Forward_Reconstruction_Yields_New_Perspectives_in_French_Diachronic_Phonology.pdf",
            "publicationVenue": "Diachronica",
            "citation_count": 4
        },
        {
            "title": "African Substrates Rather Than European Lexifiers to Augment African-diaspora Creole Translation",
            "authors": "Nathaniel Romney Robinson, Matthew Dean Stutzman, Stephen D Richardson, David R Mortensen",
            "year": "2023",
            "abstract": "Machine translation (MT) model training is difficult for low-resource languages, such African-diaspora Creole languages, because of data scarcity. Cross-lingual data augmentation methods with knowledge transfer from related high-resource languages are a common technique to overcome this disadvantage. For instance, practitioners may transfer knowledge from a language in the same language family as the low-resource language of interest. African-diaspora Creole languages are low-resource and have simultaneous relationships with multiple language groups. These languages, such as Haitian and Jamaican, are typically lexified by colonial European languages, but they are structurally similar to African languages. We explore the advantages of transferring knowledge from the European lexifier language versus the phylogenetic and typological relatives of the African substrate languages. We analysed Haitian and Jamaican MT: both controlling tightly for data properties across compared transfer languages and later allowing use of all data we collected. Our inquiry demonstrates a significant advantage in using African transfer languages in some settings.",
            "url": "https://openreview.net/pdf?id=YKUv4sSOom",
            "publicationVenue": "4th Workshop on African Natural Language Processing",
            "citation_count": 0
        },
        {
            "title": "PWESuite: Phonetic Word Embeddings and Tasks They Facilitate",
            "authors": "Vilem Zouhar, Kalvin Chang, Chenxuan Cui, Nathaniel Carlson, Nathaniel Robinson, Mrinmaya Sachan, David Mortensen",
            "year": "2023",
            "abstract": "Word embeddings that map words into a fixed-dimensional vector space are the backbone of modern NLP. Most word embedding methods encode semantic information. However, phonetic information, which is important for some tasks, is often overlooked. In this work, we develop several novel methods which leverage articulatory features to build phonetically informed word embeddings, and present a set of phonetic word embeddings to encourage their community development, evaluation and use. While several methods for learning phonetic word embeddings already exist, there is a lack of consistency in evaluating their effectiveness. Thus, we also proposes several ways to evaluate both intrinsic aspects of phonetic word embeddings, such as word retrieval and correlation with sound similarity, and extrinsic performances, such as rhyme and cognate detection and sound analogies. We hope that our suite of tasks will promote reproducibility and provide direction for future research on phonetic word embeddings.",
            "url": "https://arxiv.org/pdf/2304.02541",
            "publicationVenue": "arXiv preprint arXiv:2304.02541",
            "citation_count": 1
        },
        {
            "title": "Construction grammar provides unique insight into neural language models",
            "authors": "Leonie Weissweiler, Taiqi He, Naoki Otani, David R Mortensen, Lori Levin, Hinrich Sch\u00fctze",
            "year": "2023",
            "abstract": "Construction Grammar (CxG) has recently been used as the basis for probing studies that have investigated the performance of large pretrained language models (PLMs) with respect to the structure and meaning of constructions. In this position paper, we make suggestions for the continuation and augmentation of this line of research. We look at probing methodology that was not designed with CxG in mind, as well as probing methodology that was designed for specific constructions. We analyse selected previous work in detail, and provide our view of the most important challenges and research questions that this promising new field faces.",
            "url": "https://arxiv.org/pdf/2302.02178",
            "publicationVenue": "arXiv preprint arXiv:2302.02178",
            "citation_count": 11
        },
        {
            "title": "Kuki-Chin Phonology: An Overview",
            "authors": "David R Mortensen",
            "year": "2023",
            "abstract": "The phonology of several Kuki-Chin (South Central Trans-Himalayan) languages have been described well, and there are fragmentary sketches of numerous others. Extensive diachronic work has also been done for the languages of this group. However, there is no comprehensive survey of the synchronic phonologies of Kuki-Chin languages. This chapter attempts to fill that gap so that researchers working on one of these languages, or doing broader typological surveys, can easily grasp the broad sound patterns in, and phonological questions raised by, Kuki-Chin. The chapter covers syllable structure, onsets, rhymes, and morphophonology. Onsets and rhymes are illustrated with complete inventories for Proto-Kuki-Chin and six attested Kuki-Chin languages from various subgroups (Falam, Mara, Thado, Daai, Lemi, Sorbung, and Monsang) and a comparative perspective on each of these inventories. This is followed by a discussion of the broader issues in Kuki-Chin sound inventories and phonotactics. These issues include laryngeal contrasts in obstruents and sonorants, the special status of glottal stop, and vowel length distinctions. A range of morphophonological alternations are then addressed, including the widespread phenomenon of non-final shortening (illustrated with observations from Thado, Daai, Sorbung, Falam, and Zophei) and vowel harmony (attested in at least Lamkang and Hyow). Apophony in stem form alterations and transitivity alternations is also discussed, drawing largely on data from Hakha Lai.",
            "url": "https://escholarship.org/content/qt1d326124/qt1d326124_noSplash_625aa1572ffed0ce1077083241416796.pdf",
            "publicationVenue": "Himalayan Linguistics",
            "citation_count": 3
        }
    ],
    "Graham Neubig": [
        {
            "title": "An In-depth Look at Gemini's Language Abilities",
            "authors": "Syeda Nahida Akter, Zichun Yu, Aashiq Muhamed, Tianyue Ou, Alex B\u00e4uerle, \u00c1ngel Alexander Cabrera, Krish Dholakia, Chenyan Xiong, Graham Neubig",
            "year": "2023",
            "abstract": "The recently released Google Gemini class of models are the first to comprehensively report results that rival the OpenAI GPT series across a wide variety of tasks. In this paper, we do an in-depth exploration of Gemini's language abilities, making two contributions. First, we provide a third-party, objective comparison of the abilities of the OpenAI GPT and Google Gemini models with reproducible code and fully transparent results. Second, we take a closer look at the results, identifying areas where one of the two model classes excels. We perform this analysis over 10 datasets testing a variety of language abilities, including reasoning, answering knowledge-based questions, solving math problems, translating between languages, generating code, and acting as instruction-following agents. From this analysis, we find that Gemini Pro achieves accuracy that is close but slightly inferior to the corresponding GPT 3.5 Turbo on all tasks that we benchmarked. We further provide explanations for some of this under-performance, including failures in mathematical reasoning with many digits, sensitivity to multiple-choice answer ordering, aggressive content filtering, and others. We also identify areas where Gemini demonstrates comparably high performance, including generation into non-English languages, and handling longer and more complex reasoning chains. Code and data for reproduction can be found at https://github.com/neulab/gemini-benchmark",
            "url": "https://arxiv.org/pdf/2312.11444.pdf?trk=public_post_comment-text",
            "publicationVenue": "arXiv preprint arXiv:2312.11444",
            "citation_count": 2
        },
        {
            "title": "Alignment for honesty",
            "authors": "Yuqing Yang, Ethan Chern, Xipeng Qiu, Graham Neubig, Pengfei Liu",
            "year": "2023",
            "abstract": "Recent research has made significant strides in applying alignment techniques to enhance the helpfulness and harmlessness of large language models (LLMs) in accordance with human intentions. In this paper, we argue for the importance of alignment for honesty, ensuring that LLMs proactively refuse to answer questions when they lack knowledge, while still not being overly conservative. However, a pivotal aspect of alignment for honesty involves discerning the limits of an LLM's knowledge, which is far from straightforward. This challenge demands comprehensive solutions in terms of metric development, benchmark creation, and training methodologies. In this paper, we address these challenges by first establishing a precise problem definition and defining ``honesty'' inspired by the Analects of Confucius. This serves as a cornerstone for developing metrics that effectively measure an LLM's honesty by quantifying its progress post-alignment. Furthermore, we introduce a flexible training framework which is further instantiated by several efficient fine-tuning techniques that emphasize honesty without sacrificing performance on other tasks. Our extensive experiments reveal that these aligned models show a marked increase in honesty, as indicated by our proposed metrics. We open-source a wealth of resources to facilitate future research at https://github.com/GAIR-NLP/alignment-for-honesty, including honesty-aligned models, training and evaluation datasets for honesty alignment, concept glossary, as well as all relevant source code.",
            "url": "https://arxiv.org/html/2312.07000v1",
            "publicationVenue": "arXiv preprint arXiv:2312.07000",
            "citation_count": 15
        },
        {
            "title": "Multitask Learning Can Improve Worst-Group Outcomes",
            "authors": "Atharva Kulkarni, Lucio Dery, Amrith Setlur, Aditi Raghunathan, Ameet Talwalkar, Graham Neubig",
            "year": "2023",
            "abstract": "In order to create machine learning systems that serve a variety of users well, it is vital to not only achieve high average performance but also ensure equitable outcomes across diverse groups. However, most machine learning methods are designed to improve a model's average performance on a chosen end task without consideration for their impact on worst group error. Multitask learning (MTL) is one such widely used technique. In this paper, we seek not only to understand the impact of MTL on worst-group accuracy but also to explore its potential as a tool to address the challenge of group-wise fairness. We primarily consider the common setting of fine-tuning a pre-trained model, where, following recent work (Gururangan et al., 2020; Dery et al., 2023), we multitask the end task with the pre-training objective constructed from the end task data itself. In settings with few or no group annotations, we find that multitasking often, but not always, achieves better worst-group accuracy than Just-Train-Twice (JTT; Liu et al. (2021)) -- a representative distributionally robust optimization (DRO) method. Leveraging insights from synthetic data experiments, we propose to modify standard MTL by regularizing the joint multitask representation space. We run a large number of fine-tuning experiments across computer vision and natural language and find that our regularized MTL approach consistently outperforms JTT on both worst and average group outcomes. Our official code can be found here: https://github.com/atharvajk98/MTL-group-robustness.",
            "url": null,
            "publicationVenue": "arXiv preprint arXiv:2312.03151",
            "citation_count": 0
        },
        {
            "title": "Program-Aided Reasoners (better) Know What They Know",
            "authors": "Anubha Kabra, Sanketh Rangreji, Yash Mathur, Aman Madaan, Emmy Liu, Graham Neubig",
            "year": "2023",
            "abstract": "Prior work shows that program-aided reasoning, in which large language models (LLMs) are combined with programs written in programming languages such as Python, can significantly improve accuracy on various reasoning tasks. However, while accuracy is essential, it is also important for such reasoners to \"know what they know\", which can be quantified through the calibration of the model. In this paper, we compare the calibration of Program Aided Language Models (PAL) and text-based Chain-of-thought (COT) prompting techniques over 5 datasets and 2 model types: LLaMA models and OpenAI models. Our results indicate that PAL leads to improved calibration in 75% of the instances. Our analysis uncovers that prompting styles that produce lesser diversity in generations also have more calibrated results, and thus we also experiment with inducing lower generation diversity using temperature scaling and find that for certain temperatures, PAL is not only more accurate but is also more calibrated than COT. Overall, we demonstrate that, in the majority of cases, program-aided reasoners better know what they know than text-based counterparts.",
            "url": "https://arxiv.org/pdf/2311.09553",
            "publicationVenue": "arXiv preprint arXiv:2311.09553",
            "citation_count": 0
        },
        {
            "title": "Divergences between Language Models and Human Brains",
            "authors": "Yuchen Zhou, Emmy Liu, Graham Neubig, Leila Wehbe",
            "year": "2023",
            "abstract": "Do machines and humans process language in similar ways? A recent line of research has hinted in the affirmative, demonstrating that human brain signals can be effectively predicted using the internal representations of language models (LMs). This is thought to reflect shared computational principles between LMs and human language processing. However, there are also clear differences in how LMs and humans acquire and use language, even if the final task they are performing is the same. Despite this, there is little work exploring systematic differences between human and machine language processing using brain data. To address this question, we examine the differences between LM representations and the human brain's responses to language, specifically by examining a dataset of Magnetoencephalography (MEG) responses to a written narrative. In doing so we identify three phenomena that, in prior work, LMs have been found to not capture well: emotional understanding, figurative language processing, and physical commonsense. By fine-tuning LMs on datasets related to these phenomena, we observe that fine-tuned LMs show improved alignment with human brain responses across these tasks. Our study implies that the observed divergences between LMs and human brains may stem from LMs' inadequate representation of these specific types of knowledge.",
            "url": "https://arxiv.org/pdf/2311.09308",
            "publicationVenue": "arXiv preprint arXiv:2311.09308",
            "citation_count": 0
        },
        {
            "title": "Learning to filter context for retrieval-augmented generation",
            "authors": "Zhiruo Wang, Jun Araki, Zhengbao Jiang, Md Rizwan Parvez, Graham Neubig",
            "year": "2023",
            "abstract": "On-the-fly retrieval of relevant knowledge has proven an essential element of reliable systems for tasks such as open-domain question answering and fact verification. However, because retrieval systems are not perfect, generation models are required to generate outputs given partially or entirely irrelevant passages. This can cause over- or under-reliance on context, and result in problems in the generated output such as hallucinations. To alleviate these problems, we propose FILCO, a method that improves the quality of the context provided to the generator by (1) identifying useful context based on lexical and information-theoretic approaches, and (2) training context filtering models that can filter retrieved contexts at test time. We experiment on six knowledge-intensive tasks with FLAN-T5 and LLaMa2, and demonstrate that our method outperforms existing approaches on extractive question answering (QA), complex multi-hop and long-form QA, fact verification, and dialog generation tasks. FILCO effectively improves the quality of context, whether or not it supports the canonical output.",
            "url": "https://arxiv.org/pdf/2311.08377",
            "publicationVenue": "arXiv preprint arXiv:2311.08377",
            "citation_count": 13
        },
        {
            "title": "DeMuX: Data-efficient Multilingual Learning",
            "authors": "Simran Khanuja, Srinivas Gowriraj, Lucio Dery, Graham Neubig",
            "year": "2023",
            "abstract": "We consider the task of optimally fine-tuning pre-trained multilingual models, given small amounts of unlabelled target data and an annotation budget. In this paper, we introduce DEMUX, a framework that prescribes the exact data-points to label from vast amounts of unlabelled multilingual data, having unknown degrees of overlap with the target set. Unlike most prior works, our end-to-end framework is language-agnostic, accounts for model representations, and supports multilingual target configurations. Our active learning strategies rely upon distance and uncertainty measures to select task-specific neighbors that are most informative to label, given a model. DeMuX outperforms strong baselines in 84% of the test cases, in the zero-shot setting of disjoint source and target language sets (including multilingual target pools), across three models and four tasks. Notably, in low-budget settings (5-100 examples), we observe gains of up to 8-11 F1 points for token-level tasks, and 2-5 F1 for complex tasks. Our code is released here: https://github.com/simran-khanuja/demux.",
            "url": "https://arxiv.org/pdf/2311.06379",
            "publicationVenue": "arXiv preprint arXiv:2311.06379",
            "citation_count": 0
        },
        {
            "title": "Do llms exhibit human-like response biases? a case study in survey design",
            "authors": "Lindia Tjuatja, Valerie Chen, Sherry Tongshuang Wu, Ameet Talwalkar, Graham Neubig",
            "year": "2023",
            "abstract": "As large language models (LLMs) become more capable, there is growing excitement about the possibility of using LLMs as proxies for humans in real-world tasks where subjective labels are desired, such as in surveys and opinion polling. One widely-cited barrier to the adoption of LLMs is their sensitivity to prompt wording -- but interestingly, humans also display sensitivities to instruction changes in the form of response biases. As such, we argue that if LLMs are going to be used to approximate human opinions, it is necessary to investigate the extent to which LLMs also reflect human response biases, if at all. In this work, we use survey design as a case study, where human response biases caused by permutations in wordings of ``prompts'' have been extensively studied. Drawing from prior work in social psychology, we design a dataset and propose a framework to evaluate whether LLMs exhibit human-like response biases in survey questionnaires. Our comprehensive evaluation of nine models shows that popular open and commercial LLMs generally fail to reflect human-like behavior. These inconsistencies tend to be more prominent in models that have been instruction fine-tuned. Furthermore, even if a model shows a significant change in the same direction as humans, we find that perturbations that are not meant to elicit significant changes in humans may also result in a similar change, suggesting that such a result could be partially due to other spurious correlations. These results highlight the potential pitfalls of using LLMs to substitute humans in parts of the annotation pipeline, and further underscore the importance of finer-grained\u00a0\u2026",
            "url": "https://arxiv.org/pdf/2311.04076",
            "publicationVenue": "arXiv preprint arXiv:2311.04076",
            "citation_count": 5
        },
        {
            "title": "Teacher perception of automatically extracted grammar concepts for L2 language learning",
            "authors": "Aditi Chaudhary, Arun Sampath, Ashwin Sheshadri, Antonios Anastasopoulos, Graham Neubig",
            "year": "2023",
            "abstract": "One of the challenges in language teaching is how best to organize rules regarding syntax, semantics, or phonology in a meaningful manner. This not only requires content creators to have pedagogical skills, but also have that language's deep understanding. While comprehensive materials to develop such curricula are available in English and some broadly spoken languages, for many other languages, teachers need to manually create them in response to their students' needs. This is challenging because i) it requires that such experts be accessible and have the necessary resources, and ii) describing all the intricacies of a language is time-consuming and prone to omission. In this work, we aim to facilitate this process by automatically discovering and visualizing grammar descriptions. We extract descriptions from a natural text corpus that answer questions about morphosyntax (learning of word order, agreement, case marking, or word formation) and semantics (learning of vocabulary). We apply this method for teaching two Indian languages, Kannada and Marathi, which, unlike English, do not have well-developed resources for second language learning. To assess the perceived utility of the extracted material, we enlist the help of language educators from schools in North America to perform a manual evaluation, who find the materials have potential to be used for their lesson preparation and learner evaluation.",
            "url": "https://arxiv.org/pdf/2310.18417",
            "publicationVenue": "EMNLP 2023 Findings",
            "citation_count": 1
        },
        {
            "title": "Sotopia: Interactive evaluation for social intelligence in language agents",
            "authors": "Xuhui Zhou, Hao Zhu, Leena Mathur, Ruohong Zhang, Haofei Yu, Zhengyang Qi, Louis-Philippe Morency, Yonatan Bisk, Daniel Fried, Graham Neubig, Maarten Sap",
            "year": "2023",
            "abstract": "Humans are social beings; we pursue social goals in our daily interactions, which is a crucial aspect of social intelligence. Yet, AI systems' abilities in this realm remain elusive. We present SOTOPIA, an open-ended environment to simulate complex social interactions between artificial agents and evaluate their social intelligence. In our environment, agents role-play and interact under a wide variety of scenarios; they coordinate, collaborate, exchange, and compete with each other to achieve complex social goals. We simulate the role-play interaction between LLM-based agents and humans within this task space and evaluate their performance with a holistic evaluation framework called SOTOPIA-Eval. With SOTOPIA, we find significant differences between these models in terms of their social intelligence, and we identify a subset of SOTOPIA scenarios, SOTOPIA-hard, that is generally challenging for all models. We find that on this subset, GPT-4 achieves a significantly lower goal completion rate than humans and struggles to exhibit social commonsense reasoning and strategic communication skills. These findings demonstrate SOTOPIA's promise as a general platform for research on evaluating and improving social intelligence in artificial agents.",
            "url": "https://arxiv.org/pdf/2310.11667",
            "publicationVenue": "arXiv preprint arXiv:2310.11667",
            "citation_count": 13
        },
        {
            "title": "Discovering Divergences between Language Models and Human Brains",
            "authors": "Yuchen Zhou, Emmy Liu, Graham Neubig, Leila Wehbe",
            "year": "2023",
            "abstract": "Do machines and humans process language in similar ways? A recent line of research has hinted in the affirmative, demonstrating that human brain signals can be effectively predicted using the internal representations of language models (LMs). This is thought to reflect shared computational principles between LMs and human language processing. However, there are also clear differences in how LMs and humans acquire and use language, even if the final task they are performing is the same. Despite this, there is little work exploring systematic differences between human and machine language processing using brain data. To address this question, we examine the differences between LM representations and the human brain's responses to language, specifically by examining a dataset of Magnetoencephalography (MEG) responses to a written narrative. In doing so we identify three phenomena that, in prior work, LMs have been found to not capture well: emotional understanding, figurative language processing, and physical commonsense. We further fine-tune models on datasets related to these three phenomena, and find that LMs fine-tuned on tasks related to emotion and figurative language show improved alignment with brain responses. We emphasize the importance of understanding not just similarities between human and machine language processing, but also differences. Our work takes the first steps toward this goal in the context of narrative reading.",
            "url": "https://openreview.net/pdf?id=J7AwIJvR3d",
            "publicationVenue": null,
            "citation_count": 0
        },
        {
            "title": "Crossing the Threshold: Idiomatic Machine Translation through Retrieval Augmentation and Loss Weighting",
            "authors": "Emmy Liu, Aditi Chaudhary, Graham Neubig",
            "year": "2023",
            "abstract": "Idioms are common in everyday language, but often pose a challenge to translators because their meanings do not follow from the meanings of their parts. Despite significant advances, machine translation systems still struggle to translate idiomatic expressions. We provide a simple characterization of idiomatic translation and related issues. This allows us to conduct a synthetic experiment revealing a tipping point at which transformer-based machine translation models correctly default to idiomatic translations. To expand multilingual resources, we compile a dataset of ~4k natural sentences containing idiomatic expressions in French, Finnish, and Japanese. To improve translation of natural idioms, we introduce two straightforward yet effective techniques: the strategic upweighting of training loss on potentially idiomatic sentences, and using retrieval-augmented models. This not only improves the accuracy of a strong pretrained MT model on idiomatic sentences by up to 13% in absolute accuracy, but also holds potential benefits for non-idiomatic sentences.",
            "url": "https://arxiv.org/pdf/2310.07081",
            "publicationVenue": "EMNLP 2023",
            "citation_count": 0
        },
        {
            "title": "It's MBR All the Way Down: Modern Generation Techniques Through the Lens of Minimum Bayes Risk",
            "authors": "Amanda Bertsch, Alex Xie, Graham Neubig, Matthew R Gormley",
            "year": "2023",
            "abstract": "Minimum Bayes Risk (MBR) decoding is a method for choosing the outputs of a machine learning system based not on the output with the highest probability, but the output with the lowest risk (expected error) among multiple candidates. It is a simple but powerful method: for an additional cost at inference time, MBR provides reliable several-point improvements across metrics for a wide variety of tasks without any additional data or training. Despite this, MBR is not frequently applied in NLP works, and knowledge of the method itself is limited. We first provide an introduction to the method and the recent literature. We show that several recent methods that do not reference MBR can be written as special cases of MBR; this reformulation provides additional theoretical justification for the performance of these methods, explaining some results that were previously only empirical. We provide theoretical and empirical results about the effectiveness of various MBR variants and make concrete recommendations for the application of MBR in NLP models, including future directions in this area.",
            "url": "https://arxiv.org/pdf/2310.01387",
            "publicationVenue": "EMNLP 2023 BigPicture Workshop",
            "citation_count": 5
        },
        {
            "title": "ChatGPT MT: Competitive for High-(but not Low-) Resource Languages",
            "authors": "Nathaniel R Robinson, Perez Ogayo, David R Mortensen, Graham Neubig",
            "year": "2023",
            "abstract": "Large language models (LLMs) implicitly learn to perform a range of language tasks, including machine translation (MT). Previous studies explore aspects of LLMs' MT capabilities. However, there exist a wide variety of languages for which recent LLM MT performance has never before been evaluated. Without published experimental evidence on the matter, it is difficult for speakers of the world's diverse languages to know how and whether they can use LLMs for their languages. We present the first experimental evidence for an expansive set of 204 languages, along with MT cost analysis, using the FLORES-200 benchmark. Trends reveal that GPT models approach or exceed traditional MT model performance for some high-resource languages (HRLs) but consistently lag for low-resource languages (LRLs), under-performing traditional MT for 84.1% of languages we covered. Our analysis reveals that a language's resource level is the most important feature in determining ChatGPT's relative ability to translate it, and suggests that ChatGPT is especially disadvantaged for LRLs and African languages.",
            "url": "https://arxiv.org/pdf/2309.07423",
            "publicationVenue": "WMT 2023",
            "citation_count": 13
        },
        {
            "title": "Prompt2Model: Generating Deployable Models from Natural Language Instructions",
            "authors": "Vijay Viswanathan, Chenyang Zhao, Amanda Bertsch, Tongshuang Wu, Graham Neubig",
            "year": "2023",
            "abstract": "Large language models (LLMs) enable system builders today to create competent NLP systems through prompting, where they only need to describe the task in natural language and provide a few examples. However, in other ways, LLMs are a step backward from traditional special-purpose NLP models; they require extensive computational resources for deployment and can be gated behind APIs. In this paper, we propose Prompt2Model, a general-purpose method that takes a natural language task description like the prompts provided to LLMs, and uses it to train a special-purpose model that is conducive to deployment. This is done through a multi-step process of retrieval of existing datasets and pretrained models, dataset generation using LLMs, and supervised fine-tuning on these retrieved and generated datasets. Over three tasks, we demonstrate that given the same few-shot prompt as input, Prompt2Model trains models that outperform the results of a strong LLM, gpt-3.5-turbo, by an average of 20% while being up to 700 times smaller. We also show that this data can be used to obtain reliable performance estimates of model performance, enabling model developers to assess model reliability before deployment. Prompt2Model is available open-source at https://github.com/neulab/prompt2model.",
            "url": "https://arxiv.org/pdf/2308.12261",
            "publicationVenue": "EMNLP 2023 Demo",
            "citation_count": 5
        },
        {
            "title": "The Devil is in the Errors: Leveraging Large Language Models for Fine-grained Machine Translation Evaluation",
            "authors": "Patrick Fernandes, Daniel Deutsch, Mara Finkelstein, Parker Riley, Andre FT Martins, Graham Neubig, Ankush Garg, Jonathan H Clark, Markus Freitag, Orhan Firat",
            "year": "2023",
            "abstract": "Automatic evaluation of machine translation (MT) is a critical tool driving the rapid iterative development of MT systems. While considerable progress has been made on estimating a single scalar quality score, current metrics lack the informativeness of more detailed schemes that annotate individual errors, such as Multidimensional Quality Metrics (MQM). In this paper, we help fill this gap by proposing AutoMQM, a prompting technique which leverages the reasoning and in-context learning capabilities of large language models (LLMs) and asks them to identify and categorize errors in translations. We start by evaluating recent LLMs, such as PaLM and PaLM-2, through simple score prediction prompting, and we study the impact of labeled data through in-context learning and finetuning. We then evaluate AutoMQM with PaLM-2 models, and we find that it improves performance compared to just prompting for scores (with particularly large gains for larger models) while providing interpretability through error spans that align with human annotations.",
            "url": "https://arxiv.org/pdf/2308.07286",
            "publicationVenue": "WMT 2023",
            "citation_count": 19
        },
        {
            "title": "FacTool: Factuality Detection in Generative AI--A Tool Augmented Framework for Multi-Task and Multi-Domain Scenarios",
            "authors": "I Chern, Steffi Chern, Shiqi Chen, Weizhe Yuan, Kehua Feng, Chunting Zhou, Junxian He, Graham Neubig, Pengfei Liu",
            "year": "2023",
            "abstract": "The emergence of generative pre-trained models has facilitated the synthesis of high-quality text, but it has also posed challenges in identifying factual errors in the generated text. In particular: (1) A wider range of tasks now face an increasing risk of containing factual errors when handled by generative models. (2) Generated texts tend to be lengthy and lack a clearly defined granularity for individual facts. (3) There is a scarcity of explicit evidence available during the process of fact checking. With the above challenges in mind, in this paper, we propose FacTool, a task and domain agnostic framework for detecting factual errors of texts generated by large language models (e.g., ChatGPT). Experiments on four different tasks (knowledge-based QA, code generation, mathematical reasoning, and scientific literature review) show the efficacy of the proposed method. We release the code of FacTool associated with ChatGPT plugin interface at https://github.com/GAIR-NLP/factool .",
            "url": "https://arxiv.org/pdf/2307.13528",
            "publicationVenue": "arXiv preprint arXiv:2307.13528",
            "citation_count": 37
        },
        {
            "title": "Webarena: A realistic web environment for building autonomous agents",
            "authors": "Shuyan Zhou, Frank F Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Yonatan Bisk, Daniel Fried, Uri Alon, Graham Neubig",
            "year": "2023",
            "abstract": "With generative AI advances, the exciting potential for autonomous agents to manage daily tasks via natural language commands has emerged. However, cur rent agents are primarily created and tested in simplified synthetic environments, substantially limiting real-world scenario representation. In this paper, we build an environment for agent command and control that is highly realistic and reproducible. Specifically, we focus on agents that perform tasks on websites, and we create an environment with fully functional websites from four common domains: e-commerce, social forum discussions, collaborative software development, and content management. Our environment is enriched with tools (e.g., a map) and external knowledge bases (e.g., user manuals) to encourage human-like task-solving. Building upon our environment, we release a set of benchmark tasks focusing on evaluating the functional correctness of task completions. The tasks in our benchmark are diverse, long-horizon, and are designed to emulate tasks that humans routinely perform on the internet. We design and implement several autonomous agents, integrating recent techniques such as reasoning before acting. The results demonstrate that solving complex tasks is challenging: our best GPT-4-based agent only achieves an end-to-end task success rate of 10.59%. These results highlight the need for further development of robust agents, that current state-of-the-art LMs are far from perfect performance in these real-life tasks, and that WebArena can be used to measure such progress. Our code, data, environment reproduction resources, and video demonstrations are\u00a0\u2026",
            "url": "https://arxiv.org/pdf/2307.13854",
            "publicationVenue": "ICLR 2024",
            "citation_count": 59
        },
        {
            "title": "Improving Factuality of Abstractive Summarization via Contrastive Reward Learning",
            "authors": "I-Chun Chern, Zhiruo Wang, Sanjan Das, Bhavuk Sharma, Pengfei Liu, Graham Neubig",
            "year": "2023",
            "abstract": "Modern abstractive summarization models often generate summaries that contain hallucinated or contradictory information. In this paper, we propose a simple but effective contrastive learning framework that incorporates recent developments in reward learning and factuality metrics. Empirical studies demonstrate that the proposed framework enables summarization models to learn from feedback of factuality metrics using contrastive reward learning, leading to more factual summaries by human evaluations. This suggests that further advances in learning and evaluation algorithms can feed directly into providing more factual summaries.",
            "url": "https://arxiv.org/pdf/2307.04507",
            "publicationVenue": "arXiv preprint arXiv:2307.04507",
            "citation_count": 0
        },
        {
            "title": "Large Language Models Enable Few-Shot Clustering",
            "authors": "Vijay Viswanathan, Kiril Gashteovski, Carolin Lawrence, Tongshuang Wu, Graham Neubig",
            "year": "2023",
            "abstract": "Unlike traditional unsupervised clustering, semi-supervised clustering allows users to provide meaningful structure to the data, which helps the clustering algorithm to match the user's intent. Existing approaches to semi-supervised clustering require a significant amount of feedback from an expert to improve the clusters. In this paper, we ask whether a large language model can amplify an expert's guidance to enable query-efficient, few-shot semi-supervised text clustering. We show that LLMs are surprisingly effective at improving clustering. We explore three stages where LLMs can be incorporated into clustering: before clustering (improving input features), during clustering (by providing constraints to the clusterer), and after clustering (using LLMs post-correction). We find incorporating LLMs in the first two stages can routinely provide significant improvements in cluster quality, and that LLMs enable a user to make trade-offs between cost and accuracy to produce desired clusters. We release our code and LLM prompts for the public to use.",
            "url": "https://arxiv.org/pdf/2307.00524",
            "publicationVenue": "TACL",
            "citation_count": 5
        },
        {
            "title": "NusaCrowd: Open source initiative for Indonesian NLP resources",
            "authors": "Samuel Cahyawijaya, Holy Lovenia, Alham Fikri Aji, Genta Winata, Bryan Wilie, Fajri Koto, Rahmad Mahendra, Christian Wibisono, Ade Romadhony, Karissa Vincentio, Jennifer Santoso, David Moeljadi, Cahya Wirawan, Frederikus Hudi, Muhammad Satrio Wicaksono, Ivan Parmonangan, Ika Alfina, Ilham Firdausi Putra, Samsul Rahmadani, Yulianti Oenang, Ali Septiandri, James Jaya, Kaustubh Dhole, Arie Suryani, Rifki Afina Putri, Dan Su, Keith Stevens, Made Nindyatama Nityasya, Muhammad Adilazuarda, Ryan Hadiwijaya, Ryandito Diandaru, Tiezheng Yu, Vito Ghifari, Wenliang Dai, Yan Xu, Dyah Damapuspita, Haryo Wibowo, Cuk Tho, Ichwanul Karo Karo, Tirana Fatyanosa, Ziwei Ji, Graham Neubig, Timothy Baldwin, Sebastian Ruder, Pascale Fung, Herry Sujaini, Sakriani Sakti, Ayu Purwarianti",
            "year": "2023",
            "abstract": "We present NusaCrowd, a collaborative initiative to collect and unify existing resources for Indonesian languages, including opening access to previously non-public resources. Through this initiative, we have brought together 137 datasets and 118 standardized data loaders. The quality of the datasets has been assessed manually and automatically, and their value is demonstrated through multiple experiments. NusaCrowd\u2019s data collection enables the creation of the first zero-shot benchmarks for natural language understanding and generation in Indonesian and the local languages of Indonesia. Furthermore, NusaCrowd brings the creation of the first multilingual automatic speech recognition benchmark in Indonesian and the local languages of Indonesia. Our work strives to advance natural language processing (NLP) research for languages that are under-represented despite being widely spoken.",
            "url": "https://aclanthology.org/2023.findings-acl.868.pdf",
            "publicationVenue": "Findings of the Association for Computational Linguistics: ACL 2023",
            "citation_count": 502
        },
        {
            "title": "SigMoreFun submission to the SIGMORPHON shared task on interlinear glossing",
            "authors": "Taiqi He, Lindia Tjuatja, Nathaniel Robinson, Shinji Watanabe, David R Mortensen, Graham Neubig, Lori Levin",
            "year": "2023",
            "abstract": "In our submission to the SIGMORPHON 2023 Shared Task on interlinear glossing (IGT), we explore approaches to data augmentation and modeling across seven low-resource languages. For data augmentation, we explore two approaches: creating artificial data from the provided training data and utilizing existing IGT resources in other languages. On the modeling side, we test an enhanced version of the provided token classification baseline as well as a pretrained multilingual seq2seq model. Additionally, we apply post-correction using a dictionary for Gitksan, the language with the smallest amount of data. We find that our token classification models are the best performing, with the highest word-level accuracy for Arapaho and highest morpheme-level accuracy for Gitksan out of all submissions. We also show that data augmentation is an effective strategy, though applying artificial data pretraining has very different effects across both models tested.",
            "url": "https://aclanthology.org/2023.sigmorphon-1.22.pdf",
            "publicationVenue": "Proceedings of the 20th SIGMORPHON workshop on Computational Research in Phonetics, Phonology, and Morphology",
            "citation_count": 2
        },
        {
            "title": "Neural Machine Translation for the Indigenous Languages of the Americas: An Introduction",
            "authors": "Manuel Mager, Rajat Bhatnagar, Graham Neubig, Ngoc Thang Vu, Katharina Kann",
            "year": "2023",
            "abstract": "Neural models have drastically advanced state of the art for machine translation (MT) between high-resource languages. Traditionally, these models rely on large amounts of training data, but many language pairs lack these resources. However, an important part of the languages in the world do not have this amount of data. Most languages from the Americas are among them, having a limited amount of parallel and monolingual data, if any. Here, we present an introduction to the interested reader to the basic challenges, concepts, and techniques that involve the creation of MT systems for these languages. Finally, we discuss the recent advances and findings and open questions, product of an increased interest of the NLP community in these languages.",
            "url": "https://arxiv.org/pdf/2306.06804",
            "publicationVenue": "arXiv preprint arXiv:2306.06804",
            "citation_count": 0
        },
        {
            "title": "Multi-Dimensional Evaluation of Text Summarization with In-Context Learning",
            "authors": "Sameer Jain, Vaishakh Keshava, Swarnashree Mysore Sathyendra, Patrick Fernandes, Pengfei Liu, Graham Neubig, Chunting Zhou",
            "year": "2023",
            "abstract": "Evaluation of natural language generation (NLG) is complex and multi-dimensional. Generated text can be evaluated for fluency, coherence, factuality, or any other dimensions of interest. Most frameworks that perform such multi-dimensional evaluation require training on large manually or synthetically generated datasets. In this paper, we study the efficacy of large language models as multi-dimensional evaluators using in-context learning, obviating the need for large training datasets. Our experiments show that in-context learning-based evaluators are competitive with learned evaluation frameworks for the task of text summarization, establishing state-of-the-art on dimensions such as relevance and factual consistency. We then analyze the effects of factors such as the selection and number of in-context examples on performance. Finally, we study the efficacy of in-context learning based evaluators in evaluating zero-shot summaries written by large language models such as GPT-3.",
            "url": "https://arxiv.org/pdf/2306.01200",
            "publicationVenue": "ACL Findings 2023",
            "citation_count": 2
        },
        {
            "title": "Syntax and Semantics Meet in the\" Middle\": Probing the Syntax-Semantics Interface of LMs Through Agentivity",
            "authors": "Lindia Tjuatja, Emmy Liu, Lori Levin, Graham Neubig",
            "year": "2023",
            "abstract": "Recent advances in large language models have prompted researchers to examine their abilities across a variety of linguistic tasks, but little has been done to investigate how models handle the interactions in meaning across words and larger syntactic forms -- i.e. phenomena at the intersection of syntax and semantics. We present the semantic notion of agentivity as a case study for probing such interactions. We created a novel evaluation dataset by utilitizing the unique linguistic properties of a subset of optionally transitive English verbs. This dataset was used to prompt varying sizes of three model classes to see if they are sensitive to agentivity at the lexical level, and if they can appropriately employ these word-level priors given a specific syntactic context. Overall, GPT-3 text-davinci-003 performs extremely well across all experiments, outperforming all other models tested by far. In fact, the results are even better correlated with human judgements than both syntactic and semantic corpus statistics. This suggests that LMs may potentially serve as more useful tools for linguistic annotation, theory testing, and discovery than select corpora for certain tasks.",
            "url": "https://arxiv.org/pdf/2305.18185",
            "publicationVenue": "*SEM 2023",
            "citation_count": 1
        },
        {
            "title": "DataFinder: Scientific Dataset Recommendation from Natural Language Descriptions",
            "authors": "Vijay Viswanathan, Luyu Gao, Tongshuang Wu, Pengfei Liu, Graham Neubig",
            "year": "2023",
            "abstract": "Modern machine learning relies on datasets to develop and validate research ideas. Given the growth of publicly available data, finding the right dataset to use is increasingly difficult. Any research question imposes explicit and implicit constraints on how well a given dataset will enable researchers to answer this question, such as dataset size, modality, and domain. We introduce a new task of recommending relevant datasets given a short natural language description of a research idea, to help people find relevant datasets for their needs. Dataset recommendation poses unique challenges as an information retrieval problem; datasets are hard to directly index for search and there are no corpora readily available for this task. To operationalize this task, we build the DataFinder Dataset which consists of a larger automatically-constructed training set (17.5K queries) and a smaller expert-annotated evaluation set (392 queries). Using this data, we compare various information retrieval algorithms on our test set and present the first-ever published system for text-based dataset recommendation using machine learning techniques. This system, trained on the DataFinder Dataset, finds more relevant search results than existing third-party dataset search engines. To encourage progress on dataset recommendation, we release our dataset and models to the public.",
            "url": "https://arxiv.org/pdf/2305.16636",
            "publicationVenue": "ACL 2023",
            "citation_count": 4
        },
        {
            "title": "Multi-lingual and Multi-cultural Figurative Language Understanding",
            "authors": "Anubha Kabra, Emmy Liu, Simran Khanuja, Alham Fikri Aji, Genta Indra Winata, Samuel Cahyawijaya, Anuoluwapo Aremu, Perez Ogayo, Graham Neubig",
            "year": "2023",
            "abstract": "Figurative language permeates human communication, but at the same time is relatively understudied in NLP. Datasets have been created in English to accelerate progress towards measuring and improving figurative language processing in language models (LMs). However, the use of figurative language is an expression of our cultural and societal experiences, making it difficult for these phrases to be universally applicable. In this work, we create a figurative language inference dataset, \\datasetname, for seven diverse languages associated with a variety of cultures: Hindi, Indonesian, Javanese, Kannada, Sundanese, Swahili and Yoruba. Our dataset reveals that each language relies on cultural and regional concepts for figurative expressions, with the highest overlap between languages originating from the same region. We assess multilingual LMs' abilities to interpret figurative language in zero-shot and few-shot settings. All languages exhibit a significant deficiency compared to English, with variations in performance reflecting the availability of pre-training and fine-tuning data, emphasizing the need for LMs to be exposed to a broader range of linguistic and cultural variation during training.",
            "url": "https://arxiv.org/pdf/2305.16171",
            "publicationVenue": "ACL Findings 2023",
            "citation_count": 5
        },
        {
            "title": "GlobalBench: A Benchmark for Global Progress in Natural Language Processing",
            "authors": "Yueqi Song, Catherine Cui, Simran Khanuja, Pengfei Liu, Fahim Faisal, Alissa Ostapenko, Genta Indra Winata, Alham Fikri Aji, Samuel Cahyawijaya, Yulia Tsvetkov, Antonios Anastasopoulos, Graham Neubig",
            "year": "2023",
            "abstract": "Despite the major advances in NLP, significant disparities in NLP system performance across languages still exist. Arguably, these are due to uneven resource allocation and sub-optimal incentives to work on less resourced languages. To track and further incentivize the global development of equitable language technology, we introduce GlobalBench. Prior multilingual benchmarks are static and have focused on a limited number of tasks and languages. In contrast, GlobalBench is an ever-expanding collection that aims to dynamically track progress on all NLP datasets in all languages. Rather than solely measuring accuracy, GlobalBench also tracks the estimated per-speaker utility and equity of technology across all languages, providing a multi-faceted view of how language technology is serving people of the world. Furthermore, GlobalBench is designed to identify the most under-served languages, and rewards research efforts directed towards those languages. At present, the most under-served languages are the ones with a relatively high population, but nonetheless overlooked by composite multilingual benchmarks (like Punjabi, Portuguese, and Wu Chinese). Currently, GlobalBench covers 966 datasets in 190 languages, and has 1,128 system submissions spanning 62 languages.",
            "url": "https://arxiv.org/pdf/2305.14716",
            "publicationVenue": "EMNLP 2023",
            "citation_count": 3
        },
        {
            "title": "Solving NLP Problems through Human-System Collaboration: A Discussion-based Approach",
            "authors": "Masahiro Kaneko, Graham Neubig, Naoaki Okazaki",
            "year": "2023",
            "abstract": "Humans work together to solve common problems by having discussions, explaining, and agreeing or disagreeing with each other. Similarly, if a system can have discussions with humans when solving tasks, it can improve the system's performance and reliability. In previous research on explainability, it has only been possible for the system to make predictions and for humans to ask questions about them rather than having a mutual exchange of opinions. This research aims to create a dataset and computational framework for systems that discuss and refine their predictions through dialogue. Through experiments, we show that the proposed system can have beneficial discussions with humans improving the accuracy by up to 25 points in the natural language inference task.",
            "url": "https://arxiv.org/pdf/2305.11789",
            "publicationVenue": "arXiv preprint arXiv:2305.11789",
            "citation_count": 3
        },
        {
            "title": "Active retrieval augmented generation",
            "authors": "Zhengbao Jiang, Frank F Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig",
            "year": "2023",
            "abstract": "Despite the remarkable ability of large language models (LMs) to comprehend and generate language, they have a tendency to hallucinate and create factually inaccurate output. Augmenting LMs by retrieving information from external knowledge resources is one promising solution. Most existing retrieval-augmented LMs employ a retrieve-and-generate setup that only retrieves information once based on the input. This is limiting, however, in more general scenarios involving generation of long texts, where continually gathering information throughout the generation process is essential. There have been some past efforts to retrieve information multiple times while generating outputs, which mostly retrieve documents at fixed intervals using the previous context as queries. In this work, we provide a generalized view of active retrieval augmented generation, methods that actively decide when and what to retrieve across the course of the generation. We propose Forward-Looking Active REtrieval augmented generation (FLARE), a generic retrieval-augmented generation method which iteratively uses a prediction of the upcoming sentence to anticipate future content, which is then utilized as a query to retrieve relevant documents to regenerate the sentence if it contains low-confidence tokens. We test FLARE along with baselines comprehensively over 4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves superior or competitive performance on all tasks, demonstrating the effectiveness of our method. Code and datasets are available at https://github.com/jzbjyb/FLARE.",
            "url": "https://arxiv.org/pdf/2305.06983",
            "publicationVenue": "EMNLP 2023",
            "citation_count": 88
        },
        {
            "title": "Unlimiformer: Long-range transformers with unlimited length input",
            "authors": "Amanda Bertsch, Uri Alon, Graham Neubig, Matthew R Gormley",
            "year": "2023",
            "abstract": "Since the proposal of transformers, these models have been limited to bounded input lengths, because of their need to attend to every token in the input. In this work, we propose Unlimiformer: a general approach that wraps any existing pretrained encoder-decoder transformer, and offloads the cross-attention computation to a single-nearest-neighbor (NN) index, while the returnedNN distances are the attention dot-product scores. ThisNN index can be kept on either the GPU or CPU memory and queried in sub-linear time; this way, we can index practically unlimited input sequences, while every attention head in every decoder layer retrieves its top-keys, instead of attending to every key. We evaluate Unlimiformer on several long-document and book-summarization benchmarks, showing that it can process even** 500k** token-long inputs from the BookSum dataset, without any input truncation at test time. We demonstrate that Unlimiformer improves pretrained models such as BART and Longformer by extending them to unlimited inputs without additional learned weights and without modifying their code. Our code and models are publicly available at https://github. com/abertsch72/unlimiformer, and support LLaMA-2 as well.",
            "url": "https://proceedings.neurips.cc/paper_files/paper/2023/file/6f9806a5adc72b5b834b27e4c7c0df9b-Paper-Conference.pdf",
            "publicationVenue": "NeurIPS 2023",
            "citation_count": 35
        },
        {
            "title": "Bridging the gap: A survey on integrating (human) feedback for natural language generation",
            "authors": "Patrick Fernandes, Aman Madaan, Emmy Liu, Ant\u00f3nio Farinhas, Pedro Henrique Martins, Amanda Bertsch, Jose GC de Souza, Shuyan Zhou, Tongshuang Wu, Graham Neubig, Andre FT Martins",
            "year": "2023",
            "abstract": "Many recent advances in natural language generation have been fueled by training large language models on internet-scale data. However, this paradigm can lead to models that generate toxic, inaccurate, and unhelpful content, and automatic evaluation metrics often fail to identify these behaviors. As models become more capable, human feedback is an invaluable signal for evaluating and improving models. This survey aims to provide an overview of the recent research that has leveraged human feedback to improve natural language generation. First, we introduce an encompassing formalization of feedback, and identify and organize existing research into a taxonomy following this formalization. Next, we discuss how feedback can be described by its format and objective, and cover the two approaches proposed to use feedback (either for training or decoding): directly using the feedback or training feedback models. We also discuss existing datasets for human-feedback data collection, and concerns surrounding feedback collection. Finally, we provide an overview of the nascent field of AI feedback, which exploits large language models to make judgments based on a set of principles and minimize the need for human intervention.",
            "url": "https://arxiv.org/pdf/2305.00955",
            "publicationVenue": "TACL",
            "citation_count": 31
        },
        {
            "title": "DIRE and its Data: Neural Decompiled Variable Renamings with Respect to Software Class",
            "authors": "Luke Dramko, Jeremy Lacomis, Pengcheng Yin, Ed Schwartz, Miltiadis Allamanis, Graham Neubig, Bogdan Vasilescu, Claire Le Goues",
            "year": "2023",
            "abstract": "Thedecompileris one of the most common tools for examining executable binaries without the corresponding source code. It transforms binaries into high-level code, reversing the compilation process. Unfortunately, decompiler output is far from readable because the decompilation process is often incomplete. State-of-the-art techniques use machine learning to predict missing information like variable names. While these approaches are often able to suggest good variable names in context, no existing work examines how the selection of training data influences these machine learning models. We investigate how data provenance and the quality of training data affect performance, and how well, if at all, trained models generalize across software domains. We focus on the variable renaming problem using one such machine learning model,DIRE. We first describeDIREin detail and the accompanying technique\u00a0\u2026",
            "url": "https://dl.acm.org/doi/pdf/10.1145/3546946",
            "publicationVenue": "ACM Transactions on Software Engineering and Methodology",
            "citation_count": 3
        },
        {
            "title": "A Gold Standard Dataset for the Reviewer Assignment Problem",
            "authors": "Ivan Stelmakh, John Wieting, Graham Neubig, Nihar B Shah",
            "year": "2023",
            "abstract": "Many peer-review venues are either using or looking to use algorithms to assign submissions to reviewers. The crux of such automated approaches is the notion of the \"similarity score\"--a numerical estimate of the expertise of a reviewer in reviewing a paper--and many algorithms have been proposed to compute these scores. However, these algorithms have not been subjected to a principled comparison, making it difficult for stakeholders to choose the algorithm in an evidence-based manner. The key challenge in comparing existing algorithms and developing better algorithms is the lack of the publicly available gold-standard data that would be needed to perform reproducible research. We address this challenge by collecting a novel dataset of similarity scores that we release to the research community. Our dataset consists of 477 self-reported expertise scores provided by 58 researchers who evaluated their expertise in reviewing papers they have read previously. We use this data to compare several popular algorithms employed in computer science conferences and come up with recommendations for stakeholders. Our main findings are as follows. First, all algorithms make a non-trivial amount of error. For the task of ordering two papers in terms of their relevance for a reviewer, the error rates range from 12%-30% in easy cases to 36%-43% in hard cases, highlighting the vital need for more research on the similarity-computation problem. Second, most existing algorithms are designed to work with titles and abstracts of papers, and in this regime the Specter+MFR algorithm performs best. Third, to improve performance, it may be important to\u00a0\u2026",
            "url": "https://arxiv.org/pdf/2303.16750",
            "publicationVenue": "arXiv preprint arXiv:2303.16750",
            "citation_count": 6
        },
        {
            "title": "Computational Language Acquisition with Theory of Mind",
            "authors": "Andy Liu, Hao Zhu, Emmy Liu, Yonatan Bisk, Graham Neubig",
            "year": "2023",
            "abstract": "Unlike current state-of-the-art language models, young children actively acquire language through interactions with their surrounding environment and caretakers. One mechanism that has been argued to be critical to language learning is the ability to infer the mental states of other agents in social environments, coined Theory of Mind (ToM) by Premack & Woodruff (1978). Drawing inspiration from the modern operationalized versions of ToM implemented in Rabinowitz et al. (2018) and Zhu et al. (2021), we build language-learning agents equipped with ToM, and measure its effects on the learning process. We model ToM by giving the speaker agent an internal listener model that is trained alongside the speaker and used to rerank potential utterances. We experiment with varying task difficulty, hypothesizing that models will acquire more complex language to adapt to stronger environmental pressures. We find that training speakers with a highly weighted ToM listener component leads to performance gains in our image referential game setting. We also find some evidence that increasing task difficulty in the training process results in more fluent and precise utterances in evaluation. This suggests the potential utility of further incorporating ToM, as well as other insights from child language acquisition, into computational models of language acquisition.",
            "url": "https://arxiv.org/pdf/2303.01502",
            "publicationVenue": "ICLR 2023",
            "citation_count": 6
        },
        {
            "title": "User-Centric Evaluation of OCR Systems for Kwak'wala",
            "authors": "Shruti Rijhwani, Daisy Rosenblum, Michayla King, Antonios Anastasopoulos, Graham Neubig",
            "year": "2023",
            "abstract": "There has been recent interest in improving optical character recognition (OCR) for endangered languages, particularly because a large number of documents and books in these languages are not in machine-readable formats. The performance of OCR systems is typically evaluated using automatic metrics such as character and word error rates. While error rates are useful for the comparison of different models and systems, they do not measure whether and how the transcriptions produced from OCR tools are useful to downstream users. In this paper, we present a human-centric evaluation of OCR systems, focusing on the Kwak'wala language as a case study. With a user study, we show that utilizing OCR reduces the time spent in the manual transcription of culturally valuable documents -- a task that is often undertaken by endangered language community members and researchers -- by over 50%. Our results demonstrate the potential benefits that OCR tools can have on downstream language documentation and revitalization efforts.",
            "url": "https://arxiv.org/pdf/2302.13410",
            "publicationVenue": "arXiv preprint arXiv:2302.13410",
            "citation_count": 0
        },
        {
            "title": "Learning performance-improving code edits",
            "authors": "Alexander Shypula, Aman Madaan, Yimeng Zeng, Uri Alon, Jacob Gardner, Milad Hashemi, Graham Neubig, Parthasarathy Ranganathan, Osbert Bastani, Amir Yazdanbakhsh",
            "year": "2023",
            "abstract": "With the waning of Moore's law, optimizing program performance has become a major focus of software research. However, high-level optimizations such as API and algorithm changes remain elusive due to the difficulty of understanding the semantics of code. Simultaneously, pretrained large language models (LLMs) have demonstrated strong capabilities at solving a wide range of programming tasks. To that end, we introduce a framework for adapting LLMs to high-level program optimization. First, we curate a dataset of performance-improving edits made by human programmers of over 77K competitive C++ programming submission pairs, accompanied by extensive unit tests. A major challenge is the significant variability of measuring performance on commodity hardware, which can lead to spurious \"improvements\". To isolate and reliably evaluate the impact of program optimizations, we design an environment based on the gem5 full system simulator, the de facto simulator used in academia and industry. Next, we propose a broad range of adaptation strategies for code optimization; for prompting, these include retrieval-based few-shot prompting and chain-of-thought, and for finetuning, these include performance-conditioned generation and synthetic data augmentation based on self-play. A combination of these techniques achieves an average speedup of 5.65X on CodeLlama-13B and 6.86X on GPT-3.5, surpassing the best human performance (4.06X). We find our proposed performance-conditioned generation is particularly effective at improving performance as well as increasing the fraction of optimized programs.",
            "url": "https://arxiv.org/pdf/2302.07867",
            "publicationVenue": "arXiv preprint arXiv:2302.07867",
            "citation_count": 31
        },
        {
            "title": "Cross-Modal Fine-Tuning: Align then Refine",
            "authors": "Junhong Shen, Liam Li, Lucio M Dery, Corey Staten, Mikhail Khodak, Graham Neubig, Ameet Talwalkar",
            "year": "2023",
            "abstract": "Fine-tuning large-scale pretrained models has led to tremendous progress in well-studied modalities such as vision and NLP. However, similar gains have not been observed in many other modalities due to a lack of relevant pretrained models. In this work, we propose ORCA, a general cross-modal fine-tuning framework that extends the applicability of a single large-scale pretrained model to diverse modalities. ORCA adapts to a target task via an align-then-refine workflow: given the target input, ORCA first learns an embedding network that aligns the embedded feature distribution with the pretraining modality. The pretrained model is then fine-tuned on the embedded data to exploit the knowledge shared across modalities. Through extensive experiments, we show that ORCA obtains state-of-the-art results on 3 benchmarks containing over 60 datasets from 12 modalities, outperforming a wide range of hand-designed, AutoML, general-purpose, and task-specific methods. We highlight the importance of data alignment via a series of ablation studies and demonstrate ORCA's utility in data-limited regimes.",
            "url": "https://arxiv.org/pdf/2302.05738",
            "publicationVenue": "ICML 2023",
            "citation_count": 7
        },
        {
            "title": "Codebertscore: Evaluating code generation with pretrained models of code",
            "authors": "Shuyan Zhou, Uri Alon, Sumit Agarwal, Graham Neubig",
            "year": "2023",
            "abstract": "Since the rise of neural models of code that can generate long expressions and statements rather than a single next-token, one of the major problems has been reliably evaluating their generated output. In this paper, we propose CodeBERTScore: an automatic evaluation metric for code generation, which builds on BERTScore (Zhang et al., 2020). Instead of measuring exact token matching as BLEU, CodeBERTScore computes a soft similarity score between each token in the generated code and in the reference code, using the contextual encodings of large pretrained models. Further, instead of encoding only the generated tokens as in BERTScore, CodeBERTScore also encodes the programmatic context surrounding the generated code. We perform an extensive evaluation of CodeBERTScore across four programming languages. We find that CodeBERTScore achieves a higher correlation with human preference and with functional correctness than all existing metrics. That is, generated code that receives a higher score by CodeBERTScore is more likely to be preferred by humans, as well as to function correctly when executed. Finally, while CodeBERTScore can be used with a multilingual CodeBERT as its base model, we release five language-specific pretrained models to use with our publicly available code at https://github.com/neulab/code-bert-score . Our language-specific models have been downloaded more than 25,000 times from the Huggingface Hub.",
            "url": "https://arxiv.org/pdf/2302.05527",
            "publicationVenue": "EMNLP 2023",
            "citation_count": 20
        },
        {
            "title": "Why do Nearest Neighbor Language Models Work?",
            "authors": "Frank F Xu, Uri Alon, Graham Neubig",
            "year": "2023",
            "abstract": "Language models (LMs) compute the probability of a text by sequentially computing a representation of an already-seen context and using this representation to predict the next word. Currently, most LMs calculate these representations through a neural network consuming the immediate previous context. However recently, retrieval-augmented LMs have shown to improve over standard neural LMs, by accessing information retrieved from a large datastore, in addition to their standard, parametric, next-word prediction. In this paper, we set out to understand why retrieval-augmented language models, and specifically why k-nearest neighbor language models (kNN-LMs) perform better than standard parametric LMs, even when the k-nearest neighbor component retrieves examples from the same training set that the LM was originally trained on. To this end, we perform a careful analysis of the various dimensions over which kNN-LM diverges from standard LMs, and investigate these dimensions one by one. Empirically, we identify three main reasons why kNN-LM performs better than standard LMs: using a different input representation for predicting the next tokens, approximate kNN search, and the importance of softmax temperature for the kNN distribution. Further, we incorporate these insights into the model architecture or the training procedure of the standard parametric LM, improving its results without the need for an explicit retrieval component. The code is available at https://github.com/frankxu2004/knnlm-why.",
            "url": "https://arxiv.org/pdf/2301.02828",
            "publicationVenue": "ICML 2023",
            "citation_count": 8
        },
        {
            "title": "EXCALIBUR: Encouraging and Evaluating Embodied Exploration",
            "authors": "Hao Zhu, Raghav Kapoor, So Yeon Min, Winson Han, Jiatai Li, Kaiwen Geng, Graham Neubig, Yonatan Bisk, Aniruddha Kembhavi, Luca Weihs",
            "year": "2023",
            "abstract": "Experience precedes understanding. Humans constantly explore and learn about their environment out of curiosity, gather information, and update their models of the world. On the other hand, machines are either trained to learn passively from static and fixed datasets, or taught to complete specific goal-conditioned tasks. To encourage the development of exploratory interactive agents, we present the EXCALIBUR benchmark. EXCALIBUR allows agents to explore their environment for long durations and then query their understanding of the physical world via inquiries like:\" is the small heavy red bowl made from glass?\" or\" is there a silver spoon heavier than the egg?\". This design encourages agents to perform free-form home exploration without myopia induced by goal conditioning. Once the agents have answered a series of questions, they can renter the scene to refine their knowledge, update their beliefs, and improve their performance on the questions. Our experiments demonstrate the challenges posed by this dataset for the present-day state-of-the-art embodied systems and the headroom afforded to develop new innovative methods. Finally, we present a virtual reality interface that enables humans to seamlessly interact within the simulated world and use it to gather human performance measures. EXCALIBUR affords unique challenges in comparison to present-day benchmarks and represents the next frontier for embodied AI research.",
            "url": "http://openaccess.thecvf.com/content/CVPR2023/papers/Zhu_EXCALIBUR_Encouraging_and_Evaluating_Embodied_Exploration_CVPR_2023_paper.pdf",
            "publicationVenue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "citation_count": 5
        }
    ],
    "Eric Nyberg": [
        {
            "title": "Language-agnostic transformers and assessing ChatGPT-based query rewriting for multilingual document-grounded QA",
            "authors": "Srinivas Gowriraj, Soham Dinesh Tiwari, Mitali Potnis, Srijan Bansal, Teruko Mitamura, Eric Nyberg",
            "year": "2023",
            "abstract": "The DialDoc 2023 shared task has expanded the document-grounded dialogue task to encompass multiple languages, despite having limited annotated data. This paper assesses the effectiveness of both language-agnostic and language-aware paradigms for multilingual pre-trained transformer models in a bi-encoder-based dense passage retriever (DPR), concluding that the language-agnostic approach is superior. Additionally, the study investigates the impact of query rewriting techniques using large language models, such as ChatGPT, on multilingual, document-grounded question-answering systems. The experiments conducted demonstrate that, for the examples examined, query rewriting does not enhance performance compared to the original queries. This failure is due to topic switching in final dialogue turns and irrelevant topics being considered for query rewriting.",
            "url": "https://aclanthology.org/2023.dialdoc-1.11.pdf",
            "publicationVenue": "Proceedings of the Third DialDoc Workshop on Document-grounded Dialogue and Conversational Question Answering",
            "citation_count": 2
        },
        {
            "title": "Difference-Masking: Choosing What to Mask in Continued Pretraining",
            "authors": "Alex Wilf, Syeda Nahida Akter, Leena Mathur, Paul Pu Liang, Sheryl Mathew, Mengrou Shou, Eric Nyberg, Louis-Philippe Morency",
            "year": "2023",
            "abstract": "Self-supervised learning (SSL) and the objective of masking-and-predicting in particular have led to promising SSL performance on a variety of downstream tasks. However, while most approaches randomly mask tokens, there is strong intuition from the field of education that deciding what to mask can substantially improve learning outcomes. We introduce Difference-Masking, an approach that automatically chooses what to mask during continued pretraining by considering what makes an unlabelled target domain different from the pretraining domain. Empirically, we find that Difference-Masking outperforms baselines on continued pretraining settings across four diverse language and multimodal video tasks. The cross-task applicability of Difference-Masking supports the effectiveness of our framework for SSL pretraining in language, vision, and other domains.",
            "url": "https://arxiv.org/pdf/2305.14577",
            "publicationVenue": "arXiv preprint arXiv:2305.14577",
            "citation_count": 0
        },
        {
            "title": "Chain-of-Skills: A Configurable Model for Open-domain Question Answering",
            "authors": "Kaixin Ma, Hao Cheng, Yu Zhang, Xiaodong Liu, Eric Nyberg, Jianfeng Gao",
            "year": "2023",
            "abstract": "The retrieval model is an indispensable component for real-world knowledge-intensive tasks, e.g., open-domain question answering (ODQA). As separate retrieval skills are annotated for different datasets, recent work focuses on customized methods, limiting the model transferability and scalability. In this work, we propose a modular retriever where individual modules correspond to key skills that can be reused across datasets. Our approach supports flexible skill configurations based on the target domain to boost performance. To mitigate task interference, we design a novel modularization parameterization inspired by sparse Transformer. We demonstrate that our model can benefit from self-supervised pretraining on Wikipedia and fine-tuning using multiple ODQA datasets, both in a multi-task fashion. Our approach outperforms recent self-supervised retrievers in zero-shot evaluations and achieves state-of-the-art fine-tuned retrieval performance on NQ, HotpotQA and OTT-QA.",
            "url": "https://arxiv.org/pdf/2305.03130",
            "publicationVenue": "arXiv preprint arXiv:2305.03130",
            "citation_count": 7
        },
        {
            "title": "GameQA: Gamified Mobile App Platform for Building Multiple-Domain Question-Answering Datasets",
            "authors": "Njall Skarphedinsson, Breki Gudmundsson, Steinar Smari, Marta Kristin Larusdottir, Hafsteinn Einarsson, Abuzar Khan, Eric Nyberg, Hrafn Loftsson",
            "year": "2023",
            "abstract": "The methods used to create many of the well-known Question-Answering (QA) datasets are hard to replicate for low-resource languages. A commonality amongst these methods is hiring annotators to source answers from the internet by querying a single answer source, such as Wikipedia. Applying these methods for low-resource languages can be problematic since there is no single large answer source for these languages. Consequently, this can result in a high ratio of unanswered questions, since the amount of information in any single source is limited. To address this problem, we developed a novel crowd-sourcing platform to gather multiple-domain QA data for low-resource languages. Our platform, which consists of a mobile app and a web API, gamifies the data collection process. We successfully released the app for Icelandic (a low-resource language with about 350,000 native speakers) to build a dataset which rivals large QA datasets for high-resource languages both in terms of size and ratio of answered questions. We have made the platform open source with instructions on how to localize and deploy it to gather data for other low-resource languages.",
            "url": "https://aclanthology.org/2023.eacl-demo.18.pdf",
            "publicationVenue": "Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics: System Demonstrations",
            "citation_count": 0
        },
        {
            "title": "Using Implicit Feedback to Improve Question Generation",
            "authors": "Hugo Rodrigues, Eric Nyberg, Luisa Coheur",
            "year": "2023",
            "abstract": "Question Generation (QG) is a task of Natural Language Processing (NLP) that aims at automatically generating questions from text. Many applications can benefit from automatically generated questions, but often it is necessary to curate those questions, either by selecting or editing them. This task is informative on its own, but it is typically done post-generation, and, thus, the effort is wasted. In addition, most existing systems cannot incorporate this feedback back into them easily. In this work, we present a system, GEN, that learns from such (implicit) feedback. Following a pattern-based approach, it takes as input a small set of sentence/question pairs and creates patterns which are then applied to new unseen sentences. Each generated question, after being corrected by the user, is used as a new seed in the next iteration, so more patterns are created each time. We also take advantage of the corrections made by the user to score the patterns and therefore rank the generated questions. Results show that GEN is able to improve by learning from both levels of implicit feedback when compared to the version with no learning, considering the top 5, 10, and 20 questions. Improvements go up from 10%, depending on the metric and strategy used.",
            "url": "https://arxiv.org/pdf/2304.13664",
            "publicationVenue": "arXiv preprint arXiv:2304.13664",
            "citation_count": 0
        },
        {
            "title": "InPars-Light: Cost-Effective Unsupervised Training of Efficient Rankers",
            "authors": "Leonid Boytsov, Preksha Patel, Vivek Sourabh, Riddhi Nisar, Sayani Kundu, Ramya Ramanathan, Eric Nyberg",
            "year": "2023",
            "abstract": "We carried out a reproducibility study of InPars recipe for unsupervised training of neural rankers. As a by-product of this study, we developed a simple-yet-effective modification of InPars, which we called InPars-light. Unlike InPars, InPars-light uses only a freely available language model BLOOM and 7x-100x smaller ranking models. On all five English retrieval collections (used in the original InPars study) we obtained substantial (7-30%) and statistically significant improvements over BM25 in nDCG or MRR using only a 30M parameter six-layer MiniLM ranker. In contrast, in the InPars study only a 100x larger MonoT5-3B model consistently outperformed BM25, whereas their smaller MonoT5-220M model (which is still 7x larger than our MiniLM ranker), outperformed BM25 only on MS MARCO and TREC DL 2020. In a purely unsupervised setting, our 435M parameter DeBERTA v3 ranker was roughly at par with the 7x larger MonoT5-3B: In fact, on three out of five datasets, it slightly outperformed MonoT5-3B. Finally, these good results were achieved by re-ranking only 100 candidate documents compared to 1000 used in InPars. We believe that InPars-light is the first truly cost-effective prompt-based unsupervised recipe to train and deploy neural ranking models that outperform BM25.",
            "url": "https://arxiv.org/pdf/2301.02998",
            "publicationVenue": "arXiv preprint arXiv:2301.02998",
            "citation_count": 12
        }
    ],
    "Kemal Oflazer": [
        {
            "title": "Counting the Bugs in ChatGPT's Wugs: A Multilingual Investigation into the Morphological Capabilities of a Large Language Model",
            "authors": "Leonie Weissweiler, Valentin Hofmann, Anjali Kantharuban, Anna Cai, Ritam Dutt, Amey Hengle, Anubha Kabra, Atharva Kulkarni, Abhishek Vijayakumar, Haofei Yu, Hinrich Sch\u00fctze, Kemal Oflazer, David R Mortensen",
            "year": "2023",
            "abstract": "Large language models (LLMs) have recently reached an impressive level of linguistic capability, prompting comparisons with human language skills. However, there have been relatively few systematic inquiries into the linguistic capabilities of the latest generation of LLMs, and those studies that do exist (i) ignore the remarkable ability of humans to generalize, (ii) focus only on English, and (iii) investigate syntax or semantics and overlook other capabilities that lie at the heart of human language, like morphology. Here, we close these gaps by conducting the first rigorous analysis of the morphological capabilities of ChatGPT in four typologically varied languages (specifically, English, German, Tamil, and Turkish). We apply a version of Berko's (1958) wug test to ChatGPT, using novel, uncontaminated datasets for the four examined languages. We find that ChatGPT massively underperforms purpose-built systems, particularly in English. Overall, our results -- through the lens of morphology -- cast a new light on the linguistic capabilities of ChatGPT, suggesting that claims of human-like language skills are premature and misleading.",
            "url": "https://arxiv.org/pdf/2310.15113",
            "publicationVenue": "arXiv preprint arXiv:2310.15113",
            "citation_count": 1
        },
        {
            "title": "Abstractive summarization with deep reinforcement learning using semantic similarity rewards",
            "authors": "Figen Beken Fikri, Kemal Oflazer, Berrin Yan\u0131ko\u011flu",
            "year": "2023",
            "abstract": "Abstractive summarization is an approach to document summarization that is not limited to selecting sentences from the document but can generate new sentences as well. We address the two main challenges in abstractive summarization: how to evaluate the performance of a summarization model and what is a good training objective. We first introduce new evaluation measures based on the semantic similarity of the input and corresponding summary. The similarity scores are obtained by the fine-tuned BERTurk model using either the cross-encoder or a bi-encoder architecture. The fine-tuning is done on the Turkish Natural Language Inference and Semantic Textual Similarity benchmark datasets. We show that these measures have better correlations with human evaluations compared to Recall-Oriented Understudy for Gisting Evaluation (ROUGE) scores and BERTScore. We then introduce a deep reinforcement learning algorithm that uses the proposed semantic similarity measures as rewards, together with a mixed training objective, in order to generate more natural summaries in terms of human readability. We show that training with a mixed training objective function compared to only the maximum-likelihood objective improves similarity scores.",
            "url": null,
            "publicationVenue": "Natural Language Engineering",
            "citation_count": 0
        }
    ],
    "Bhiksha Raj": [
        {
            "title": "Espnet-Summ: Introducing a Novel Large Dataset, Toolkit, and a Cross-Corpora Evaluation of Speech Summarization Systems",
            "authors": "Roshan Sharma, William Chen, Takatomo Kano, Ruchira Sharma, Siddhant Arora, Shinji Watanabe, Atsunori Ogawa, Marc Delcroix, Rita Singh, Bhiksha Raj",
            "year": "2023",
            "abstract": "Speech summarization has garnered significant interest and progressed rapidly over the past few years. In particular, end-to-end models have recently emerged as a competitive alternative to cascade systems for abstractive video summarization. This paper aims to establish progress in this rapidly evolving research field, by introducing ESPNet-SUMM, a new open-source toolkit that facilitates a comprehensive comparison of end-to-end and cascade speech summarization models on 4 different speech summarization tasks spanning diverse applications. Experiments demonstrate that end-to-end models perform better for larger corpora with shorter inputs. This work also introduces Interview, the largest public open-domain multiparty interview corpus withof conversations between radio hosts and guests. Finally, this work explores the use of multiple datasets to improve end-to-end summarization, and\u00a0\u2026",
            "url": null,
            "publicationVenue": "2023 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)",
            "citation_count": 0
        },
        {
            "title": "Towards noise-tolerant speech-referring video object segmentation: Bridging speech and text",
            "authors": "Xiang Li, Jinglu Wang, Xiaohao Xu, Muqiao Yang, Fan Yang, Yizhou Zhao, Rita Singh, Bhiksha Raj",
            "year": "2023",
            "abstract": "Linguistic communication is prevalent in Human-Computer Interaction (HCI). Speech (spoken language) serves as a convenient yet potentially ambiguous form due to noise and accents, exposing a gap compared to text. In this study, we investigate the prominent HCI task, Referring Video Object Segmentation (R-VOS), which aims to segment and track objects using linguistic references. While text input is well-investigated, speech input is under-explored. Our objective is to bridge the gap between speech and text, enabling the adaptation of existing text-input R-VOS models to accommodate noisy speech input effectively. Specifically, we propose a method to align the semantic spaces between speech and text by incorporating two key modules: 1) Noise-Aware Semantic Adjustment (NSA) for clear semantics extraction from noisy speech; and 2) Semantic Jitter Suppression (SJS) enabling R-VOS models to tolerate noisy queries. Comprehensive experiments conducted on the challenging AVOS benchmarks reveal that our proposed method outperforms state-of-the-art approaches.",
            "url": "https://aclanthology.org/2023.emnlp-main.140.pdf",
            "publicationVenue": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
            "citation_count": 1
        },
        {
            "title": "FALCON: Fairness Learning via Contrastive Attention Approach to Continual Semantic Scene Understanding in Open World",
            "authors": "Thanh-Dat Truong, Utsav Prabhu, Bhiksha Raj, Jackson Cothren, Khoa Luu",
            "year": "2023",
            "abstract": "Continual Learning in semantic scene segmentation aims to continually learn new unseen classes in dynamic environments while maintaining previously learned knowledge. Prior studies focused on modeling the catastrophic forgetting and background shift challenges in continual learning. However, fairness, another major challenge that causes unfair predictions leading to low performance among major and minor classes, still needs to be well addressed. In addition, prior methods have yet to model the unknown classes well, thus resulting in producing non-discriminative features among unknown classes. This paper presents a novel Fairness Learning via Contrastive Attention Approach to continual learning in semantic scene understanding. In particular, we first introduce a new Fairness Contrastive Clustering loss to address the problems of catastrophic forgetting and fairness. Then, we propose an attention-based visual grammar approach to effectively model the background shift problem and unknown classes, producing better feature representations for different unknown classes. Through our experiments, our proposed approach achieves State-of-the-Art (SOTA) performance on different continual learning settings of three standard benchmarks, i.e., ADE20K, Cityscapes, and Pascal VOC. It promotes the fairness of the continual semantic segmentation model.",
            "url": "https://arxiv.org/pdf/2311.15965",
            "publicationVenue": "arXiv preprint arXiv:2311.15965",
            "citation_count": 0
        },
        {
            "title": "Token Prediction as Implicit Classification to Identify LLM-Generated Text",
            "authors": "Yutian Chen, Hao Kang, Vivian Zhai, Liangze Li, Rita Singh, Bhiksha Raj",
            "year": "2023",
            "abstract": "This paper introduces a novel approach for identifying the possible large language models (LLMs) involved in text generation. Instead of adding an additional classification layer to a base LM, we reframe the classification task as a next-token prediction task and directly fine-tune the base LM to perform it. We utilize the Text-to-Text Transfer Transformer (T5) model as the backbone for our experiments. We compared our approach to the more direct approach of utilizing hidden states for classification. Evaluation shows the exceptional performance of our method in the text classification task, highlighting its simplicity and efficiency. Furthermore, interpretability studies on the features extracted by our model reveal its ability to differentiate distinctive writing styles among various LLMs even in the absence of an explicit classifier. We also collected a dataset named OpenLLMText, containing approximately 340k text samples from human and LLMs, including GPT3.5, PaLM, LLaMA, and GPT2.",
            "url": "https://arxiv.org/pdf/2311.08723",
            "publicationVenue": "arXiv preprint arXiv:2311.08723",
            "citation_count": 2
        },
        {
            "title": "Rethinking Voice-Face Correlation: A Geometry View",
            "authors": "Xiang Li, Yandong Wen, Muqiao Yang, Jinglu Wang, Rita Singh, Bhiksha Raj",
            "year": "2023",
            "abstract": "Previous works on voice-face matching and voice-guided face synthesis demonstrate strong correlations between voice and face, but mainly rely on coarse semantic cues such as gender, age, and emotion. In this paper, we aim to investigate the capability of reconstructing the 3D facial shape from voice from a geometry perspective without any semantic information. We propose a voice-anthropometric measurement (AM)-face paradigm, which identifies predictable facial AMs from the voice and uses them to guide 3D face reconstruction. By leveraging AMs as a proxy to link the voice and face geometry, we can eliminate the influence of unpredictable AMs and make the face geometry tractable. Our approach is evaluated on our proposed dataset with ground-truth 3D face scans and corresponding voice recordings, and we find significant correlations between voice and specific parts of the face geometry, such as the\u00a0\u2026",
            "url": "https://arxiv.org/pdf/2307.13948",
            "publicationVenue": "Proceedings of the 31st ACM International Conference on Multimedia",
            "citation_count": 0
        },
        {
            "title": "Psychoacoustic Challenges Of Speech Enhancement On VoIP Platforms",
            "authors": "Joseph Konan, Ojas Bhargave, Shikhar Agnihotri, Shuo Han, Yunyang Zeng, Ankit Shah, Bhiksha Raj",
            "year": "2023",
            "abstract": "Within the ambit of VoIP (Voice over Internet Protocol) telecommunications, the complexities introduced by acoustic transformations merit rigorous analysis. This research, rooted in the exploration of proprietary sender-side denoising effects, meticulously evaluates platforms such as Google Meets and Zoom. The study draws upon the Deep Noise Suppression (DNS) 2020 dataset, ensuring a structured examination tailored to various denoising settings and receiver interfaces. A methodological novelty is introduced via the Oaxaca decomposition, traditionally an econometric tool, repurposed herein to analyze acoustic-phonetic perturbations within VoIP systems. To further ground the implications of these transformations, psychoacoustic metrics, specifically PESQ and STOI, were harnessed to furnish a comprehensive understanding of speech alterations. Cumulatively, the insights garnered underscore the intricate landscape of VoIP-influenced acoustic dynamics. In addition to the primary findings, a multitude of metrics are reported, extending the research purview. Moreover, out-of-domain benchmarking for both time and time-frequency domain speech enhancement models is included, thereby enhancing the depth and applicability of this inquiry.",
            "url": "https://arxiv.org/pdf/2310.07161",
            "publicationVenue": "arXiv preprint arXiv:2310.07161",
            "citation_count": 0
        },
        {
            "title": "Privacy-oriented manipulation of speaker representations",
            "authors": "Francisco Teixeira, Alberto Abad, Bhiksha Raj, Isabel Trancoso",
            "year": "2023",
            "abstract": "Speaker embeddings are ubiquitous, with applications ranging from speaker recognition and diarization to speech synthesis and voice anonymisation. The amount of information held by these embeddings lends them versatility, but also raises privacy concerns. Speaker embeddings have been shown to contain information on age, sex, health and more, which speakers may want to keep private, especially when this information is not required for the target task. In this work, we propose a method for removing and manipulating private attributes from speaker embeddings that leverages a Vector-Quantized Variational Autoencoder architecture, combined with an adversarial classifier and a novel mutual information loss. We validate our model on two attributes, sex and age, and perform experiments with ignorant and fully-informed attackers, and with in-domain and out-of-domain data.",
            "url": "https://arxiv.org/pdf/2310.06652",
            "publicationVenue": "arXiv preprint arXiv:2310.06652",
            "citation_count": 0
        },
        {
            "title": "Continual Contrastive Spoken Language Understanding",
            "authors": "Umberto Cappellazzo, Enrico Fini, Muqiao Yang, Daniele Falavigna, Alessio Brutti, Bhiksha Raj",
            "year": "2023",
            "abstract": "Recently, neural networks have shown impressive progress across diverse fields, with speech processing being no exception. However, recent breakthroughs in this area require extensive offline training using large datasets and tremendous computing resources. Unfortunately, these models struggle to retain their previously acquired knowledge when learning new tasks continually, and retraining from scratch is almost always impractical. In this paper, we investigate the problem of learning sequence-to-sequence models for spoken language understanding in a class-incremental learning (CIL) setting and we propose COCONUT, a CIL method that relies on the combination of experience replay and contrastive learning. Through a modified version of the standard supervised contrastive loss applied only to the rehearsal samples, COCONUT preserves the learned representations by pulling closer samples from the same class and pushing away the others. Moreover, we leverage a multimodal contrastive loss that helps the model learn more discriminative representations of the new data by aligning audio and text features. We also investigate different contrastive designs to combine the strengths of the contrastive loss with teacher-student architectures used for distillation. Experiments on two established SLU datasets reveal the effectiveness of our proposed approach and significant improvements over the baselines. We also show that COCONUT can be combined with methods that operate on the decoder side of the model, resulting in further metrics improvements.",
            "url": "https://arxiv.org/pdf/2310.02699",
            "publicationVenue": "arXiv preprint arXiv:2310.02699",
            "citation_count": 1
        },
        {
            "title": "Prompting Audios Using Acoustic Properties For Emotion Representation",
            "authors": "Hira Dhamyal, Benjamin Elizalde, Soham Deshmukh, Huaming Wang, Bhiksha Raj, Rita Singh",
            "year": "2023",
            "abstract": "Emotions lie on a continuum, but current models treat emotions as a finite valued discrete variable. This representation does not capture the diversity in the expression of emotion. To better represent emotions we propose the use of natural language descriptions (or prompts). In this work, we address the challenge of automatically generating these prompts and training a model to better learn emotion representations from audio and prompt pairs. We use acoustic properties that are correlated to emotion like pitch, intensity, speech rate, and articulation rate to automatically generate prompts i.e. 'acoustic prompts'. We use a contrastive learning objective to map speech to their respective acoustic prompts. We evaluate our model on Emotion Audio Retrieval and Speech Emotion Recognition. Our results show that the acoustic prompts significantly improve the model's performance in EAR, in various Precision@K metrics. In SER, we observe a 3.8% relative accuracy improvement on the Ravdess dataset.",
            "url": "https://arxiv.org/pdf/2310.02298",
            "publicationVenue": "arXiv preprint arXiv:2310.02298",
            "citation_count": 1
        },
        {
            "title": "Loft: Local proxy fine-tuning for improving transferability of adversarial attacks against large language model",
            "authors": "Muhammad Ahmed Shah, Roshan Sharma, Hira Dhamyal, Raphael Olivier, Ankit Shah, Dareen Alharthi, Hazim T Bukhari, Massa Baali, Soham Deshmukh, Michael Kuhlmann, Bhiksha Raj, Rita Singh",
            "year": "2023",
            "abstract": "It has been shown that Large Language Model (LLM) alignments can be circumvented by appending specially crafted attack suffixes with harmful queries to elicit harmful responses. To conduct attacks against private target models whose characterization is unknown, public models can be used as proxies to fashion the attack, with successful attacks being transferred from public proxies to private target models. The success rate of attack depends on how closely the proxy model approximates the private model. We hypothesize that for attacks to be transferrable, it is sufficient if the proxy can approximate the target model in the neighborhood of the harmful query. Therefore, in this paper, we propose \\emph{Local Fine-Tuning (LoFT)}, \\textit{i.e.}, fine-tuning proxy models on similar queries that lie in the lexico-semantic neighborhood of harmful queries to decrease the divergence between the proxy and target models. First, we demonstrate three approaches to prompt private target models to obtain similar queries given harmful queries. Next, we obtain data for local fine-tuning by eliciting responses from target models for the generated similar queries. Then, we optimize attack suffixes to generate attack prompts and evaluate the impact of our local fine-tuning on the attack's success rate. Experiments show that local fine-tuning of proxy models improves attack transferability and increases attack success rate by,, and(absolute) on target models ChatGPT, GPT-4, and Claude respectively.",
            "url": "https://arxiv.org/pdf/2310.04445",
            "publicationVenue": "arXiv preprint arXiv:2310.04445",
            "citation_count": 5
        },
        {
            "title": "uSee: Unified Speech Enhancement and Editing with Conditional Diffusion Models",
            "authors": "Muqiao Yang, Chunlei Zhang, Yong Xu, Zhongweiyang Xu, Heming Wang, Bhiksha Raj, Dong Yu",
            "year": "2023",
            "abstract": "Speech enhancement aims to improve the quality of speech signals in terms of quality and intelligibility, and speech editing refers to the process of editing the speech according to specific user needs. In this paper, we propose a Unified Speech Enhancement and Editing (uSee) model with conditional diffusion models to handle various tasks at the same time in a generative manner. Specifically, by providing multiple types of conditions including self-supervised learning embeddings and proper text prompts to the score-based diffusion model, we can enable controllable generation of the unified speech enhancement and editing model to perform corresponding actions on the source speech. Our experiments show that our proposed uSee model can achieve superior performance in both speech denoising and dereverberation compared to other related generative speech enhancement models, and can perform speech editing given desired environmental sound text description, signal-to-noise ratios (SNR), and room impulse responses (RIR). Demos of the generated speech are available at https://muqiaoy.github.io/usee.",
            "url": "https://arxiv.org/pdf/2310.00900",
            "publicationVenue": "arXiv preprint arXiv:2310.00900",
            "citation_count": 0
        },
        {
            "title": "Evaluating speech synthesis by training recognizers on synthetic speech",
            "authors": "Dareen Alharthi, Roshan Sharma, Hira Dhamyal, Soumi Maiti, Bhiksha Raj, Rita Singh",
            "year": "2023",
            "abstract": "Modern speech synthesis systems have improved significantly, with synthetic speech being indistinguishable from real speech. However, efficient and holistic evaluation of synthetic speech still remains a significant challenge. Human evaluation using Mean Opinion Score (MOS) is ideal, but inefficient due to high costs. Therefore, researchers have developed auxiliary automatic metrics like Word Error Rate (WER) to measure intelligibility. Prior works focus on evaluating synthetic speech based on pre-trained speech recognition models, however, this can be limiting since this approach primarily measures speech intelligibility. In this paper, we propose an evaluation technique involving the training of an ASR model on synthetic speech and assessing its performance on real speech. Our main assumption is that by training the ASR model on the synthetic speech, the WER on real speech reflects the similarity between distributions, a broader assessment of synthetic speech quality beyond intelligibility. Our proposed metric demonstrates a strong correlation with both MOS naturalness and MOS intelligibility when compared to SpeechLMScore and MOSNet on three recent Text-to-Speech (TTS) systems: MQTTS, StyleTTS, and YourTTS.",
            "url": "https://arxiv.org/pdf/2310.00706",
            "publicationVenue": "arXiv preprint arXiv:2310.00706",
            "citation_count": 1
        },
        {
            "title": "Completing visual objects via bridging generation and segmentation",
            "authors": "Xiang Li, Yinpeng Chen, Chung-Ching Lin, Rita Singh, Bhiksha Raj, Zicheng Liu",
            "year": "2023",
            "abstract": "This paper presents a novel approach to object completion, with the primary goal of reconstructing a complete object from its partially visible components. Our method, named MaskComp, delineates the completion process through iterative stages of generation and segmentation. In each iteration, the object mask is provided as an additional condition to boost image generation, and, in return, the generated images can lead to a more accurate mask by fusing the segmentation of images. We demonstrate that the combination of one generation and one segmentation stage effectively functions as a mask denoiser. Through alternation between the generation and segmentation stages, the partial object mask is progressively refined, providing precise shape guidance and yielding superior object completion results. Our experiments demonstrate the superiority of MaskComp over existing approaches, e.g., ControlNet and Stable Diffusion, establishing it as an effective solution for object completion.",
            "url": "https://arxiv.org/pdf/2310.00808",
            "publicationVenue": "arXiv preprint arXiv:2310.00808",
            "citation_count": 1
        },
        {
            "title": "Towards Robust Audiovisual Segmentation in Complex Environments with Quantization-based Semantic Decomposition",
            "authors": "Xiang Li, Jinglu Wang, Xiaohao Xu, Xiulian Peng, Rita Singh, Yan Lu, Bhiksha Raj",
            "year": "2023",
            "abstract": "Audiovisual segmentation (AVS) is a challenging task that aims to segment visual objects in videos according to their associated acoustic cues. With multiple sound sources and background disturbances involved, establishing robust correspondences between audio and visual contents poses unique challenges due to (1) complex entanglement across sound sources and (2) frequent changes in the occurrence of distinct sound events. Assuming sound events occur independently, the multi-source semantic space can be represented as the Cartesian product of single-source sub-spaces. We are motivated to decompose the multi-source audio semantics into single-source semantics for more effective interactions with visual content. We propose a semantic decomposition method based on product quantization, where the multi-source semantics can be decomposed and represented by several disentangled and noise-suppressed single-source semantics. Furthermore, we introduce a global-to-local quantization mechanism, which distills knowledge from stable global (clip-level) features into local (frame-level) ones, to handle frequent changes in audio semantics. Extensive experiments demonstrate that our semantically decomposed audio representation significantly improves AVS performance, e.g., +21.2% mIoU on the challenging AVS-Semantic benchmark with ResNet50 backbone. https://github.com/lxa9867/QSD.",
            "url": "https://arxiv.org/html/2310.00132v2",
            "publicationVenue": "arXiv preprint arXiv:2310.00132",
            "citation_count": 0
        },
        {
            "title": "Understanding and mitigating the label noise in pre-training on downstream tasks",
            "authors": "Hao Chen, Jindong Wang, Ankit Shah, Ran Tao, Hongxin Wei, Xing Xie, Masashi Sugiyama, Bhiksha Raj",
            "year": "2023",
            "abstract": "Pre-training on large-scale datasets and then fine-tuning on downstream tasks have become a standard practice in deep learning. However, pre-training data often contain label noise that may adversely affect the generalization of the model. This paper aims to understand the nature of noise in pre-training datasets and to mitigate its impact on downstream tasks. More specifically, through extensive experiments of supervised pre-training models on synthetic noisy ImageNet-1K and YFCC15M datasets, we demonstrate that while slight noise in pre-training can benefit in-domain (ID) transfer performance, where the training and testing data share the same distribution, it always deteriorates out-of-domain (OOD) performance, where training and testing data distribution are different. We empirically verify that the reason behind is noise in pre-training shapes the feature space differently. We then propose a lightweight black-box tuning method (NMTune) to affine the feature space to mitigate the malignant effect of noise and improve generalization on both ID and OOD tasks, considering one may not be able to fully fine-tune or even access the pre-trained models. We conduct practical experiments on popular vision and language models that are pre-trained on noisy data for evaluation of our approach. Our analysis and results show the importance of this interesting and novel research direction, which we term Noisy Model Learning.",
            "url": "https://arxiv.org/pdf/2309.17002",
            "publicationVenue": "arXiv preprint arXiv:2309.17002",
            "citation_count": 2
        },
        {
            "title": "Importance of negative sampling in weak label learning",
            "authors": "Ankit Shah, Fuyu Tang, Zelin Ye, Rita Singh, Bhiksha Raj",
            "year": "2023",
            "abstract": "Weak-label learning is a challenging task that requires learning from data \"bags\" containing positive and negative instances, but only the bag labels are known. The pool of negative instances is usually larger than positive instances, thus making selecting the most informative negative instance critical for performance. Such a selection strategy for negative instances from each bag is an open problem that has not been well studied for weak-label learning. In this paper, we study several sampling strategies that can measure the usefulness of negative instances for weak-label learning and select them accordingly. We test our method on CIFAR-10 and AudioSet datasets and show that it improves the weak-label classification performance and reduces the computational cost compared to random sampling methods. Our work reveals that negative instances are not all equally irrelevant, and selecting them wisely can benefit weak-label learning.",
            "url": "https://arxiv.org/pdf/2309.13227",
            "publicationVenue": "arXiv preprint arXiv:2309.13227",
            "citation_count": 0
        },
        {
            "title": "Training audio captioning models without audio",
            "authors": "Soham Deshmukh, Benjamin Elizalde, Dimitra Emmanouilidou, Bhiksha Raj, Rita Singh, Huaming Wang",
            "year": "2023",
            "abstract": "Automated Audio Captioning (AAC) is the task of generating natural language descriptions given an audio stream. A typical AAC system requires manually curated training data of audio segments and corresponding text caption annotations. The creation of these audio-caption pairs is costly, resulting in general data scarcity for the task. In this work, we address this major limitation and propose an approach to train AAC systems using only text. Our approach leverages the multimodal space of contrastively trained audio-text models, such as CLAP. During training, a decoder generates captions conditioned on the pretrained CLAP text encoder. During inference, the text encoder is replaced with the pretrained CLAP audio encoder. To bridge the modality gap between text and audio embeddings, we propose the use of noise injection or a learnable adapter, during training. We find that the proposed text-only framework performs competitively with state-of-the-art models trained with paired audio, showing that efficient text-to-audio transfer is possible. Finally, we showcase both stylized audio captioning and caption enrichment while training without audio or human-created text captions.",
            "url": "https://arxiv.org/pdf/2309.07372",
            "publicationVenue": "arXiv preprint arXiv:2309.07372",
            "citation_count": 4
        },
        {
            "title": "Rethinking audiovisual segmentation with semantic quantization and decomposition",
            "authors": "Xiang Li, Jinglu Wang, Xiaohao Xu, Xiulian Peng, Rita Singh, Yan Lu, Bhiksha Raj",
            "year": "2023",
            "abstract": "Audiovisual segmentation (AVS) is a challenging task that aims to segment visual objects in videos based on their associated acoustic cues. With multiple sound sources involved, establishing robust correspondences between audio and visual contents poses unique challenges due to its (1) intricate entanglement across sound sources and (2) frequent shift among sound events. Assuming sound events occur independently, the multi-source semantic space (which encompasses all possible semantic categories) can be viewed as the Cartesian product of single-source sub-spaces. This motivates us to decompose the multi-source audio semantics into single-source semantics, allowing for more effective interaction with visual content. Specifically, we propose a semantic decomposition method based on product quantization, where the multi-source semantics can be decomposed and represented by several\u00a0\u2026",
            "url": null,
            "publicationVenue": "arXiv e-prints",
            "citation_count": 2
        },
        {
            "title": "Transferable Adversarial Perturbations between Self-Supervised Speech Recognition Models",
            "authors": "Raphael Olivier, Hadi Abdullah, Bhiksha Raj",
            "year": "2023",
            "abstract": "A targeted adversarial attack produces audio samples that can force an Automatic Speech Recognition (ASR) system to output attacker-chosen text. To exploit ASR models in real-world, black-box settings, an adversary can leverage the \\textit{transferability} property, i.e. that an adversarial sample produced for a proxy ASR can also fool a different remote ASR. Recent work has shown that transferability against large ASR models is extremely difficult. In this work, we show that modern ASR architectures, specifically ones based on Self-Supervised Learning, are uniquely affected by transferability. We successfully demonstrate this phenomenon by evaluating state-of-the-art self-supervised ASR models like Wav2Vec2, HuBERT, Data2Vec and WavLM. We show that with relatively low-level additive noise achieving a 30dB Signal-Noise Ratio, we can achieve target transferability with up to 80\\% accuracy. We then use an ablation study to show that Self-Supervised learning is a major cause of that phenomenon. Our results present a dual interest: they show that modern ASR architectures are uniquely vulnerable to adversarial security threats, and they help understanding the specificities of SSL training paradigms.",
            "url": "https://openreview.net/pdf?id=XHtyRYd3m3",
            "publicationVenue": "The Second Workshop on New Frontiers in Adversarial Machine Learning",
            "citation_count": 0
        },
        {
            "title": "Fixed Inter-Neuron Covariability Induces Adversarial Robustness",
            "authors": "Muhammad Ahmed Shah, Bhiksha Raj",
            "year": "2023",
            "abstract": "The vulnerability to adversarial perturbations is a major flaw of Deep Neural Networks (DNNs) that raises question about their reliability when in real-world scenarios. On the other hand, human perception, which DNNs are supposed to emulate, is highly robust to such perturbations, indicating that there may be certain features of the human perception that make it robust but are not represented in the current class of DNNs. One such feature is that the activity of biological neurons is correlated and the structure of this correlation tends to be rather rigid over long spans of times, even if it hampers performance and learning. We hypothesize that integrating such constraints on the activations of a DNN would improve its adversarial robustness, and, to test this hypothesis, we have developed the Self-Consistent Activation (SCA) layer, which comprises of neurons whose activations are consistent with each other, as they conform to a fixed, but learned, covariability pattern. When evaluated on image and sound recognition tasks, the models with a SCA layer achieved high accuracy, and exhibited significantly greater robustness than multi-layer perceptron models to state-of-the-art Auto-PGD adversarial attacks \\textit{without being trained on adversarially perturbed data",
            "url": "https://arxiv.org/pdf/2308.03956",
            "publicationVenue": "arXiv preprint arXiv:2308.03956",
            "citation_count": 0
        },
        {
            "title": "The hidden dance of phonemes and visage: Unveiling the enigmatic link between phonemes and facial features",
            "authors": "Liao Qu, Xianwei Zou, Xiang Li, Yandong Wen, Rita Singh, Bhiksha Raj",
            "year": "2023",
            "abstract": "This work unveils the enigmatic link between phonemes and facial features. Traditional studies on voice-face correlations typically involve using a long period of voice input, including generating face images from voices and reconstructing 3D face meshes from voices. However, in situations like voice-based crimes, the available voice evidence may be short and limited. Additionally, from a physiological perspective, each segment of speech -- phoneme -- corresponds to different types of airflow and movements in the face. Therefore, it is advantageous to discover the hidden link between phonemes and face attributes. In this paper, we propose an analysis pipeline to help us explore the voice-face relationship in a fine-grained manner, i.e., phonemes v.s. facial anthropometric measurements (AM). We build an estimator for each phoneme-AM pair and evaluate the correlation through hypothesis testing. Our results indicate that AMs are more predictable from vowels compared to consonants, particularly with plosives. Additionally, we observe that if a specific AM exhibits more movement during phoneme pronunciation, it is more predictable. Our findings support those in physiology regarding correlation and lay the groundwork for future research on speech-face multimodal learning.",
            "url": "https://arxiv.org/pdf/2307.13953",
            "publicationVenue": "arXiv preprint arXiv:2307.13953",
            "citation_count": 3
        },
        {
            "title": "BASS: Block-wise Adaptation for Speech Summarization",
            "authors": "Roshan Sharma, Kenneth Zheng, Siddhant Arora, Shinji Watanabe, Rita Singh, Bhiksha Raj",
            "year": "2023",
            "abstract": "End-to-end speech summarization has been shown to improve performance over cascade baselines. However, such models are difficult to train on very large inputs (dozens of minutes or hours) owing to compute restrictions and are hence trained with truncated model inputs. Truncation leads to poorer models, and a solution to this problem rests in block-wise modeling, i.e., processing a portion of the input frames at a time. In this paper, we develop a method that allows one to train summarization models on very long sequences in an incremental manner. Speech summarization is realized as a streaming process, where hypothesis summaries are updated every block based on new acoustic information. We devise and test strategies to pass semantic context across the blocks. Experiments on the How2 dataset demonstrate that the proposed block-wise training method improves by 3 points absolute on ROUGE-L over a truncated input baseline.",
            "url": "https://arxiv.org/pdf/2307.08217",
            "publicationVenue": "arXiv preprint arXiv:2307.08217",
            "citation_count": 0
        },
        {
            "title": "Panoramic video salient object detection with ambisonic audio guidance",
            "authors": "Xiang Li, Haoyuan Cao, Shijie Zhao, Junlin Li, Li Zhang, Bhiksha Raj",
            "year": "2023",
            "abstract": "Video salient object detection (VSOD), as a fundamental computer vision problem, has been extensively discussed in the last decade. However, all existing works focus on addressing the VSOD problem in 2D scenarios. With the rapid development of VR devices, panoramic videos have been a promising alternative to 2D videos to provide immersive feelings of the real world. In this paper, we aim to tackle the video salient object detection problem for panoramic videos, with their corresponding ambisonic audios. A multimodal fusion module equipped with two pseudo-siamese audio-visual context fusion (ACF) blocks is proposed to effectively conduct audio-visual interaction. The ACF block equipped with spherical positional encoding enables the fusion in the 3D context to capture the spatial correspondence between pixels and sound sources from the equirectangular frames and ambisonic audios. Experimental results verify the effectiveness of our proposed components and demonstrate that our method achieves state-of-the-art performance on the ASOD60K dataset.",
            "url": "https://ojs.aaai.org/index.php/AAAI/article/download/25227/24999",
            "publicationVenue": "Proceedings of the AAAI Conference on Artificial Intelligence",
            "citation_count": 6
        },
        {
            "title": "VLTinT: visual-linguistic transformer-in-transformer for coherent video paragraph captioning",
            "authors": "Kashu Yamazaki, Khoa Vo, Quang Sang Truong, Bhiksha Raj, Ngan Le",
            "year": "2023",
            "abstract": "Video Paragraph Captioning aims to generate a multi-sentence description of an untrimmed video with multiple temporal event locations in a coherent storytelling. Following the human perception process, where the scene is effectively understood by decomposing it into visual (eg human, animal) and non-visual components (eg action, relations) under the mutual influence of vision and language, we first propose a visual-linguistic (VL) feature. In the proposed VL feature, the scene is modeled by three modalities including (i) a global visual environment;(ii) local visual main agents;(iii) linguistic scene elements. We then introduce an autoregressive Transformer-in-Transformer (TinT) to simultaneously capture the semantic coherence of intra-and inter-event contents within a video. Finally, we present a new VL contrastive loss function to guarantee the learnt embedding features are consistent with the captions semantics. Comprehensive experiments and extensive ablation studies on the ActivityNet Captions and YouCookII datasets show that the proposed Visual-Linguistic Transformer-in-Transform (VLTinT) outperforms previous state-of-the-art methods in terms of accuracy and diversity. The source code is made publicly available at: https://github. com/UARK-AICV/VLTinT.",
            "url": "https://ojs.aaai.org/index.php/AAAI/article/download/25412/25184",
            "publicationVenue": "Proceedings of the AAAI Conference on Artificial Intelligence",
            "citation_count": 8
        },
        {
            "title": "UTOPIA: Unconstrained Tracking Objects without Preliminary Examination via Cross-Domain Adaptation",
            "authors": "Pha Nguyen, Kha Gia Quach, John Gauch, Samee U Khan, Bhiksha Raj, Khoa Luu",
            "year": "2023",
            "abstract": "Multiple Object Tracking (MOT) aims to find bounding boxes and identities of targeted objects in consecutive video frames. While fully-supervised MOT methods have achieved high accuracy on existing datasets, they cannot generalize well on a newly obtained dataset or a new unseen domain. In this work, we first address the MOT problem from the cross-domain point of view, imitating the process of new data acquisition in practice. Then, a new cross-domain MOT adaptation from existing datasets is proposed without any pre-defined human knowledge in understanding and modeling objects. It can also learn and update itself from the target data feedback. The intensive experiments are designed on four challenging settings, including MOTSynth to MOT17, MOT17 to MOT20, MOT17 to VisDrone, and MOT17 to DanceTrack. We then prove the adaptability of the proposed self-supervised learning strategy. The experiments also show superior performance on tracking metrics MOTA and IDF1, compared to fully supervised, unsupervised, and self-supervised state-of-the-art methods.",
            "url": "https://arxiv.org/pdf/2306.09613",
            "publicationVenue": "arXiv preprint arXiv:2306.09613",
            "citation_count": 1
        },
        {
            "title": "How Many Perturbations Break This Model? Evaluating Robustness Beyond Adversarial Accuracy",
            "authors": "Raphael Olivier, Bhiksha Raj",
            "year": "2023",
            "abstract": "Robustness to adversarial attacks is typically evaluated with adversarial accuracy. While essential, this metric does not capture all aspects of robustness and in particular leaves out the question of how many perturbations can be found for each point. In this work, we introduce an alternative approach, adversarial sparsity, which quantifies how difficult it is to find a successful perturbation given both an input point and a constraint on the direction of the perturbation. We show that sparsity provides valuable insight into neural networks in multiple ways: for instance, it illustrates important differences between current state-of-the-art robust models them that accuracy analysis does not, and suggests approaches for improving their robustness. When applying broken defenses effective against weak attacks but not strong ones, sparsity can discriminate between the totally ineffective and the partially effective defenses. Finally, with sparsity we can measure increases in robustness that do not affect accuracy: we show for example that data augmentation can by itself increase adversarial robustness, without using adversarial training.",
            "url": "https://openreview.net/pdf?id=3m9c6uYKK7",
            "publicationVenue": null,
            "citation_count": 1
        },
        {
            "title": "An Approach to Ontological Learning from Weak Labels",
            "authors": "Ankit Shah, Larry Tang, Po Hao Chou, Yi Yu Zheng, Ziqian Ge, Bhiksha Raj",
            "year": "2023",
            "abstract": "Ontologies encompass a formal representation of knowledge through the definition of concepts or properties of a domain, and the relationships between those concepts. In this work, we seek to investigate whether using this ontological information will improve learning from weakly labeled data, which are easier to collect since it requires only the presence or absence of an event to be known. We use the AudioSet ontology and dataset, which contains audio clips weakly labeled with the ontology concepts and the ontology providing the \"Is A\" relations between the concepts. We first re-implemented the model proposed by [1] with modifications to fit the multi-label scenario and then expand on that idea by using a Graph Convolutional Network (GCN) to model the ontology information to learn the concepts. We find that the baseline Twin Neural Network (TNN) does not perform better by incorporating ontology\u00a0\u2026",
            "url": null,
            "publicationVenue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
            "citation_count": 0
        },
        {
            "title": "Privacy-Preserving Automatic Speaker Diarization",
            "authors": "Francisco Teixeira, Alberto Abad, Bhiksha Raj, Isabel Trancoso",
            "year": "2023",
            "abstract": "Automatic Speaker Diarization (ASD) is an enabling technology with numerous applications, which deals with recordings of multiple speakers, raising special concerns in terms of privacy. In fact, in remote settings, where recordings are shared with a server, clients relinquish not only the privacy of their conversation, but also of all the information that can be inferred from their voices. However, to the best of our knowledge, the development of privacy-preserving ASD systems has been overlooked thus far. In this work, we tackle this problem using a combination of two cryptographic techniques, Secure Multiparty Computation (SMC) and Secure Modular Hashing, and apply them to the two main steps of a cascaded ASD system: speaker embedding extraction and agglomerative hierarchical clustering. Our system is able to achieve a reasonable trade-off between performance and efficiency, presenting real-time factors\u00a0\u2026",
            "url": "https://arxiv.org/pdf/2210.14995",
            "publicationVenue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
            "citation_count": 1
        },
        {
            "title": "The Interactive Machine Learning Paradigm",
            "authors": "Mark Lindsey, Richard M Stern, Bhiksha Raj, Aswin Sankaranarayanan, Rita Singh, Francis Kubala",
            "year": "2023",
            "abstract": "Machine Learning (ML) is a powerful tool that can assist humans in doing a wide variety of tasks. However, typical ML paradigms tend to remove the human element from the equation as frequently as possible in the name of automation. This approach may be useful for creating powerful solutions to a selection of pre-defined tasks, but it has lead to a dearth of methods for quickly training a machine to assist a human domain expert in a task previously undefined for ML. This problem is compounded in cases where data for the intended task are scarce or proprietary, since typical ML paradigms rely on large amounts of training data. To address this and other related issues with typical ML paradigms, I propose an Interactive Machine Learning (IML) paradigm in which a machine classifier is trained to perform a new task alongside a human domain expert as part of his or her operational workflow. This proposal includes a description of the IML paradigm and how it differs from other existing paradigms, as well as preliminary results showing the efficacy of the IML paradigm compared to typical supervised training. It also includes three pieces of proposed work that build on the existing IML pipeline to jointly optimize the performance of the classifier being trained and the amount of feedback required of the domain expert. These include a mathematical formulation of the IML objective function based on the joint optimization problem, a method for handling weighted error functions and imbalanced class distributions, and methods for exploiting the temporal nature of the IML training process.",
            "url": "http://cvis.cs.cmu.edu/cvis/docs/mark_proposal_final.pdf",
            "publicationVenue": null,
            "citation_count": 0
        },
        {
            "title": "TAPLoss: A Temporal Acoustic Parameter Loss for Speech Enhancement",
            "authors": "Yunyang Zeng, Joseph Konan, David Bick, Muqiao Wang, Anurag Kumar, Shinji Watanabe, Bhiksha Raj",
            "year": "2023",
            "abstract": null,
            "url": null,
            "publicationVenue": "ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
            "citation_count": 5
        },
        {
            "title": "PaintSeg: Training-free Segmentation via Painting",
            "authors": "Xiang Li, Chung-Ching Lin, Yinpeng Chen, Zicheng Liu, Jinglu Wang, Bhiksha Raj",
            "year": "2023",
            "abstract": "The paper introduces PaintSeg, a new unsupervised method for segmenting objects without any training. We propose an adversarial masked contrastive painting (AMCP) process, which creates a contrast between the original image and a painted image in which a masked area is painted using off-the-shelf generative models. During the painting process, inpainting and outpainting are alternated, with the former masking the foreground and filling in the background, and the latter masking the background while recovering the missing part of the foreground object. Inpainting and outpainting, also referred to as I-step and O-step, allow our method to gradually advance the target segmentation mask toward the ground truth without supervision or training. PaintSeg can be configured to work with a variety of prompts, e.g. coarse masks, boxes, scribbles, and points. Our experimental results demonstrate that PaintSeg outperforms existing approaches in coarse mask-prompt, box-prompt, and point-prompt segmentation tasks, providing a training-free solution suitable for unsupervised segmentation.",
            "url": "https://arxiv.org/pdf/2305.19406",
            "publicationVenue": "arXiv preprint arXiv:2305.19406",
            "citation_count": 1
        },
        {
            "title": "Imprecise label learning: A unified framework for learning with various imprecise label configurations",
            "authors": "Hao Chen, Ankit Shah, Jindong Wang, Ran Tao, Yidong Wang, Xing Xie, Masashi Sugiyama, Rita Singh, Bhiksha Raj",
            "year": "2023",
            "abstract": "In this paper, we introduce the imprecise label learning (ILL) framework, a unified approach to handle various imprecise label configurations, which are commonplace challenges in machine learning tasks. ILL leverages an expectation-maximization (EM) algorithm for the maximum likelihood estimation (MLE) of the imprecise label information, treating the precise labels as latent variables. Compared to previous versatile methods attempting to infer correct labels from the imprecise label information, our ILL framework considers all possible labeling imposed by the imprecise label information, allowing a unified solution to deal with any imprecise labels. With comprehensive experimental results, we demonstrate that ILL can seamlessly adapt to various situations, including partial label learning, semi-supervised learning, noisy label learning, and a mixture of these settings. Notably, our simple method surpasses the existing techniques for handling imprecise labels, marking the first unified framework with robust and effective performance across various imprecise labels. We believe that our approach has the potential to significantly enhance the performance of machine learning models on tasks where obtaining precise labels is expensive and complicated. We hope our work will inspire further research on this topic with an open-source codebase release.",
            "url": "https://arxiv.org/pdf/2305.12715",
            "publicationVenue": "arXiv preprint arXiv:2305.12715",
            "citation_count": 2
        },
        {
            "title": "Improving Perceptual Quality, Intelligibility, and Acoustics on VoIP Platforms",
            "authors": "Joseph Konan, Ojas Bhargave, Shikhar Agnihotri, Hojeong Lee, Ankit Shah, Shuo Han, Yunyang Zeng, Amanda Shu, Haohui Liu, Xuankai Chang, Hamza Khalid, Minseon Gwak, Kawon Lee, Minjeong Kim, Bhiksha Raj",
            "year": "2023",
            "abstract": "In this paper, we present a method for fine-tuning models trained on the Deep Noise Suppression (DNS) 2020 Challenge to improve their performance on Voice over Internet Protocol (VoIP) applications. Our approach involves adapting the DNS 2020 models to the specific acoustic characteristics of VoIP communications, which includes distortion and artifacts caused by compression, transmission, and platform-specific processing. To this end, we propose a multi-task learning framework for VoIP-DNS that jointly optimizes noise suppression and VoIP-specific acoustics for speech enhancement. We evaluate our approach on a diverse VoIP scenarios and show that it outperforms both industry performance and state-of-the-art methods for speech enhancement on VoIP applications. Our results demonstrate the potential of models trained on DNS-2020 to be improved and tailored to different VoIP platforms using VoIP-DNS, whose findings have important applications in areas such as speech recognition, voice assistants, and telecommunication.",
            "url": "https://arxiv.org/pdf/2303.09048",
            "publicationVenue": "arXiv preprint arXiv:2303.09048",
            "citation_count": 1
        },
        {
            "title": "Approach to Learning Generalized Audio Representation Through Batch Embedding Covariance Regularization and Constant-Q Transforms",
            "authors": "Ankit Shah, Shuyi Chen, Kejun Zhou, Yue Chen, Bhiksha Raj",
            "year": "2023",
            "abstract": "General-purpose embedding is highly desirable for few-shot even zero-shot learning in many application scenarios, including audio tasks. In order to understand representations better, we conducted a thorough error analysis and visualization of HEAR 2021 submission results. Inspired by the analysis, this work experiments with different front-end audio preprocessing methods, including Constant-Q Transform (CQT) and Short-time Fourier transform (STFT), and proposes a Batch Embedding Covariance Regularization (BECR) term to uncover a more holistic simulation of the frequency information received by the human auditory system. We tested the models on the suite of HEAR 2021 tasks, which encompass a broad category of tasks. Preliminary results show (1) the proposed BECR can incur a more dispersed embedding on the test set, (2) BECR improves the PaSST model without extra computation complexity, and (3) STFT preprocessing outperforms CQT in all tasks we tested. Github:https://github.com/ankitshah009/general_audio_embedding_hear_2021",
            "url": "https://arxiv.org/pdf/2303.03591",
            "publicationVenue": "arXiv preprint arXiv:2303.03591",
            "citation_count": 0
        },
        {
            "title": "Improving sound event detection with ontologies",
            "authors": "Bhiksha Raj",
            "year": "2023",
            "abstract": "Sound event recognition is the task of identifying and categorizing sounds in audio data. Automated algorithms for sound event recognition depend on having explicit models for individual sound event types to be recognized, which are trained on data tagged explicitly for those classes. The approach is data hungryand is fundamentally limited by the number of classes for which such data may be obtained. It also ignores the relationship between sounds being modeled. In this work, we attempt to address these deficiencies through the use of a human-generated sound ontology which represents sibling and parent\u2013child relations between sound classes. We incorporate the relationships in the ontology through the design of an appropriate \u201closs\u201d function (the objective function optimized to train sound-classifier models) that incorporates the relationships in the ontology, and through appropriate model update rules\u00a0\u2026",
            "url": null,
            "publicationVenue": "The Journal of the Acoustical Society of America",
            "citation_count": 0
        },
        {
            "title": "Synergy between human and machine approaches to sound/scene recognition and processing: An overview of ICASSP special session",
            "authors": "Laurie M Heller, Benjamin Elizalde, Bhiksha Raj, Soham Deshmukh",
            "year": "2023",
            "abstract": "Machine Listening, as usually formalized, attempts to perform a task that is, from our perspective, fundamentally human-performable, and performed by humans. Current automated models of Machine Listening vary from purely data-driven approaches to approaches imitating human systems. In recent years, the most promising approaches have been hybrid in that they have used data-driven approaches informed by models of the perceptual, cognitive, and semantic processes of the human system. Not only does the guidance provided by models of human perception and domain knowledge enable better, and more generalizable Machine Listening, in the converse, the lessons learned from these models may be used to verify or improve our models of human perception themselves. This paper summarizes advances in the development of such hybrid approaches, ranging from Machine Listening models that are informed by models of peripheral (human) auditory processes, to those that employ or derive semantic information encoded in relations between sounds. The research described herein was presented in a special session on \"Synergy between human and machine approaches to sound/scene recognition and processing\" at the 2023 ICASSP meeting.",
            "url": "https://arxiv.org/pdf/2302.09719",
            "publicationVenue": "arXiv preprint arXiv:2302.09719",
            "citation_count": 10
        },
        {
            "title": "Softmatch: Addressing the quantity-quality trade-off in semi-supervised learning",
            "authors": "Hao Chen, Ran Tao, Yue Fan, Yidong Wang, Jindong Wang, Bernt Schiele, Xing Xie, Bhiksha Raj, Marios Savvides",
            "year": "2023",
            "abstract": "The critical challenge of Semi-Supervised Learning (SSL) is how to effectively leverage the limited labeled data and massive unlabeled data to improve the model's generalization performance. In this paper, we first revisit the popular pseudo-labeling methods via a unified sample weighting formulation and demonstrate the inherent quantity-quality trade-off problem of pseudo-labeling with thresholding, which may prohibit learning. To this end, we propose SoftMatch to overcome the trade-off by maintaining both high quantity and high quality of pseudo-labels during training, effectively exploiting the unlabeled data. We derive a truncated Gaussian function to weight samples based on their confidence, which can be viewed as a soft version of the confidence threshold. We further enhance the utilization of weakly-learned classes by proposing a uniform alignment approach. In experiments, SoftMatch shows substantial improvements across a wide variety of benchmarks, including image, text, and imbalanced classification.",
            "url": "https://arxiv.org/pdf/2301.10921",
            "publicationVenue": "arXiv preprint arXiv:2301.10921",
            "citation_count": 56
        },
        {
            "title": "Understanding Political Polarisation using Language Models: A dataset and method",
            "authors": "Samiran Gode, Supreeth Bare, Bhiksha Raj, Hyungon Yoo",
            "year": "2023",
            "abstract": "Our paper aims to analyze political polarization in US political system using Language Models, and thereby help candidates make an informed decision. The availability of this information will help voters understand their candidates views on the economy, healthcare, education and other social issues. Our main contributions are a dataset extracted from Wikipedia that spans the past 120 years and a Language model based method that helps analyze how polarized a candidate is. Our data is divided into 2 parts, background information and political information about a candidate, since our hypothesis is that the political views of a candidate should be based on reason and be independent of factors such as birthplace, alma mater, etc. We further split this data into 4 phases chronologically, to help understand if and how the polarization amongst candidates changes. This data has been cleaned to remove biases. To understand the polarization we begin by showing results from some classical language models in Word2Vec and Doc2Vec. And then use more powerful techniques like the Longformer, a transformer based encoder, to assimilate more information and find the nearest neighbors of each candidate based on their political view and their background.",
            "url": "https://arxiv.org/pdf/2301.00891",
            "publicationVenue": "arXiv preprint arXiv:2301.00891",
            "citation_count": 0
        },
        {
            "title": "Robust referring video object segmentation with cyclic structural consensus",
            "authors": "Xiang Li, Jinglu Wang, Xiaohao Xu, Xiao Li, Bhiksha Raj, Yan Lu",
            "year": "2023",
            "abstract": "Referring Video Object Segmentation (R-VOS) is a challenging task that aims to segment an object in a video based on a linguistic expression. Most existing R-VOS methods have a critical assumption: the object referred to must appear in the video. This assumption, which we refer to as\" semantic consensus\", is often violated in real-world scenarios, where the expression may be queried against false videos. In this work, we highlight the need for a robust R-VOS model that can handle semantic mismatches. Accordingly, we propose an extended task called Robust R-VOS (RRVOS), which accepts unpaired video-text inputs. We tackle this problem by jointly modeling the primary R-VOS problem and its dual (text reconstruction). A structural text-to-text cycle constraint is introduced to discriminate semantic consensus between video-text pairs and impose it in positive pairs, thereby achieving multi-modal alignment from both positive and negative pairs. Our structural constraint effectively addresses the challenge posed by linguistic diversity, overcoming the limitations of previous methods that relied on the point-wise constraint. A new evaluation dataset, RRYTVOS is constructed to measure the model robustness. Our model achieves state-of-the-art performance on R-VOS benchmarks, Ref-DAVIS17 and Ref-Youtube-VOS, and also our RRYTVOS dataset.",
            "url": "http://openaccess.thecvf.com/content/ICCV2023/papers/Li_Robust_Referring_Video_Object_Segmentation_with_Cyclic_Structural_Consensus_ICCV_2023_paper.pdf",
            "publicationVenue": "Proceedings of the IEEE/CVF International Conference on Computer Vision",
            "citation_count": 6
        },
        {
            "title": "Pairwise Similarity Learning is SimPLE",
            "authors": "Yandong Wen, Weiyang Liu, Yao Feng, Bhiksha Raj, Rita Singh, Adrian Weller, Michael J Black, Bernhard Sch\u00f6lkopf",
            "year": "2023",
            "abstract": "In this paper, we focus on a general yet important learning problem, pairwise similarity learning (PSL). PSL subsumes a wide range of important applications, such as open-set face recognition, speaker verification, image retrieval and person re-identification. The goal of PSL is to learn a pairwise similarity function assigning a higher similarity score to positive pairs (ie, a pair of samples with the same label) than to negative pairs (ie, a pair of samples with different label). We start by identifying a key desideratum for PSL, and then discuss how existing methods can achieve this desideratum. We then propose a surprisingly simple proxy-free method, called SimPLE, which requires neither feature/proxy normalization nor angular margin and yet is able to generalize well in open-set recognition. We apply the proposed method to three challenging PSL tasks: open-set face recognition, image retrieval and speaker verification. Comprehensive experimental results on large-scale benchmarks show that our method performs significantly better than current state-of-the-art methods.",
            "url": "https://openaccess.thecvf.com/content/ICCV2023/papers/Wen_Pairwise_Similarity_Learning_is_SimPLE_ICCV_2023_paper.pdf",
            "publicationVenue": "Proceedings of the IEEE/CVF International Conference on Computer Vision",
            "citation_count": 1
        },
        {
            "title": "Fredom: Fairness domain adaptation approach to semantic scene understanding",
            "authors": "Thanh-Dat Truong, Ngan Le, Bhiksha Raj, Jackson Cothren, Khoa Luu",
            "year": "2023",
            "abstract": "Although Domain Adaptation in Semantic Scene Segmentation has shown impressive improvement in recent years, the fairness concerns in the domain adaptation have yet to be well defined and addressed. In addition, fairness is one of the most critical aspects when deploying the segmentation models into human-related real-world applications, eg, autonomous driving, as any unfair predictions could influence human safety. In this paper, we propose a novel Fairness Domain Adaptation (FREDOM) approach to semantic scene segmentation. In particular, from the proposed formulated fairness objective, a new adaptation framework will be introduced based on the fair treatment of class distributions. Moreover, to generally model the context of structural dependency, a new conditional structural constraint is introduced to impose the consistency of predicted segmentation. Thanks to the proposed Conditional Structure Network, the self-attention mechanism has sufficiently modeled the structural information of segmentation. Through the ablation studies, the proposed method has shown the performance improvement of the segmentation models and promoted fairness in the model predictions. The experimental results on the two standard benchmarks, ie, SYNTHIA-> Cityscapes and GTA5-> Cityscapes, have shown that our method achieved State-of-the-Art (SOTA) performance.",
            "url": "http://openaccess.thecvf.com/content/CVPR2023/papers/Truong_FREDOM_Fairness_Domain_Adaptation_Approach_to_Semantic_Scene_Understanding_CVPR_2023_paper.pdf",
            "publicationVenue": "Proceedings of the IEEE/CVF International Conference on Computer Vision",
            "citation_count": 12
        },
        {
            "title": "Aoe-net: Entities interactions modeling with adaptive attention mechanism for temporal action proposals generation",
            "authors": "Khoa Vo, Sang Truong, Kashu Yamazaki, Bhiksha Raj, Minh-Triet Tran, Ngan Le",
            "year": "2023",
            "abstract": "Temporal action proposal generation (TAPG) is a challenging task, which requires localizing action intervals in an untrimmed video. Intuitively, we as humans, perceive an action through the interactions between actors, relevant objects, and the surrounding environment. Despite the significant progress of TAPG, a vast majority of existing methods ignore the aforementioned principle of the human perceiving process by applying a backbone network into a given video as a black-box. In this paper, we propose to model these interactions with a multi-modal representation network, namely,Actors-Objects-Environment Interaction Network (AOE-Net). Our AOE-Net consists of two modules, i.e., perception-based multi-modal representation (PMR) and boundary-matching module (BMM). Additionally, we introduceadaptive attention mechanism (AAM)in PMR to focus only on main actors (or relevant objects) and model the\u00a0\u2026",
            "url": "https://arxiv.org/pdf/2210.02578",
            "publicationVenue": "International Journal of Computer Vision",
            "citation_count": 11
        }
    ],
    "Carolyn Penstein Rose": [
        {
            "title": "Reflecting on what counts as collaboration: Reaching forward without losing what is behind",
            "authors": "Sanna J\u00e4rvel\u00e4, Carolyn P Rose",
            "year": "2023",
            "abstract": "In this final issue of our co-editor-in-chiefship, we take the opportunity to reflect, both on the past four years and on the road ahead. Thinking back to the time when we were just starting to imagine what our experience would be taking on this task, we had no idea what was in store for us, our community, our nations, and the world. It has been four years filled with challenges and big questions. With the backdrop of COVID accompanied by waves of political and economic turmoil, we find ourselves at a crossroads in terms of our own identity as a research community. At the heart of all of this, we arrive at a focal question: What is the essence of collaboration? As we reach out in openness to new ideas, how do we maintain our identity? What defines the frontiers of what we embrace as a community? In short, how do we emerge from this period of uncertainty and instability to move forward in strength as a cutting edge\u00a0\u2026",
            "url": "https://link.springer.com/article/10.1007/s11412-023-09415-y",
            "publicationVenue": "International Journal of Computer-Supported Collaborative Learning",
            "citation_count": 0
        },
        {
            "title": "Assessing AI capabilities with education tests",
            "authors": "Mila Staneva, Abel Baret, \u00c1ngel Aso-Mollar, Joseph Blass, Salvador Carri\u00f3n Ponz, Vincent Conitzer, Ulises Cortes, Pradeep Dasigi, Angel de Paula, Carlos Galindo, Janice Gobert, Jordi Gonz\u00e0lez, Fredrik Heintz, Jim Hendler, Daniel Hendrycks, Lawrence Hunter, Juan Izquierdo-Domenech, Maria Juarez, Aina Juraco Frias, Aviv Keren, Rik Koncel-Kedziorski, David Leake, Bao Sheng Loe, Fernando Martinez-Plumed, Aqueasha Martin-Hammond, Cynthia Matuszek, Antoni Mestre Gasc\u00f3n, Jose Andres Moreno, Constantine Nakos, Taylor Olson, Carolyn Rose, Areg Mikael Sarvazyan, Brian Scassellati, Wout Schellaert, Claes Stranneg\u00e5rd, Neset Tan, Tadahiro Taniguchi, Karina Vold, Michael Wooldridge",
            "year": "2023",
            "abstract": "This chapter introduces three exploratory studies that assessed the capabilities of artificial intelligence (AI) through standardised education tests designed for humans. The first two studies, conducted in 2016 and 2021/22, asked experts to evaluate AI\u2019s performance on the literacy and numeracy tests of the OECD\u2019s Survey of Adult Skills (PIAAC). The third study collected expert judgements of whether AI can solve science questions from the OECD's Programme for International Student Assessment (PISA). The studies aimed to refine the assessment framework for eliciting expert knowledge on AI using established educational assessments. They explored different test formats, response methodologies and rating instructions, along with two distinct assessment approaches. A \u201cbehavioural approach\u201d used in the PIAAC studies emphasised smaller expert groups engaging in discussions, and a \"mathematical approach\"\u00a0\u2026",
            "url": null,
            "publicationVenue": "OECD",
            "citation_count": 0
        },
        {
            "title": "Data Augmentation for Code Translation with Comparable Corpora and Multiple References",
            "authors": "Yiqing Xie, Atharva Naik, Daniel Fried, Carolyn Rose",
            "year": "2023",
            "abstract": "One major challenge of translating code between programming languages is that parallel training data is often limited. To overcome this challenge, we present two data augmentation techniques, one that builds comparable corpora (i.e., code pairs with similar functionality), and another that augments existing parallel data with multiple reference translations. Specifically, we build and analyze multiple types of comparable corpora, including programs generated from natural language documentation using a code generation model. Furthermore, to reduce overfitting to a single reference translation, we automatically generate additional translation references for available parallel data and filter the translations by unit tests, which increases variation in target translations. Experiments show that our data augmentation techniques significantly improve CodeT5 for translation between Java, Python, and C++ by an average of 7.5% Computational Accuracy (CA@1), which verifies the correctness of translations by execution. The code is available at https://github.com/Veronicium/CMTrans.",
            "url": "https://arxiv.org/pdf/2311.00317",
            "publicationVenue": "arXiv preprint arXiv:2311.00317",
            "citation_count": 1
        },
        {
            "title": "SPEERLoom: An Open-Source Loom Kit for Interdisciplinary Engagement in Math, Engineering, and Textiles",
            "authors": "Samantha Speer, Ana P Garcia-Alonzo, Joey Huang, Nickolina Yankova, Carolyn Rose, Kylie A Peppler, James McCann, Melisa Orta Martinez",
            "year": "2023",
            "abstract": "Weaving is a fabrication process that is grounded in mathematics and engineering: from the binary, matrix-like nature of the pattern drafts weavers have used for centuries, to the punch card programming of the first Jacquard looms. This intersection of disciplines provides an opportunity to ground abstract mathematical concepts in a concrete and embodied art, viewing this textile art through the lens of engineering. Currently, available looms are not optimized to take advantage of this opportunity to increase mathematics learning by providing hands-on interdisciplinary learning in collegiate classrooms. In this work, we present SPEERLoom: an open-source, robotic Jacquard loom kit designed to be a tool for interweaving cloth fabrication, mathematics, and engineering to support interdisciplinary learning in the classroom. We discuss the design requirements and subsequent design of SPEERLoom. We also present\u00a0\u2026",
            "url": "https://dl.acm.org/doi/pdf/10.1145/3586183.3606724",
            "publicationVenue": "Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology",
            "citation_count": 0
        },
        {
            "title": "Enhancing student learning and achievement through orchestration of group processes and group composition",
            "authors": "Carolyn P Rose, Sanna J\u00e4rvel\u00e4",
            "year": "2023",
            "abstract": "Fall is a time of new beginnings, when we consider the academic calendar, and yet it is also the time of year when we become aware of closings on the horizon as the year begins to draw to a close. Because of that, the Fall is an excellent time for reflection, and in this September issue of the International Journal of Computer-Supported Collaborative Learning, we reflect on the importance of productive collaborative processes, with an emphasis on feedback processes, and the scaffolding that upholds and promotes productive learning processes, whether it is explicit or implicit. In particular, this issue features four full articles, two of which focus directly on intervention studies with a focus on promoting productive collaborative processes through explicit scaffolding, and two of which focus on differential effects of feedback based on features of the feedback, in one case informing design of implicit scaffolding of processes\u00a0\u2026",
            "url": "https://link.springer.com/article/10.1007/s11412-023-09408-x",
            "publicationVenue": "International Journal of Computer-Supported Collaborative Learning",
            "citation_count": 0
        },
        {
            "title": "Linguistic representations for fewer-shot relation extraction across domains",
            "authors": "Sireesh Gururaja, Ritam Dutt, Tinglong Liao, Carolyn Rose",
            "year": "2023",
            "abstract": "Recent work has demonstrated the positive impact of incorporating linguistic representations as additional context and scaffolding on the in-domain performance of several NLP tasks. We extend this work by exploring the impact of linguistic representations on cross-domain performance in a few-shot transfer setting. An important question is whether linguistic representations enhance generalizability by providing features that function as cross-domain pivots. We focus on the task of relation extraction on three datasets of procedural text in two domains, cooking and materials science. Our approach augments a popular transformer-based architecture by alternately incorporating syntactic and semantic graphs constructed by freely available off-the-shelf tools. We examine their utility for enhancing generalization, and investigate whether earlier findings, e.g. that semantic representations can be more helpful than syntactic ones, extend to relation extraction in multiple domains. We find that while the inclusion of these graphs results in significantly higher performance in few-shot transfer, both types of graph exhibit roughly equivalent utility.",
            "url": "https://arxiv.org/pdf/2307.03823",
            "publicationVenue": "arXiv preprint arXiv:2307.03823",
            "citation_count": 2
        },
        {
            "title": "Using counterfactual contrast to improve compositional generalization for multi-step quantitative reasoning",
            "authors": "Armineh Nourbakhsh, Sameena Shah, Carolyn Rose",
            "year": "2023",
            "abstract": "In quantitative question answering, compositional generalization is one of the main challenges of state of the art models, especially when longer sequences of reasoning steps are required. In this paper we propose CounterComp, a method that uses counterfactual scenarios to generate samples with compositional contrast. Instead of a data augmentation approach, CounterComp is based on metric learning, which allows for direct sampling from the training set and circumvents the need for additional human labels. Our proposed auxiliary metric learning loss improves the performance of three state of the art models on four recently released datasets. We also show how the approach can improve OOD performance on unseen domains, as well as unseen compositions. Lastly, we demonstrate how the method can lead to better compositional attention patterns during training.",
            "url": "https://aclanthology.org/2023.acl-long.834.pdf",
            "publicationVenue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
            "citation_count": 0
        },
        {
            "title": "Exploring Artificial Intelligence in English Language Arts with StoryQ",
            "authors": "Jie Chao, Rebecca Ellis, Shiyan Jiang, Carolyn Rose, William Finzer, Cansu Tatar, James Fiacco, Kenia Wiedemann",
            "year": "2023",
            "abstract": "Exploring Artificial Intelligence (AI) in English Language Arts (ELA) with StoryQ is a 10-hour curriculum module designed for high school ELA classes. The module introduces students to fundamental AI concepts and essential machine learning workflow using StoryQ, a web-based GUI environment for Grades 6-12 learners. In this module, students work with unstructured text data and learn to train, test, and improve text classification models such as intent recognition, clickbait filter, and sentiment analysis. As they interact with machine-learning language models deeply, students also gain a nuanced understanding of language and how to wield it, not just as a data structure, but as a tool in our human-human encounters as well. The current version contains eight lessons, all delivered through a full-featured online learning and teaching platform. Computers and Internet access are required to implement the module. The module was piloted in an ELA class in the Spring of 2022, and the student learning outcomes were positive. The module is currently undergoing revision and will be further tested and improved in Fall 2022.",
            "url": "https://ojs.aaai.org/index.php/AAAI/article/view/26899/26671",
            "publicationVenue": "Proceedings of the AAAI Conference on Artificial Intelligence",
            "citation_count": 3
        },
        {
            "title": "Making Sense of Machine Learning: Integrating Youth's Conceptual, Creative, and Critical Understandings of AI",
            "authors": "Luis Morales-Navarro, Yasmin B Kafai, Francisco Castro, William Payne, Kayla DesPortes, Daniella DiPaola, Randi Williams, Safinah Ali, Cynthia Breazeal, Clifford Lee, Elisabeth Soep, Duri Long, Brian Magerko, Jaemarie Solyst, Amy Ogan, Cansu Tatar, Shiyan Jiang, Jie Chao, Carolyn P Rose, Sepehr Vakil",
            "year": "2023",
            "abstract": "Understanding how youth make sense of machine learning and how learning about machine learning can be supported in and out of school is more relevant than ever before as young people interact with machine learning powered applications everyday; while connecting with friends, listening to music, playing games, or attending school. In this symposium, we present different perspectives on understanding how learners make sense of machine learning in their everyday lives, how sensemaking of machine learning can be supported in and out of school through the construction of applications, and how youth critically evaluate machine learning powered systems. We discuss how sensemaking of machine learning applications involves the development and integration of conceptual, creative, and critical understandings that are increasingly important to prepare youth to participate in the world.",
            "url": "https://arxiv.org/pdf/2305.02840",
            "publicationVenue": "arXiv preprint arXiv:2305.02840",
            "citation_count": 1
        },
        {
            "title": "Social analytics to support engagement with learning communities",
            "authors": "Carolyn Rose, Meredith Riggs, Nicole Barbaro",
            "year": "2023",
            "abstract": "Over the past 30 years, culminating with the recent pandemic, human life has increasingly moved online and the world of learning, including AI-enhanced learning, has been no exception. Because of this movement, we would be remiss not to include a chapter in this volume related to AI in support of learning at a large scale online.",
            "url": null,
            "publicationVenue": "Handbook of Artificial Intelligence in Education",
            "citation_count": 0
        },
        {
            "title": "High school students\u2019 data modeling practices and processes: From modeling unstructured data to evaluating automated decisions",
            "authors": "Shiyan Jiang, Hengtao Tang, Cansu Tatar, Carolyn P Rose, Jie Chao",
            "year": "2023",
            "abstract": "It\u2019s critical to foster artificial intelligence (AI) literacy for high school students, the first generation to grow up surrounded by AI, to understand working mechanism of data-driven AI technologies and critically evaluate automated decisions from predictive models. While efforts have been made to engage youth in understanding AI through developing machine learning models, few provided in-depth insights into the nuanced learning processes. In this study, we examined high school students\u2019 data modeling practices and processes. Twenty-eight students developed machine learning models with text data for classifying negative and positive reviews of ice cream stores. We identified nine data modeling practices that describe students\u2019 processes of model exploration, development, and testing and two themes about evaluating automated decisions from data technologies. The results provide implications for designing\u00a0\u2026",
            "url": "https://par.nsf.gov/servlets/purl/10416959",
            "publicationVenue": "Learning, Media and Technology",
            "citation_count": 3
        },
        {
            "title": "Examining computational thinking processes in modeling unstructured data",
            "authors": "Shiyan Jiang, Yingxiao Qian, Hengtao Tang, Rabia Yalcinkaya, Carolyn P Rose, Jie Chao, William Finzer",
            "year": "2023",
            "abstract": "As artificial intelligence (AI) technologies are increasingly pervasive in our daily lives, the need for students to understand the working mechanisms of AI technologies has become more urgent. Data modeling is an activity that has been proposed to engage students in reasoning about the working mechanism of AI technologies. While Computational thinking (CT) has been conceptualized as critical processes that students engage in during data modeling, much remains unexplored regarding how students created features from unstructured data to develop machine learning models. In this study, we examined high school students\u2019 patterns of iterative model development and themes of CT processes in iterative model development. Twenty-eight students from a journalism class engaged in refining machine learning models iteratively for classifying negative and positive reviews of ice cream stores. This study draws on\u00a0\u2026",
            "url": null,
            "publicationVenue": "Education and Information Technologies",
            "citation_count": 4
        },
        {
            "title": "Nine elements for robust collaborative learning analytics: A constructive collaborative critique",
            "authors": "Alyssa Friend Wise, Carolyn Rose, Sanna J\u00e4rvel\u00e4",
            "year": "2023",
            "abstract": "This editorial represents a collaborative effort between the current co-editors-in-chief of the International Journal of Computer-Supported Collaborative Learning (ijCSCL) and the recent co-editor-in-chief of the Journal of Learning Analytics (JLA), Alyssa Wise, who is also a member of the ijC (LA) have made a presence in ijCSCL. This issue in particular comprises four full articles within this scope, in addition to a timely exposition on Collaborative Learning from an ethics perspective. Thus, it is high time to bring in a voice of leadership from the LA community together with those of the CSCL community to think together about the intersection of work across the two fields. The four full articles of this March issue offer a view of the kind of work that the CSCL community is engaging in to a) capture meaningful traces of learning, b) map them onto valued learning constructs, and discover useful ways to c) present them back\u00a0\u2026",
            "url": "https://link.springer.com/article/10.1007/s11412-023-09389-x",
            "publicationVenue": "International Journal of Computer-Supported Collaborative Learning",
            "citation_count": 4
        },
        {
            "title": "Examining High School Students\u2019 Self-Efficacy in Machine Learning Practices",
            "authors": "Cansu Tatar, Jeanne McClure, Franziska Bickel, Rebecca Ellis, Kenia Wiedemann, Jie Chao, Shiyan Jiang, Carolyn P Rose",
            "year": "2023",
            "abstract": "Artificial Intelligence (AI) has increasingly become a ubiquitous face in our daily lives. Following this trend, many organizations and educational researchers started fostering AI education at the K-12 level. Yet, there is less knowledge about the impact of curriculum interventions on students' self-efficacy. In order to understand K-12 students' AI learning and interests, it is critical to examine their self-efficacy. This paper examines high school students\u2019 self-efficacy in machine learning practices before and after participating in a technology-enhanced AI curriculum intervention for three weeks. We analyzed students\u2019 pre- and post-questionnaire responses to investigate the impact of the AI curriculum intervention on students\u2019 self-efficacy. Our analysis revealed that students\u2019 self-efficacy toward text classification tasks significantly increased after they completed the AI curriculum activities. Additionally, we found that students\u2019 characteristics in terms of their interests and engagement in the activities played a critical role in their self-efficacy.",
            "url": "https://repository.isls.org/bitstream/1/9940/1/ICLS2023_1434-1437.pdf",
            "publicationVenue": "Proceedings of the 17th International Conference of the Learning Sciences-ICLS 2023, pp. 1434-1437",
            "citation_count": 0
        },
        {
            "title": "Doing Quantitative Research in the Learning Sciences and CSCL: Current Developments and Applications",
            "authors": "Frank Fischer, Freydis Vogel, Daniel Bodemer, Olga Chernikova, Ulrike Cress, Bram De Wever, Julia Eberle, Heisawn Jeong, Ingo Kollar, Jim Pellegrino, Peter Reimann, Carolyn Rose, Nikol Rummel, David Williamson Shafer, Matthias Stadler, JW Strijbos, Armin Weinberger, Jianwei Zhang",
            "year": "2023",
            "abstract": "While quantitative methods are continuously developed in various fields of origin, such as psychology, the specific applications in the core field of learning sciences and CSCL are less well advanced. In this workshop, we explore and discuss current methodological topics in three relevant fields of empirical research:(1) obtaining data,(2) analyzing data, and (3) sharing data, replicating and integrating findings. Outcomes of the discussions are planned to be published in short guidelines facilitating the application of latest developments in quantitative methods in the learning sciences and CSCL research.",
            "url": "https://www.researchgate.net/profile/Aireale-Rodgers/publication/374505086_Towards_a_Learning_Sciences_of_Higher_Ed_Engaging_the_Pedagogical_Possibilities_of_Postsecondary_Education/links/65206a7d3ab6cb4ec6c0ad6c/Towards-a-Learning-Sciences-of-Higher-Ed-Engaging-the-Pedagogical-Possibilities-of-Postsecondary-Education.pdf#page=101",
            "publicationVenue": "ISLS Annual Meeting 2023",
            "citation_count": 0
        },
        {
            "title": "Studying Interdisciplinary Collaboration as a Core Skill",
            "authors": "Rosanna Vitiello, Joey Huang, Samantha Speer, Nickolina Yankova, Kylie Peppler, Melisa Orta-Martinez, Carolyn P Rose",
            "year": "2023",
            "abstract": "At its core, collaboration is about bringing diverse perspectives together to create something new.  Diversity may arise along a multiplicity of dimensions, leading to some very similar challenges, and other dimension-specific challenges, each of which require discrete skills to address. Interdisciplinary collaboration, while understudied, has particular workplace relevance.  This research seeks to understand what is specific to interdisciplinary collaboration as part of a broader agenda to operationalize key underlying skills that enable interdisciplinary collaboration and subsequently assess and support interdisciplinary collaboration, both in the classroom and in the workplace. The aim of this poster presentation is to engage the community in an intellectual exchange about underlying questions to inform work in progress.",
            "url": "https://repository.isls.org/bitstream/1/9240/1/CSCL2023_378-379.pdf",
            "publicationVenue": "Proceedings of the 16th International Conference on Computer-Supported Collaborative Learning-CSCL 2023, pp. 378-379",
            "citation_count": 0
        },
        {
            "title": "Towards Visible Socially-Shared Regulation of Learning: Exploring the Role of Learning Design",
            "authors": "Cristina Villa-Torrano, Rosanna Vitiello, Jiaxin Shi, Carolyn P Rose, Juan I \u0391sensio-Perez, Yannis Dimitriadis, Eduardo G\u00f3mez-S\u00e1nchez, Miguel Bote-Lorenzo",
            "year": "2023",
            "abstract": "Socially Shared-Regulation of Learning processes are critical for successful collaborative learning. Despite the work done to develop a theoretical understanding of it, much less work has focused on what SSRL processes look like. This study explores how the use of the learning design, tuned to foster the phases of SSRL, and the use of tools that collect trace data, can be useful to provide evidence of where and when SSRL processes occur. The goal is to shed light on how SSRL processes look like with the global aim of supporting collaborative learning in real time. The study involved two undergraduate courses with 33 students. We identified the SSRL processes from conversations during collaboration and checked their alignment with student actions in the available learning tools. Results suggest that, at least for high performing groups, trace data interpreted in the light of the Learning Design align well with SSRL phases.",
            "url": "https://repository.isls.org/bitstream/1/9216/1/CSCL2023_289-292.pdf",
            "publicationVenue": "Proceedings of the 16th International Conference on Computer-Supported Collaborative Learning-CSCL 2023, pp. 289-292",
            "citation_count": 0
        },
        {
            "title": "SPEERLoom: Collaboratively Re-Crafting CS Education",
            "authors": "Samantha Speer, Joey Huang, Nickolina Yankova, Carolyn Rose, Kylie Peppler, Melisa Orta Martinez",
            "year": "2023",
            "abstract": "Our work aims to increase the collaborative ability of college students in computer science classrooms where students must work towards a shared goal with peers from different backgrounds and abilities. Our work focuses specifically on leveraging high-quality collaborative design to bridge the gap between fiber arts and robotics by enlightening students to their shared foundations in mathematics and computational thinking. We achieve this goal through the design of SPEERLoom (Semi-automated Pattern Executing Educational Robotic Loom), a new open-source Jacquard loom kit designed to foster students' exploration of weaving, mechatronics, mathematics, and computational thinking. In this demonstration we present SPEERLoom and allow the exploration of a sample lesson using the loom.",
            "url": "https://par.nsf.gov/servlets/purl/10435020",
            "publicationVenue": "International Collaboration toward Educational Innovation for All: International Society of the Learning Sciences (ISLS) Annual Meeting 2023",
            "citation_count": 0
        },
        {
            "title": "Traveling Bazaar: Portable Support for Face-to-Face Collaboration",
            "authors": "Rosanna Vitiello, Soham D Tiwari, R Charles Murray, Carolyn Rose",
            "year": "2023",
            "abstract": "For nearly two decades, conversational agents have been used to structure group interactions in online chat-based environments. More recently, this form of dynamic support for collaborative learning has been extended to physical spaces using a combination of multimodal sensing technologies and instrumentation installed within a physical space. This demo extends the reach of dynamic support for collaboration still further through an application of what has recently been termed on-device machine learning, which enables a portable form of multimodal detection to trigger real-time responses.",
            "url": "https://par.nsf.gov/servlets/purl/10437737",
            "publicationVenue": "International Collaboration toward Educational Innovation for All: International Society of the Learning Sciences (ISLS) Annual Meeting 2023",
            "citation_count": 1
        },
        {
            "title": "\u201cWhere is the Z-Axis?\u201d: Negotiating Understanding of Servo Rotation Through Gestures and Tools",
            "authors": "Nickolina Yankova, Joey Huang, Rosanna Vitiello, Samantha Speer, Melisa Orta-Martinez, Carolyn Rose, Kylie Peppler",
            "year": "2023",
            "abstract": "Understanding abstract concepts in mathematics has continuously presented as a challenge, but the use of directed and spontaneous gestures has shown to support learning and ground higher-order thought. Within embodied learning, gesture has been investigated as part of a multimodal assemblage with speech and movement, centering the body in interaction with the environment. We present a case study of one dyad\u2019s undertaking of a robotic arm activity, targeting learning outcomes in matrix algebra, robotics, and spatial thinking. Through a body syntonicity lens and drawing on video and pre- and post- assessment data, we evaluate learning gains and investigate the multimodal processes contributing to them. We found gesture, speech, and body movement grounded understanding of vector and matrix operations, spatial reasoning, and robotics, as anchored by the physical robotic arm, with implications for the design of learning environments that employ directed gestures.",
            "url": "https://repository.isls.org/bitstream/1/9893/1/ICLS2023_1254-1257.pdf",
            "publicationVenue": "Proceedings of the 17th International Conference of the Learning Sciences-ICLS 2023, pp. 1254-1257",
            "citation_count": 0
        },
        {
            "title": "How Do Students Deliberate for Socially Shared Regulation in Collaborative Learning? A Process-Oriented Approach",
            "authors": "Belle Dang, Rosanna Vitiello, Andy Nguyen, Carolyn P Rose, Sanna J\u00e4rvel\u00e4",
            "year": "2023",
            "abstract": "Socially shared regulation (SSRL) has been recognized as a contributing factor to successful collaborative learning. In this paper, we adopted a process-oriented approach to examine how students deliberate for SSRL through different regulatory triggers in a collaborative learning context. More specifically, this study examines the relationship between different types of regulatory and deliberative characteristics of interactions and then explores their sequential patterns through cognitive and emotional triggers. The study involved ten triads of secondary students (N= 30) working on a collaborative learning task. The process mining results showed that following regulatory triggers, groups switched to more metacognitive and socio-emotional interactions as they adopted control strategies, such as defining problems, establishing strategies, and providing social support. This study not only contributes to a better understanding of SSRL by exploring learners\u2019 deliberative negotiation but also presents a novel fine-grain video analysis approach to examine SSRL in collaborative learning.",
            "url": "https://www.researchgate.net/profile/Belle-Dang-2/publication/371672449_How_Do_Students_Deliberate_for_Socially_Shared_Regulation_in_Collaborative_Learning_A_Process-Oriented_Approach/links/648eb8abc41fb852dd0dae1f/How-Do-Students-Deliberate-for-Socially-Shared-Regulation-in-Collaborative-Learning-A-Process-Oriented-Approach.pdf",
            "publicationVenue": "Proceedings of the 16th International Conference on Computer-Supported Collaborative Learning-CSCL 2023, pp. 59-66",
            "citation_count": 4
        },
        {
            "title": "Using Technology to Foster Equitable Access and Diverse Learning Communities",
            "authors": "Rene F Kizilcec, Jon Mason, Kathryn S McCarthy, Maria Merceds T Rodrigo, Carolyn Penstein Rose",
            "year": "2023",
            "abstract": "Educators and learners worldwide turned to technology to continue teaching and learning during the pandemic. A partnership among research societies, the International Alliance to Advance Learning in a Digital Era, convened a public engagement event to highlight key lessons and practical issues for education policymakers and educators who want to create more inclusive learning environments and foster equitable access to learning opportunities using technology. The event interactively engaged more than 100 researchers, administrators, and policymakers. A key finding was that while the pandemic inadvertently deepened issues of equity and inclusion, it also demonstrated opportunities to use technology to foster access and diversity. The authors discuss these opportunities and raise questions that still need to be answered.",
            "url": "https://repository.isls.org/bitstream/1/8002/3/Kizilcec%20et%20al.%20Apr2023.pdf",
            "publicationVenue": "Digital Promise and the International Society of the Learning Sciences",
            "citation_count": 2
        },
        {
            "title": "Learning analytics",
            "authors": "James Fiacco, Shiyan Jiang, David Adamson, Carolyn P Rose",
            "year": "2023",
            "abstract": "Educational Data Mining and Learning Analytics have emerged over the past decade as interdisciplinary fields encompassing learning (e.g., learning sciences), analytics (e.g., data science), and human-centered design (e.g., usability of dashboards). In this chapter, we provide a broad overview of research in these fields, with a focus on the area of modeling unstructured natural language data. We present key lessons learned and suggest potential steps forward for building a productive interdisciplinary community. We also discuss the need of learning from each other's perspectives and challenging the underlying assumptions and unintentional bias that we may bring into the process of mining educational data.",
            "url": null,
            "publicationVenue": null,
            "citation_count": 0
        },
        {
            "title": "Examining socially shared regulation and shared physiological arousal events with multimodal learning analytics",
            "authors": "Andy Nguyen, Sanna J\u00e4rvel\u00e4, Carolyn Rose, Hanna J\u00e4rvenoja, Jonna Malmberg",
            "year": "2023",
            "abstract": "Socially shared regulation contributes to the success of collaborative learning. However, the assessment of socially shared regulation of learning (SSRL) faces several challenges in the effort to increase the understanding of collaborative learning and support outcomes due to the unobservability of the related cognitive and emotional processes. The recent development of trace\u2010based assessment has enabled innovative opportunities to overcome the problem. Despite the potential of a trace\u2010based approach to study SSRL, there remains a paucity of evidence on how trace\u2010based evidence could be captured and utilised to assess and promote SSRL. This study aims to investigate the assessment of electrodermal activities (EDA) data to understand and support SSRL in collaborative learning, hence enhancing learning outcomes. The data collection involves secondary school students (N=\u00a094) working\u00a0\u2026",
            "url": "https://bera-journals.onlinelibrary.wiley.com/doi/pdfdirect/10.1111/bjet.13280",
            "publicationVenue": "British Journal of Educational Technology",
            "citation_count": 22
        }
    ],
    "Alexander Rudnicky": [
        {
            "title": "Attention Alignment and Flexible Positional Embeddings Improve Transformer Length Extrapolation",
            "authors": "Ta-Chung Chi, Ting-Han Fan, Alexander I Rudnicky",
            "year": "2023",
            "abstract": "An ideal length-extrapolatable Transformer language model can handle sequences longer than the training length without any long sequence fine-tuning. Such long-context utilization capability highly relies on a flexible positional embedding design. Upon investigating the flexibility of existing large pre-trained Transformer language models, we find that the T5 family deserves a closer look, as its positional embeddings capture rich and flexible attention patterns. However, T5 suffers from the dispersed attention issue: the longer the input sequence, the flatter the attention distribution. To alleviate the issue, we propose two attention alignment strategies via temperature scaling. Our findings improve the long-context utilization capability of T5 on language modeling, retrieval, and multi-document question answering without any fine-tuning, suggesting that a flexible positional embedding design and attention alignment go a long way toward Transformer length extrapolation.\\footnote{\\url{https://github.com/chijames/Attention-Alignment-Transformer-Length-Extrapolation}}",
            "url": "https://arxiv.org/pdf/2311.00684",
            "publicationVenue": "arXiv preprint arXiv:2311.00684",
            "citation_count": 0
        },
        {
            "title": "Advancing Regular Language Reasoning in Linear Recurrent Neural Networks",
            "authors": "Ting-Han Fan, Ta-Chung Chi, Alexander I Rudnicky",
            "year": "2023",
            "abstract": "In recent studies, linear recurrent neural networks (LRNNs) have achieved Transformer-level performance in natural language modeling and long-range modeling while offering rapid parallel training and constant inference costs. With the resurged interest in LRNNs, we study whether they can learn the hidden rules in training sequences, such as the grammatical structures of regular language. We theoretically analyze some existing LRNNs and discover their limitations on regular language. Motivated by the analysis, we propose a new LRNN equipped with a block-diagonal and input-dependent transition matrix. Experiments suggest that the proposed model is the only LRNN that can perform length extrapolation on regular language tasks such as Sum, Even Pair, and Modular Arithmetic.",
            "url": "https://arxiv.org/pdf/2309.07412",
            "publicationVenue": "arXiv preprint arXiv:2309.07412",
            "citation_count": 0
        },
        {
            "title": "Learning to Ask Questions for Zero-shot Dialogue State Tracking",
            "authors": "Diogo Tavares, David Semedo, Alexander Rudnicky, Joao Magalhaes",
            "year": "2023",
            "abstract": "We present a method for performing zero-shot Dialogue State Tracking (DST) by casting the task as a learning-to-ask-questions framework. The framework learns to pair the best question generation (QG) strategy with in-domain question answering (QA) methods to extract slot values from a dialogue without any human intervention. A novel self-supervised QA pretraining step using in-domain data is essential to learn the structure without requiring any slot-filling annotations. Moreover, we show that QG methods need to be aligned with the same grammatical person used in the dialogue. Empirical evaluation on the MultiWOZ 2.1 dataset demonstrates that our approach, when used alongside robust QA models, outperforms existing zero-shot methods in the challenging task of zero-shot cross domain adaptation-given a comparable amount of domain knowledge during data creation. Finally, we analyze the impact of\u00a0\u2026",
            "url": "https://dl.acm.org/doi/pdf/10.1145/3539618.3592010",
            "publicationVenue": "Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval",
            "citation_count": 0
        },
        {
            "title": "Overview of the Tenth Dialog System Technology Challenge: DSTC10",
            "authors": "Koichiro Yoshino, Yun-Nung Chen, Paul Crook, Satwik Kottur, Jinchao Li, Behnam Hedayatnia, Seungwhan Moon, Zhengcong Fei, Zekang Li, Jinchao Zhang, Yang Feng, Jie Zhou, Seokhwan Kim, Yang Liu, Di Jin, Alexandros Papangelis, Karthik Gopalakrishnan, Dilek Hakkani-Tur, Babak Damavandi, Alborz Geramifard, Chiori Hori, Ankit Shah, Chen Zhang, Haizhou Li, Jo\u00e3o Sedoc, Luis F D'haro, Rafael Banchs, Alexander Rudnicky",
            "year": "2023",
            "abstract": "This article introduces the Tenth Dialog System Technology Challenge (DSTC-10). This edition of the DSTC focuses on applying end-to-end dialog technologies for five distinct tasks in dialog systems, namely 1. Incorporation of Meme images into open domain dialogs, 2. Knowledge-grounded Task-oriented Dialogue Modeling on Spoken Conversations, 3. Situated Interactive Multimodal dialogs, 4. Reasoning for Audio Visual Scene-Aware Dialog, and 5. Automatic Evaluation and Moderation of Open-domainDialogue Systems. This article describes the task definition, provided datasets, baselines, and evaluation setup for each track. We also summarize the results of the submitted systems to highlight the general trends of the state-of-the-art technologies for the tasks.",
            "url": "https://ieeexplore.ieee.org/iel7/6570655/6633080/10174647.pdf",
            "publicationVenue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing",
            "citation_count": 2
        },
        {
            "title": "Dissecting transformer length extrapolation via the lens of receptive field analysis",
            "authors": "Ta-Chung Chi, Ting-Han Fan, Alexander Rudnicky, Peter Ramadge",
            "year": "2023",
            "abstract": "Length extrapolation permits training a transformer language model on short sequences that preserves perplexities when tested on substantially longer sequences. A relative positional embedding design, ALiBi, has had the widest usage to date. We dissect ALiBi via the lens of receptive field analysis empowered by a novel cumulative normalized gradient tool. The concept of receptive field further allows us to modify the vanilla Sinusoidal positional embedding to create Sandwich, the first parameter-free relative positional embedding design that truly length information uses longer than the training sequence. Sandwich shares with KERPLE and T5 the same logarithmic decaying temporal bias pattern with learnable relative positional embeddings; these elucidate future extrapolatable positional embedding design.",
            "url": "https://aclanthology.org/2023.acl-long.756.pdf",
            "publicationVenue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
            "citation_count": 12
        },
        {
            "title": "Structured dialogue discourse parsing",
            "authors": "Ta-Chung Chi, Alexander I Rudnicky",
            "year": "2023",
            "abstract": "Dialogue discourse parsing aims to uncover the internal structure of a multi-participant conversation by finding all the discourse~\\emph{links} and corresponding~\\emph{relations}. Previous work either treats this task as a series of independent multiple-choice problems, in which the link existence and relations are decoded separately, or the encoding is restricted to only local interaction, ignoring the holistic structural information. In contrast, we propose a principled method that improves upon previous work from two perspectives: encoding and decoding. From the encoding side, we perform structured encoding on the adjacency matrix followed by the matrix-tree learning algorithm, where all discourse links and relations in the dialogue are jointly optimized based on latent tree-level distribution. From the decoding side, we perform structured inference using the modified Chiu-Liu-Edmonds algorithm, which explicitly generates the labeled multi-root non-projective spanning tree that best captures the discourse structure. In addition, unlike in previous work, we do not rely on hand-crafted features; this improves the model's robustness. Experiments show that our method achieves new state-of-the-art, surpassing the previous model by 2.3 on STAC and 1.5 on Molweni (F1 scores). \\footnote{Code released at~\\url{https://github.com/chijames/structured_dialogue_discourse_parsing}.}",
            "url": "https://arxiv.org/pdf/2306.15103",
            "publicationVenue": "arXiv preprint arXiv:2306.15103",
            "citation_count": 8
        },
        {
            "title": "Overview of Robust and Multilingual Automatic Evaluation Metrics for Open-Domain Dialogue Systems at DSTC 11 Track 4",
            "authors": "Mario Rodr\u00edguez-Cantelar, Chen Zhang, Chengguang Tang, Ke Shi, Sarik Ghazarian, Jo\u00e3o Sedoc, Luis Fernando D'Haro, Alexander Rudnicky",
            "year": "2023",
            "abstract": "The advent and fast development of neural networks have revolutionized the research on dialogue systems and subsequently have triggered various challenges regarding their automatic evaluation. Automatic evaluation of open-domain dialogue systems as an open challenge has been the center of the attention of many researchers. Despite the consistent efforts to improve automatic metrics' correlations with human evaluation, there have been very few attempts to assess their robustness over multiple domains and dimensions. Also, their focus is mainly on the English language. All of these challenges prompt the development of automatic evaluation metrics that are reliable in various domains, dimensions, and languages. This track in the 11th Dialogue System Technology Challenge (DSTC11) is part of the ongoing effort to promote robust and multilingual automatic evaluation metrics. This article describes the datasets and baselines provided to participants and discusses the submission and result details of the two proposed subtasks.",
            "url": "https://arxiv.org/pdf/2306.12794",
            "publicationVenue": "arXiv preprint arXiv:2306.12794",
            "citation_count": 4
        },
        {
            "title": "A unified one-shot prosody and speaker conversion system with self-supervised discrete speech units",
            "authors": "Li-Wei Chen, Shinji Watanabe, Alexander Rudnicky",
            "year": "2023",
            "abstract": "We present a unified system to realize one-shot voice conversion (VC) on the pitch, rhythm, and speaker attributes. Existing works generally ignore the correlation between prosody and language content, leading to the degradation of naturalness in converted speech. Additionally, the lack of proper language features prevents these systems from accurately preserving language content after conversion. To address these issues, we devise a cascaded modular system leveraging self-supervised discrete speech units as language representation. These discrete units provide duration information essential for rhythm modeling. Our system first extracts utterance-level prosody and speaker representations from the raw waveform. Given the prosody representation, a prosody predictor estimates pitch, energy, and duration for each discrete unit in the utterance. A synthesizer further reconstructs speech based on the\u00a0\u2026",
            "url": "https://arxiv.org/pdf/2211.06535",
            "publicationVenue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
            "citation_count": 2
        },
        {
            "title": "Exploring Wav2vec 2.0 Fine Tuning for Improved Speech Emotion Recognition",
            "authors": "Li-Wei Chen, Alexander Rudnicky",
            "year": "2023",
            "abstract": "While Wav2Vec 2.0 has been proposed for speech recognition (ASR), it can also be used for speech emotion recognition (SER); its performance can be significantly improved using different fine-tuning strategies. Two baseline methods, vanilla fine-tuning (V-FT) and task adaptive pretraining (TAPT) are first presented. We show that V-FT is able to outperform state-of-the-art models on the IEMOCAP dataset. TAPT, an existing NLP fine-tuning strategy, further improves the performance on SER. We also introduce a novel fine-tuning method termed P-TAPT, which modifies the TAPT objective to learn contextualized emotion representations. Experiments show that P-TAPT performs better than TAPT, especially under low-resource settings. Compared to prior works in this literature, our top-line system achieved a 7.4% absolute improvement in unweighted accuracy (UA) over the state-of-the-art performance on IEMOCAP\u00a0\u2026",
            "url": "https://arxiv.org/pdf/2110.06309",
            "publicationVenue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
            "citation_count": 74
        },
        {
            "title": "Latent Positional Information is in the Self-Attention Variance of Transformer Language Models Without Positional Embeddings",
            "authors": "Ta-Chung Chi, Ting-Han Fan, Li-Wei Chen, Alexander I Rudnicky, Peter J Ramadge",
            "year": "2023",
            "abstract": "The use of positional embeddings in transformer language models is widely accepted. However, recent research has called into question the necessity of such embeddings. We further extend this inquiry by demonstrating that a randomly initialized and frozen transformer language model, devoid of positional embeddings, inherently encodes strong positional information through the shrinkage of self-attention variance. To quantify this variance, we derive the underlying distribution of each step within a transformer layer. Through empirical validation using a fully pretrained model, we show that the variance shrinkage effect still persists after extensive gradient updates. Our findings serve to justify the decision to discard positional embeddings and thus facilitate more efficient pretraining of transformer language models.",
            "url": "https://arxiv.org/pdf/2305.13571",
            "publicationVenue": "arXiv preprint arXiv:2305.13571",
            "citation_count": 1
        },
        {
            "title": "Transformer Working Memory Enables Regular Language Reasoning and Natural Language Length Extrapolation",
            "authors": "Ta-Chung Chi, Ting-Han Fan, Alexander I Rudnicky, Peter J Ramadge",
            "year": "2023",
            "abstract": "Unlike recurrent models, conventional wisdom has it that Transformers cannot perfectly model regular languages. Inspired by the notion of working memory, we propose a new Transformer variant named RegularGPT. With its novel combination of Weight-Sharing, Adaptive-Depth, and Sliding-Dilated-Attention, RegularGPT constructs working memory along the depth dimension, thereby enabling efficient and successful modeling of regular languages such as PARITY. We further test RegularGPT on the task of natural language length extrapolation and surprisingly find that it rediscovers the local windowed attention effect deemed necessary in prior work for length extrapolation.",
            "url": "https://arxiv.org/pdf/2305.03796",
            "publicationVenue": "arXiv preprint arXiv:2305.03796",
            "citation_count": 2
        },
        {
            "title": "A vector quantized approach for text to speech synthesis on real-world spontaneous speech",
            "authors": "Li-Wei Chen, Shinji Watanabe, Alexander Rudnicky",
            "year": "2023",
            "abstract": "Recent Text-to-Speech (TTS) systems trained on reading or acted corpora have achieved near human-level naturalness. The diversity of human speech, however, often goes beyond the coverage of these corpora. We believe the ability to handle such diversity is crucial for AI systems to achieve human-level communication. Our work explores the use of more abundant real-world data for building speech synthesizers. We train TTS systems using real-world speech from YouTube and podcasts. We observe the mismatch between training and inference alignments in mel-spectrogram based autoregressive models, leading to unintelligible synthesis, and demonstrate that learned discrete codes within multiple code groups effectively resolves this issue. We introduce our MQTTS system whose architecture is designed for multiple code generation and monotonic alignment, along with the use of a clean silence prompt to improve synthesis quality. We conduct ablation analyses to identify the efficacy of our methods. We show that MQTTS outperforms existing TTS systems in several objective and subjective measures.",
            "url": "https://arxiv.org/pdf/2302.04215",
            "publicationVenue": "arXiv preprint arXiv:2302.04215",
            "citation_count": 16
        }
    ],
    "Maarten Sap": [
        {
            "title": "Beyond Denouncing Hate: Strategies for Countering Implied Biases and Stereotypes in Language",
            "authors": "Jimin Mun, Emily Allaway, Akhila Yerukola, Laura Vianna, Sarah-Jane Leslie, Maarten Sap",
            "year": "2023",
            "abstract": null,
            "url": null,
            "publicationVenue": "Findings of the Association for Computational Linguistics: EMNLP 2023",
            "citation_count": 2
        },
        {
            "title": "Where Do People Tell Stories Online? Story Detection Across Online Communities",
            "authors": "Maria Antoniak, Joel Mire, Maarten Sap, Elliott Ash, Andrew Piper",
            "year": "2023",
            "abstract": "People share stories online for a myriad of purposes, whether as a means of self-disclosure, processing difficult personal experiences, providing needed information or entertainment, or persuading others to share their beliefs. Better understanding of online storytelling can illuminate the dynamics of social movements, sensemaking practices, persuasion strategies, and more. However, unlike other media such as books and visual content where the narrative nature of the content is often overtly signaled at the document level, studying storytelling in online communities is challenging due to the mixture of storytelling and non-storytelling behavior, which can be interspersed within documents and across diverse topics and settings. We introduce a codebook and create the Storytelling in Online Communities Corpus, an expert-annotated dataset of 502 English-language posts and comments with labeled story and event spans. Using our corpus, we train and evaluate an online story detection model, which we use to investigate the role storytelling of in different social contexts. We identify distinctive features of online storytelling, the prevalence of storytelling among different communities, and the conversational patterns of storytelling.",
            "url": "https://arxiv.org/pdf/2311.09675",
            "publicationVenue": "arXiv preprint arXiv:2311.09675",
            "citation_count": 0
        },
        {
            "title": "Can llms keep a secret? testing privacy implications of language models via contextual integrity theory",
            "authors": "Niloofar Mireshghallah, Hyunwoo Kim, Xuhui Zhou, Yulia Tsvetkov, Maarten Sap, Reza Shokri, Yejin Choi",
            "year": "2023",
            "abstract": "The interactive use of large language models (LLMs) in AI assistants (at work, home, etc.) introduces a new set of inference-time privacy risks: LLMs are fed different types of information from multiple sources in their inputs and are expected to reason about what to share in their outputs, for what purpose and with whom, within a given context. In this work, we draw attention to the highly critical yet overlooked notion of contextual privacy by proposing ConfAIde, a benchmark designed to identify critical weaknesses in the privacy reasoning capabilities of instruction-tuned LLMs. Our experiments show that even the most capable models such as GPT-4 and ChatGPT reveal private information in contexts that humans would not, 39% and 57% of the time, respectively. This leakage persists even when we employ privacy-inducing prompts or chain-of-thought reasoning. Our work underscores the immediate need to explore novel inference-time privacy-preserving approaches, based on reasoning and theory of mind.",
            "url": "https://arxiv.org/pdf/2310.17884",
            "publicationVenue": "arXiv preprint arXiv:2310.17884",
            "citation_count": 8
        },
        {
            "title": "FANToM: A benchmark for stress-testing machine theory of mind in interactions",
            "authors": "Hyunwoo Kim, Melanie Sclar, Xuhui Zhou, Ronan Le Bras, Gunhee Kim, Yejin Choi, Maarten Sap",
            "year": "2023",
            "abstract": "Theory of mind (ToM) evaluations currently focus on testing models using passive narratives that inherently lack interactivity. We introduce FANToM, a new benchmark designed to stress-test ToM within information-asymmetric conversational contexts via question answering. Our benchmark draws upon important theoretical requisites from psychology and necessary empirical considerations when evaluating large language models (LLMs). In particular, we formulate multiple types of questions that demand the same underlying reasoning to identify illusory or false sense of ToM capabilities in LLMs. We show that FANToM is challenging for state-of-the-art LLMs, which perform significantly worse than humans even with chain-of-thought reasoning or fine-tuning.",
            "url": "https://arxiv.org/pdf/2310.15421",
            "publicationVenue": "arXiv preprint arXiv:2310.15421",
            "citation_count": 4
        },
        {
            "title": "Sotopia: Interactive evaluation for social intelligence in language agents",
            "authors": "Xuhui Zhou, Hao Zhu, Leena Mathur, Ruohong Zhang, Haofei Yu, Zhengyang Qi, Louis-Philippe Morency, Yonatan Bisk, Daniel Fried, Graham Neubig, Maarten Sap",
            "year": "2023",
            "abstract": "Humans are social beings; we pursue social goals in our daily interactions, which is a crucial aspect of social intelligence. Yet, AI systems' abilities in this realm remain elusive. We present SOTOPIA, an open-ended environment to simulate complex social interactions between artificial agents and evaluate their social intelligence. In our environment, agents role-play and interact under a wide variety of scenarios; they coordinate, collaborate, exchange, and compete with each other to achieve complex social goals. We simulate the role-play interaction between LLM-based agents and humans within this task space and evaluate their performance with a holistic evaluation framework called SOTOPIA-Eval. With SOTOPIA, we find significant differences between these models in terms of their social intelligence, and we identify a subset of SOTOPIA scenarios, SOTOPIA-hard, that is generally challenging for all models. We find that on this subset, GPT-4 achieves a significantly lower goal completion rate than humans and struggles to exhibit social commonsense reasoning and strategic communication skills. These findings demonstrate SOTOPIA's promise as a general platform for research on evaluating and improving social intelligence in artificial agents.",
            "url": "https://arxiv.org/pdf/2310.11667",
            "publicationVenue": "arXiv preprint arXiv:2310.11667",
            "citation_count": 13
        },
        {
            "title": "Value kaleidoscope: Engaging ai with pluralistic human values, rights, and duties",
            "authors": "Taylor Sorensen, Liwei Jiang, Jena Hwang, Sydney Levine, Valentina Pyatkin, Peter West, Nouha Dziri, Ximing Lu, Kavel Rao, Chandra Bhagavatula, Maarten Sap, John Tasioulas, Yejin Choi",
            "year": "2023",
            "abstract": "Human values are crucial to human decision-making. Value pluralism is the view that multiple correct values may be held in tension with one another (e.g., when considering lying to a friend to protect their feelings, how does one balance honesty with friendship?). As statistical learners, AI systems fit to averages by default, washing out these potentially irreducible value conflicts. To improve AI systems to better reflect value pluralism, the first-order challenge is to explore the extent to which AI systems can model pluralistic human values, rights, and duties as well as their interaction. We introduce ValuePrism, a large-scale dataset of 218k values, rights, and duties connected to 31k human-written situations. ValuePrism's contextualized values are generated by GPT-4 and deemed high-quality by human annotators 91% of the time. We conduct a large-scale study with annotators across diverse social and demographic backgrounds to try to understand whose values are represented. With ValuePrism, we build Kaleido, an open, light-weight, and structured language-based multi-task model that generates, explains, and assesses the relevance and valence (i.e., support or oppose) of human values, rights, and duties within a specific context. Humans prefer the sets of values output by our system over the teacher GPT-4, finding them more accurate and with broader coverage. In addition, we demonstrate that Kaleido can help explain variability in human decision-making by outputting contrasting values. Finally, we show that Kaleido's representations transfer to other philosophical frameworks and datasets, confirming the benefit of an explicit, modular, and\u00a0\u2026",
            "url": "https://arxiv.org/pdf/2309.00779",
            "publicationVenue": "arXiv preprint arXiv:2309.00779",
            "citation_count": 8
        },
        {
            "title": "Queer In AI: A Case Study in Community-Led Participatory AI",
            "authors": "Organizers Of Queerinai, Anaelia Ovalle, Arjun Subramonian, Ashwin Singh, Claas Voelcker, Danica J Sutherland, Davide Locatelli, Eva Breznik, Filip Klubicka, Hang Yuan, Huan Zhang, Jaidev Shriram, Kruno Lehman, Luca Soldaini, Maarten Sap, Marc Peter Deisenroth, Maria Leonor Pacheco, Maria Ryskina, Martin Mundt, Milind Agarwal, Nyx Mclean, Pan Xu, A Pranav, Raj Korpan, Ruchira Ray, Sarah Mathew, Sarthak Arora, St John, Tanvi Anand, Vishakha Agrawal, William Agnew, Yanan Long, Zijie J Wang, Zeerak Talat, Avijit Ghosh, Nathaniel Dennler, Michael Noseworthy, Sharvani Jha, Emi Baylor, Aditya Joshi, Natalia Y Bilenko, Andrew Mcnamara, Raphael Gontijo-Lopes, Alex Markham, Evyn Dong, Jackie Kay, Manu Saraswat, Nikhil Vytla, Luke Stark",
            "year": "2023",
            "abstract": "Queerness and queer people face an uncertain future in the face of ever more widely deployed and invasive artificial intelligence (AI). These technologies have caused numerous harms to queer people, including privacy violations, censoring and downranking queer content, exposing queer people and spaces to harassment by making them hypervisible, deadnaming and outing queer people. More broadly, they have violated core tenets of queerness by classifying and controlling queer identities. In response to this, the queer community in AI has organized Queer in AI, a global, decentralized, volunteer-run grassroots organization that employs intersectional and community-led participatory design to build an inclusive and equitable AI future. In this paper, we present Queer in AI as a case study for community-led participatory design in AI. We examine how participatory design and intersectional tenets started and\u00a0\u2026",
            "url": "https://discovery.ucl.ac.uk/id/eprint/10173681/1/2303.16972v3.pdf",
            "publicationVenue": "Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency",
            "citation_count": 9
        },
        {
            "title": "Counter Statements Effect Study",
            "authors": "Jimin Mun, Maarten Sap, Laura Vianna",
            "year": "2023",
            "abstract": "This study aims to measure how stereotypical beliefs expressed as generics are affected by different types of counter statements. We look at four different treatments: alternate qualities based counter statement, positive quality subgroup examples counter statement, and external cause based counter statement, and general denouncing counter statement.",
            "url": null,
            "publicationVenue": null,
            "citation_count": 0
        },
        {
            "title": "Cobra frames: Contextual reasoning about effects and harms of offensive statements",
            "authors": "Xuhui Zhou, Hao Zhu, Akhila Yerukola, Thomas Davidson, Jena D Hwang, Swabha Swayamdipta, Maarten Sap",
            "year": "2023",
            "abstract": "Warning: This paper contains content that may be offensive or upsetting. Understanding the harms and offensiveness of statements requires reasoning about the social and situational context in which statements are made. For example, the utterance \"your English is very good\" may implicitly signal an insult when uttered by a white man to a non-white colleague, but uttered by an ESL teacher to their student would be interpreted as a genuine compliment. Such contextual factors have been largely ignored by previous approaches to toxic language detection. We introduce COBRA frames, the first context-aware formalism for explaining the intents, reactions, and harms of offensive or biased statements grounded in their social and situational context. We create COBRACORPUS, a dataset of 33k potentially offensive statements paired with machine-generated contexts and free-text explanations of offensiveness, implied biases, speaker intents, and listener reactions. To study the contextual dynamics of offensiveness, we train models to generate COBRA explanations, with and without access to the context. We find that explanations by context-agnostic models are significantly worse than by context-aware ones, especially in situations where the context inverts the statement's offensiveness (29% accuracy drop). Our work highlights the importance and feasibility of contextualized NLP by modeling social factors.",
            "url": "https://arxiv.org/pdf/2306.01985",
            "publicationVenue": "arXiv preprint arXiv:2306.01985",
            "citation_count": 7
        },
        {
            "title": "NLPositionality: Characterizing Design Biases of Datasets and Models",
            "authors": "Sebastin Santy, Jenny T Liang, Ronan Le Bras, Katharina Reinecke, Maarten Sap",
            "year": "2023",
            "abstract": "Design biases in NLP systems, such as performance differences for different populations, often stem from their creator's positionality, i.e., views and lived experiences shaped by identity and background. Despite the prevalence and risks of design biases, they are hard to quantify because researcher, system, and dataset positionality is often unobserved. We introduce NLPositionality, a framework for characterizing design biases and quantifying the positionality of NLP datasets and models. Our framework continuously collects annotations from a diverse pool of volunteer participants on LabintheWild, and statistically quantifies alignment with dataset labels and model predictions. We apply NLPositionality to existing datasets and models for two tasks -- social acceptability and hate speech detection. To date, we have collected 16,299 annotations in over a year for 600 instances from 1,096 annotators across 87 countries. We find that datasets and models align predominantly with Western, White, college-educated, and younger populations. Additionally, certain groups, such as non-binary people and non-native English speakers, are further marginalized by datasets and models as they rank least in alignment across all tasks. Finally, we draw from prior literature to discuss how researchers can examine their own positionality and that of their datasets and models, opening the door for more inclusive NLP systems.",
            "url": "https://arxiv.org/pdf/2306.01943",
            "publicationVenue": "arXiv preprint arXiv:2306.01943",
            "citation_count": 21
        },
        {
            "title": "From Dogwhistles to Bullhorns: Unveiling Coded Rhetoric with Language Models",
            "authors": "Julia Mendelsohn, Ronan Le Bras, Yejin Choi, Maarten Sap",
            "year": "2023",
            "abstract": "Dogwhistles are coded expressions that simultaneously convey one meaning to a broad audience and a second one, often hateful or provocative, to a narrow in-group; they are deployed to evade both political repercussions and algorithmic content moderation. For example, in the sentence 'we need to end the cosmopolitan experiment,' the word 'cosmopolitan' likely means 'worldly' to many, but secretly means 'Jewish' to a select few. We present the first large-scale computational investigation of dogwhistles. We develop a typology of dogwhistles, curate the largest-to-date glossary of over 300 dogwhistles with rich contextual information and examples, and analyze their usage in historical U.S. politicians' speeches. We then assess whether a large language model (GPT-3) can identify dogwhistles and their meanings, and find that GPT-3's performance varies widely across types of dogwhistles and targeted groups. Finally, we show that harmful content containing dogwhistles avoids toxicity detection, highlighting online risks of such coded language. This work sheds light on the theoretical and applied importance of dogwhistles in both NLP and computational social science, and provides resources for future research in modeling dogwhistles and mitigating their online harms.",
            "url": "https://arxiv.org/pdf/2305.17174",
            "publicationVenue": "arXiv preprint arXiv:2305.17174",
            "citation_count": 6
        },
        {
            "title": "Improving Language Models with Advantage-based Offline Policy Gradients",
            "authors": "Ashutosh Baheti, Ximing Lu, Faeze Brahman, Ronan Le Bras, Maarten Sap, Mark Riedl",
            "year": "2023",
            "abstract": "Improving language model generations according to some user-defined quality or style constraints is challenging. Typical approaches include learning on additional human-written data, filtering ``low-quality'' data using heuristics and/or using reinforcement learning with human feedback (RLHF). However, filtering can remove valuable training signals, whereas data collection and RLHF constantly require additional human-written or LM exploration data which can be costly to obtain. A natural question to ask is ``Can we leverage RL to optimize LM utility on existing crowd-sourced and internet data?'' To this end, we present Left-over Lunch RL (LoL-RL), a simple training algorithm that uses offline policy gradients for learning language generation tasks as a 1-step RL game. LoL-RL can finetune LMs to optimize arbitrary classifier-based or human-defined utility functions on any sequence-to-sequence data. Experiments with five different language generation tasks using models of varying sizes and multiple rewards show that models trained with LoL-RL can consistently outperform the best supervised learning models. We also release our experimental code. https://github.com/abaheti95/LoL-RL",
            "url": "https://arxiv.org/pdf/2305.14718",
            "publicationVenue": "arXiv preprint arXiv:2305.14718",
            "citation_count": 3
        },
        {
            "title": "Don't Take This Out of Context! On the Need for Contextual Models and Evaluations for Stylistic Rewriting",
            "authors": "Akhila Yerukola, Xuhui Zhou, Maarten Sap",
            "year": "2023",
            "abstract": "Most existing stylistic text rewriting methods operate on a sentence level, but ignoring the broader context of the text can lead to generic, ambiguous, and incoherent rewrites. In this paper, we propose the integration of preceding textual context into both the rewriting and evaluation stages of stylistic text rewriting, focusing on formality, toxicity, and sentiment transfer tasks. We conduct a comparative evaluation of rewriting through few-shot prompting of GPT-3.5 and GPT NeoX, comparing non-contextual rewrites to contextual rewrites. Our experiments show that humans often prefer contextual rewrites over non-contextual ones, but automatic metrics (e.g., BLEU, sBERT) do not. To bridge this gap, we propose context-infused versions of common automatic metrics, and show that these better reflect human preferences. Overall, our paper highlights the importance of integrating preceding textual context into both the rewriting and evaluation stages of stylistic text rewriting.",
            "url": "https://arxiv.org/pdf/2305.14755",
            "publicationVenue": "arXiv preprint arXiv:2305.14755",
            "citation_count": 0
        },
        {
            "title": "Clever hans or neural theory of mind? stress testing social reasoning in large language models",
            "authors": "Natalie Shapira, Mosh Levy, Seyed Hossein Alavi, Xuhui Zhou, Yejin Choi, Yoav Goldberg, Maarten Sap, Vered Shwartz",
            "year": "2023",
            "abstract": "The escalating debate on AI's capabilities warrants developing reliable metrics to assess machine \"intelligence\". Recently, many anecdotal examples were used to suggest that newer large language models (LLMs) like ChatGPT and GPT-4 exhibit Neural Theory-of-Mind (N-ToM); however, prior work reached conflicting conclusions regarding those abilities. We investigate the extent of LLMs' N-ToM through an extensive evaluation on 6 tasks and find that while LLMs exhibit certain N-ToM abilities, this behavior is far from being robust. We further examine the factors impacting performance on N-ToM tasks and discover that LLMs struggle with adversarial examples, indicating reliance on shallow heuristics rather than robust ToM abilities. We caution against drawing conclusions from anecdotal examples, limited benchmark testing, and using human-designed psychological tests to evaluate models.",
            "url": "https://arxiv.org/pdf/2305.14763",
            "publicationVenue": "arXiv preprint arXiv:2305.14763",
            "citation_count": 34
        },
        {
            "title": "BiasX:\" Thinking Slow\" in Toxic Content Moderation with Explanations of Implied Social Biases",
            "authors": "Yiming Zhang, Sravani Nanduri, Liwei Jiang, Tongshuang Wu, Maarten Sap",
            "year": "2023",
            "abstract": "Toxicity annotators and content moderators often default to mental shortcuts when making decisions. This can lead to subtle toxicity being missed, and seemingly toxic but harmless content being over-detected. We introduce BiasX, a framework that enhances content moderation setups with free-text explanations of statements' implied social biases, and explore its effectiveness through a large-scale crowdsourced user study. We show that indeed, participants substantially benefit from explanations for correctly identifying subtly (non-)toxic content. The quality of explanations is critical: imperfect machine-generated explanations (+2.4% on hard toxic examples) help less compared to expert-written human explanations (+7.2%). Our results showcase the promise of using free-text explanations to encourage more thoughtful toxicity moderation.",
            "url": "https://arxiv.org/pdf/2305.13589",
            "publicationVenue": "arXiv preprint arXiv:2305.13589",
            "citation_count": 0
        },
        {
            "title": "Modeling Empathic Similarity in Personal Narratives",
            "authors": "Jocelyn Shen, Maarten Sap, Pedro Colon-Hernandez, Hae Won Park, Cynthia Breazeal",
            "year": "2023",
            "abstract": "The most meaningful connections between people are often fostered through expression of shared vulnerability and emotional experiences in personal narratives. We introduce a new task of identifying similarity in personal stories based on empathic resonance, i.e., the extent to which two people empathize with each others' experiences, as opposed to raw semantic or lexical similarity, as has predominantly been studied in NLP. Using insights from social psychology, we craft a framework that operationalizes empathic similarity in terms of three key features of stories: main events, emotional trajectories, and overall morals or takeaways. We create EmpathicStories, a dataset of 1,500 personal stories annotated with our empathic similarity features, and 2,000 pairs of stories annotated with empathic similarity scores. Using our dataset, we fine-tune a model to compute empathic similarity of story pairs, and show that this outperforms semantic similarity models on automated correlation and retrieval metrics. Through a user study with 150 participants, we also assess the effect our model has on retrieving stories that users empathize with, compared to naive semantic similarity-based retrieval, and find that participants empathized significantly more with stories retrieved by our model. Our work has strong implications for the use of empathy-aware models to foster human connection and empathy between people.",
            "url": "https://arxiv.org/pdf/2305.14246",
            "publicationVenue": "arXiv preprint arXiv:2305.14246",
            "citation_count": 1
        },
        {
            "title": "Queer In AI: A Case Study in Community-Led Participatory AI",
            "authors": "Anaelia Ovalle, Arjun Subramonian, Ashwin Singh, Claas Voelcker, Danica J Sutherland, Davide Locatelli, Eva Breznik, Filip Klubi\u010dka, Hang Yuan, Huan Zhang, Jaidev Shriram, Kruno Lehman, Luca Soldaini, Maarten Sap, Marc Peter Deisenroth, Maria Leonor Pacheco, Maria Ryskina, Martin Mundt, Melvin Selim Atay, Milind Agarwal, Nyx McLean, Pan Xu, A Pranav, Raj Korpan, Ruchira Ray, Sarah Mathew, Sarthak Arora, St John, Tanvi Anand, Vishakha Agrawal, William Agnew, Yanan Long, Zijie J Wang, Zeerak Talat, Avijit Ghosh, Nathaniel Dennler, Michael Noseworthy, Sharvani Jha, Emi Baylor, Aditya Joshi, Natalia Y Bilenko, Andrew McNamara, Raphael Gontijo-Lopes, Alex Markham, Evyn D\u01d2ng, Jackie Kay, Manu Saraswat, Nikhil Vytla, Luke Stark",
            "year": "2023",
            "abstract": "We present Queer in AI as a case study for community-led participatory design in AI. We examine how participatory design and intersectional tenets started and shaped this community's programs over the years. We discuss different challenges that emerged in the process, look at ways this organization has fallen short of operationalizing participatory and intersectional principles, and then assess the organization's impact. Queer in AI provides important lessons and insights for practitioners and theorists of participatory methods broadly through its rejection of hierarchy in favor of decentralization, success at building aid and programs by and for the queer community, and effort to change actors and institutions outside of the queer community. Finally, we theorize how communities like Queer in AI contribute to the participatory design in AI more broadly by fostering cultures of participation in AI, welcoming and empowering marginalized participants, critiquing poor or exploitative participatory practices, and bringing participation to institutions outside of individual research projects. Queer in AI's work serves as a case study of grassroots activism and participatory methods within AI, demonstrating the potential of community-led participatory methods and intersectional praxis, while also providing challenges, case studies, and nuanced insights to researchers developing and using participatory methods.",
            "url": "https://arxiv.org/pdf/2303.16972",
            "publicationVenue": "arXiv preprint arXiv:2303.16972",
            "citation_count": 2
        },
        {
            "title": "Queer In AI: A Case Study in Community-Led Participatory AI",
            "authors": "ORGANIZERS OF QUEER IN AI, ANAELIA OVALLE, ARJUN SUBRAMONIAN, ASHWIN SINGH, CLAAS VOELCKER, DANICA J SUTHERLAND, DAVIDE LOCATELLI, EVA BREZNIK, FILIP KLUBI\u010cKA, HANG YUAN, HUAN ZHANG, JAIDEV SHRIRAM, KRUNO LEHMAN, LUCA SOLDAINI, MAARTEN SAP, MARC PETER DEISENROTH, MARIA LEONOR PACHECO, MARIA RYSKINA, MARTIN MUNDT, MILIND AGARWAL, NYX MCLEAN, PAN XU, A PRANAV, RAJ KORPAN, RUCHIRA RAY, SARAH MATHEW",
            "year": "2023",
            "abstract": "Unfortunately, there are many challenges to incorporating participatory approaches across top-down structures, such as corporations that operate within capitalism. Popular modes of participation within AI suffer from extractive and exploitative forms of community involvement or \u201cparticipation washing\u201d[140]. For example, a recent report [118] sheds light on how OpenAI used exploitative labor practices to make ChatGPT less toxic, subjecting Kenyan workers to psychologically distressing content 1 without sufficient provision for mental health support; Gray and Suri [66] also uncover many similar examples on the exploitative labor performed by minorities to power AI systems.",
            "url": "https://www.researchgate.net/profile/Avijit-Ghosh-5/publication/369655816_Queer_In_AI_A_Case_Study_in_Community-Led_Participatory_AI/links/64edf10845865f47bbbeb177/Queer-In-AI-A-Case-Study-in-Community-Led-Participatory-AI.pdf",
            "publicationVenue": "arXiv preprint arXiv:2303.16972",
            "citation_count": 0
        },
        {
            "title": "Riveter: Measuring Power and Social Dynamics Between Entities",
            "authors": "Maria Antoniak, Anjalie Field, Jimin Mun, Melanie Walsh, Lauren F Klein, Maarten Sap",
            "year": "2023",
            "abstract": "Riveter provides a complete easy-to-use pipeline for analyzing verb connotations associated with entities in text corpora. We prepopulate the package with connotation frames of sentiment, power, and agency, which have demonstrated usefulness for capturing social phenomena, such as gender bias, in a broad range of corpora. For decades, lexical frameworks have been foundational tools in computational social science, digital humanities, and natural language processing, facilitating multifaceted analysis of text corpora. But working with verb-centric lexica specifically requires natural language processing skills, reducing their accessibility to other researchers. By organizing the language processing pipeline, providing complete lexicon scores and visualizations for all entities in a corpus, and providing functionality for users to target specific research questions, Riveter greatly improves the accessibility of verb lexica and can facilitate a broad range of future research.",
            "url": "https://arxiv.org/pdf/2312.09536",
            "publicationVenue": "ACL demonstrations",
            "citation_count": 3
        }
    ],
    "Michael Shamos": [],
    "Emma Strubell": [
        {
            "title": "Understanding the effect of model compression on social bias in large language models",
            "authors": "Gustavo Gon\u00e7alves, Emma Strubell",
            "year": "2023",
            "abstract": "Large Language Models (LLMs) trained with self-supervision on vast corpora of web text fit to the social biases of that text. Without intervention, these social biases persist in the model's predictions in downstream tasks, leading to representational harm. Many strategies have been proposed to mitigate the effects of inappropriate social biases learned during pretraining. Simultaneously, methods for model compression have become increasingly popular to reduce the computational burden of LLMs. Despite the popularity and need for both approaches, little work has been done to explore the interplay between these two. We perform a carefully controlled study of the impact of model compression via quantization and knowledge distillation on measures of social bias in LLMs. Longer pretraining and larger models led to higher social bias, and quantization showed a regularizer effect with its best trade-off around 20% of the original pretraining time.",
            "url": "https://arxiv.org/html/2312.05662v2",
            "publicationVenue": "arXiv preprint arXiv:2312.05662",
            "citation_count": 3
        },
        {
            "title": "Power hungry processing: Watts driving the cost of ai deployment?",
            "authors": "Alexandra Sasha Luccioni, Yacine Jernite, Emma Strubell",
            "year": "2023",
            "abstract": "Recent years have seen a surge in the popularity of commercial AI products based on generative, multi-purpose AI systems promising a unified approach to building machine learning (ML) models into technology. However, this ambition of \"generality\" comes at a steep cost to the environment, given the amount of energy these systems require and the amount of carbon that they emit. In this work, we propose the first systematic comparison of the ongoing inference cost of various categories of ML systems, covering both task-specific (i.e. finetuned models that carry out a single task) and `general-purpose' models, (i.e. those trained for multiple tasks). We measure deployment cost as the amount of energy and carbon required to perform 1,000 inferences on representative benchmark dataset using these models. We find that multi-purpose, generative architectures are orders of magnitude more expensive than task-specific systems for a variety of tasks, even when controlling for the number of model parameters. We conclude with a discussion around the current trend of deploying multi-purpose generative ML systems, and caution that their utility should be more intentionally weighed against increased costs in terms of energy and emissions. All the data from our study can be accessed via an interactive demo to carry out further exploration and analysis.",
            "url": "https://arxiv.org/pdf/2311.16863.pdf?trk=public_post_comment-text",
            "publicationVenue": "arXiv preprint arXiv:2311.16863",
            "citation_count": 8
        },
        {
            "title": "Energy and Carbon Considerations of Fine-Tuning BERT",
            "authors": "Xiaorong Wang, Clara Na, Emma Strubell, Sorelle Friedler, Sasha Luccioni",
            "year": "2023",
            "abstract": "Despite the popularity of the `pre-train then fine-tune' paradigm in the NLP community, existing work quantifying energy costs and associated carbon emissions has largely focused on language model pre-training. Although a single pre-training run draws substantially more energy than fine-tuning, fine-tuning is performed more frequently by many more individual actors, and thus must be accounted for when considering the energy and carbon footprint of NLP. In order to better characterize the role of fine-tuning in the landscape of energy and carbon emissions in NLP, we perform a careful empirical study of the computational costs of fine-tuning across tasks, datasets, hardware infrastructure and measurement modalities. Our experimental results allow us to place fine-tuning energy and carbon costs into perspective with respect to pre-training and inference, and outline recommendations to NLP researchers and practitioners who wish to improve their fine-tuning energy efficiency.",
            "url": "https://arxiv.org/pdf/2311.10267",
            "publicationVenue": "arXiv preprint arXiv:2311.10267",
            "citation_count": 0
        },
        {
            "title": "Power Hungry Processing: Watts Driving the Cost of AI Deployment?",
            "authors": "Alexandra Sasha Luccioni, Yacine Jernite, Emma Strubell",
            "year": "2023",
            "abstract": "Recent years have seen a surge in the popularity of commercial AI products based on generative, multi-purpose AI systems promising a unified approach to building machine learning (ML) models into technology. However, this ambition of\" generality\" comes at a steep cost to the environment, given the amount of energy these systems require and the amount of carbon that they emit. In this work, we propose the first systematic comparison of the ongoing inference cost of various categories of ML systems, covering both task-specific (ie finetuned models that carry out a single task) andgeneral-purpose'models,(ie those trained for multiple tasks). We measure deployment cost as the amount of energy and carbon required to perform 1,000 inferences on representative benchmark dataset using these models. We find that multi-purpose, generative architectures are orders of magnitude more expensive than task-specific\u00a0\u2026",
            "url": null,
            "publicationVenue": "arXiv e-prints",
            "citation_count": 0
        },
        {
            "title": "Efficiency Pentathlon: A Standardized Benchmark for Efficiency Evaluation",
            "authors": "Hao Peng, Qingqing Cao, Jesse Dodge, Matthew E Peters, Jared Fernandez, Tom Sherborne, Kyle Lo, Sam Skjonsberg, Emma Strubell, Darrell Plessas, Iz Beltagy, Evan Pete Walsh, Noah A Smith, Hannaneh Hajishirzi",
            "year": "2023",
            "abstract": "Rising computational demands of modern natural language processing (NLP) systems have increased the barrier to entry for cutting-edge research while posing serious environmental concerns. Yet, progress on model efficiency has been impeded by practical challenges in model evaluation and comparison. For example, hardware is challenging to control due to disparate levels of accessibility across different institutions. Moreover, improvements in metrics such as FLOPs often fail to translate to progress in real-world applications. In response, we introduce Pentathlon, a benchmark for holistic and realistic evaluation of model efficiency. Pentathlon focuses on inference, which accounts for a majority of the compute in a model\u2019s lifecycle. It offers a strictly-controlled hardware platform, and is designed to mirror real-world applications scenarios. It incorporates a suite of metrics that target different aspects of efficiency, including latency, throughput, memory overhead, and energy consumption. Pentathlon also comes with a software library that can be seamlessly integrated into any codebase and enable evaluation. As a standardized and centralized evaluation platform, Pentathlon can drastically reduce the workload to make fair and reproducible efficiency comparisons. While initially focused on natural language processing (NLP) models, Pentathlon is designed to allow flexible extension to other fields. We envision Pentathlon will stimulate algorithmic innovations in building efficient models, and foster an increased awareness of the social and environmental implications in the development of future-generation NLP models.",
            "url": "https://openreview.net/pdf?id=Qyp3Rni2g1",
            "publicationVenue": null,
            "citation_count": 0
        },
        {
            "title": "Generate to discriminate: Expert routing for continual learning",
            "authors": "Yewon Byun, Sanket Vaibhav Mehta, Saurabh Garg, Emma Strubell, Bryan Wilder, Zachary Chase Lipton",
            "year": "2023",
            "abstract": "In many real-world settings, norms, regulations, or economic incentives permit the sharing of models but not data across environments.  Prominent examples arise in healthcare  due to regulatory concerns.  In this scenario, the practitioner wishes to adapt the model to each new environment but faces the danger of losing performance on previous environments due to the well-known problem of catastrophic forgetting.  In this paper, we propose Generate-to-Discriminate (G2D), a novel approach that leverages recent advancements in generative models to alleviate the catastrophic forgetting problem in continual learning.  Unlike previous approaches based on generative models  that primarily use synthetic data for training the label classifier, we use synthetic data to train a domain discriminator. Our method involves the following steps: For each domain, (i) fine-tune the classifier and adapt a  generative model to the current domain data; (ii) train a domain discriminator to distinguish synthetic samples  from past versus current domain data;  and (iii) during inference, route samples to the respective classifier. We compare G2D to an alternative approach, where we simply replay the generated synthetic data, and, surprisingly, we find that training a domain discriminator is significantly more effective than augmenting the training data with the same synthetic samples. We consistently outperform previous state-of-the-art domain-incremental learning algorithms  by up toandpoints across three standard  domain incremental learning benchmarks in the vision and language modalities, respectively, andpoints on a challenging real-world\u00a0\u2026",
            "url": "https://openreview.net/pdf?id=ntUmktUfZg",
            "publicationVenue": null,
            "citation_count": 1
        },
        {
            "title": "To Build Our Future, We Must Know Our Past: Contextualizing Paradigm Shifts in Natural Language Processing",
            "authors": "Sireesh Gururaja, Amanda Bertsch, Clara Na, David Gray Widder, Emma Strubell",
            "year": "2023",
            "abstract": "NLP is in a period of disruptive change that is impacting our methodologies, funding sources, and public perception. In this work, we seek to understand how to shape our future by better understanding our past. We study factors that shape NLP as a field, including culture, incentives, and infrastructure by conducting long-form interviews with 26 NLP researchers of varying seniority, research area, institution, and social identity. Our interviewees identify cyclical patterns in the field, as well as new shifts without historical parallel, including changes in benchmark culture and software infrastructure. We complement this discussion with quantitative analysis of citation, authorship, and language use in the ACL Anthology over time. We conclude by discussing shared visions, concerns, and hopes for the future of NLP. We hope that this study of our field's past and present can prompt informed discussion of our community's implicit norms and more deliberate action to consciously shape the future.",
            "url": "https://arxiv.org/pdf/2310.07715",
            "publicationVenue": "arXiv preprint arXiv:2310.07715",
            "citation_count": 1
        },
        {
            "title": "Efficiency pentathlon: A standardized arena for efficiency evaluation",
            "authors": "Hao Peng, Qingqing Cao, Jesse Dodge, Matthew E Peters, Jared Fernandez, Tom Sherborne, Kyle Lo, Sam Skjonsberg, Emma Strubell, Darrell Plessas, Iz Beltagy, Evan Pete Walsh, Noah A Smith, Hannaneh Hajishirzi",
            "year": "2023",
            "abstract": "Rising computational demands of modern natural language processing (NLP) systems have increased the barrier to entry for cutting-edge research while posing serious environmental concerns. Yet, progress on model efficiency has been impeded by practical challenges in model evaluation and comparison. For example, hardware is challenging to control due to disparate levels of accessibility across different institutions. Moreover, improvements in metrics such as FLOPs often fail to translate to progress in real-world applications. In response, we introduce Pentathlon, a benchmark for holistic and realistic evaluation of model efficiency. Pentathlon focuses on inference, which accounts for a majority of the compute in a model's lifecycle. It offers a strictly-controlled hardware platform, and is designed to mirror real-world applications scenarios. It incorporates a suite of metrics that target different aspects of efficiency, including latency, throughput, memory overhead, and energy consumption. Pentathlon also comes with a software library that can be seamlessly integrated into any codebase and enable evaluation. As a standardized and centralized evaluation platform, Pentathlon can drastically reduce the workload to make fair and reproducible efficiency comparisons. While initially focused on natural language processing (NLP) models, Pentathlon is designed to allow flexible extension to other fields. We envision Pentathlon will stimulate algorithmic innovations in building efficient models, and foster an increased awareness of the social and environmental implications in the development of future-generation NLP models.",
            "url": "https://arxiv.org/pdf/2307.09701",
            "publicationVenue": "arXiv preprint arXiv:2307.09701",
            "citation_count": 4
        },
        {
            "title": "Dissecting Efficient Architectures for Wake-Word Detection",
            "authors": "Cody Berger, Juncheng B Li, Yiyuan Li, Aaron Berger, Dmitri Berger, Karthik Ganesan, Emma Strubell, Florian Metze",
            "year": "2023",
            "abstract": "Wake-word detection models running on edge devices have stringent efficiency requirements. We observe that over-the-air test accuracy of models trained on parallel devices (GPU/TPU) usually degrades when deployed on edge devices using a CPU for over-the-air, real-time  Further, differing inference time when migrating between GPU and CPU varies across models. This drop is due to hardware latency and acoustic impulse response, while non-uniform growth of inference time results from models' varying exploitation of hardware acceleration. We compare five Convolutional Neural Network (CNN) architectures and one pure Transformer architecture, train them for wake-word detection on the Speech Commands dataset, and quantize two representative models. We seek to quantify their accuracy-efficiency tradeoffs to inform researchers and practicioners about the key components in models influencing this tradeoff.",
            "url": "https://openreview.net/pdf?id=lmaAcSViye",
            "publicationVenue": "Workshop on Efficient Systems for Foundation Models@ ICML2023",
            "citation_count": 0
        },
        {
            "title": "Efficient methods for natural language processing: A survey",
            "authors": "Marcos Treviso, Ji-Ung Lee, Tianchu Ji, Betty van Aken, Qingqing Cao, Manuel R Ciosici, Michael Hassid, Kenneth Heafield, Sara Hooker, Colin Raffel, Pedro H Martins, Andre FT Martins, Jessica Zosa Forde, Peter Milder, Edwin Simpson, Noam Slonim, Jesse Dodge, Emma Strubell, Niranjan Balasubramanian, Leon Derczynski, Iryna Gurevych, Roy Schwartz",
            "year": "2023",
            "abstract": "Recent work in natural language processing (NLP) has yielded appealing results from scaling model parameters and training data; however, using only scale to improve performance means that resource consumption also grows. Such resources include data, time, storage, or energy, all of which are naturally limited and unevenly distributed. This motivates research intoefficientmethods that require fewer resources to achieve similar results. This survey synthesizes and relates current methods and findings in efficient NLP. We aim to provide both guidance for conducting NLP under limited resources, and point towards promising research directions for developing more efficient methods.",
            "url": "https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00577/116725",
            "publicationVenue": "Transactions of the Association for Computational Linguistics",
            "citation_count": 45
        },
        {
            "title": "On the Interactions of Structural Constraints and Data Resources for Structured Prediction",
            "authors": "Zhisong Zhang, Emma Strubell, Eduard Hovy",
            "year": "2023",
            "abstract": "In this work, we provide an analysis on the interactions of the effectiveness of decoding with structural constraints and the amount of available training data for structured prediction tasks in NLP. Our exploration adopts a simple protocol that enforces constraints upon constraint-agnostic local models at testing time. With evaluations on three typical structured prediction tasks (named entity recognition, dependency parsing, and event argument extraction), we find that models trained with less data predict outputs with more structural violations in greedy decoding mode. Incorporating constraints provides consistent performance improvements and such benefits are larger in lower resource scenarios. Moreover, there are similar patterns with regard to the model sizes and more efficient models tend to enjoy more benefits. Finally, we also investigate settings with genre transfer and discover patterns that are related to domain discrepancies.",
            "url": "https://aclanthology.org/2023.sustainlp-1.10.pdf",
            "publicationVenue": "Proceedings of The Fourth Workshop on Simple and Efficient Natural Language Processing (SustaiNLP)",
            "citation_count": 0
        },
        {
            "title": "Annotating Mentions Alone Enables Efficient Domain Adaptation for Coreference Resolution",
            "authors": "Nupoor Gandhi, Anjalie Field, Emma Strubell",
            "year": "2023",
            "abstract": "Although recent neural models for coreference resolution have led to substantial improvements on benchmark datasets, it remains a challenge to successfully transfer these models to new target domains containing many out-of-vocabulary spans and requiring differing annotation schemes. Typical approaches involve continued training on annotated target-domain data, but obtaining annotations is costly and time-consuming. In this work, we show that adapting mention detection is the key component to successful domain adaptation of coreference models, rather than antecedent linking. We also show annotating mentions alone is nearly twice as fast as annotating full coreference chains. Based on these insights, we propose a method for efficiently adapting coreference models, which includes a high-precision mention detection objective and requires only mention annotations in the target domain. Extensive evaluation across three English coreference datasets: CoNLL-2012 (news/conversation), i2b2/VA (medical notes), and child welfare notes, reveals that our approach facilitates annotation-efficient transfer and results in a 7-14% improvement in average F1 without increasing annotator time.",
            "url": "https://aclanthology.org/2023.acl-long.588.pdf",
            "publicationVenue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
            "citation_count": 0
        },
        {
            "title": "Queer people are people first: Deconstructing sexual identity stereotypes in large language models",
            "authors": "Harnoor Dhingra, Preetiha Jayashanker, Sayali Moghe, Emma Strubell",
            "year": "2023",
            "abstract": "Large Language Models (LLMs) are trained primarily on minimally processed web text, which exhibits the same wide range of social biases held by the humans who created that content. Consequently, text generated by LLMs can inadvertently perpetuate stereotypes towards marginalized groups, like the LGBTQIA+ community. In this paper, we perform a comparative study of how LLMs generate text describing people with different sexual identities. Analyzing bias in the text generated by an LLM using regard score shows measurable bias against queer people. We then show that a post-hoc method based on chain-of-thought prompting using SHAP analysis can increase the regard of the sentence, representing a promising approach towards debiasing the output of LLMs in this setting.",
            "url": "https://www.researchgate.net/profile/Harnoor-Dhingra/publication/372074898_Queer_People_are_People_First_Deconstructing_Sexual_Identity_Stereotypes_in_Large_Language_Models/links/651cea4ed717ef1293c8fac9/Queer-People-are-People-First-Deconstructing-Sexual-Identity-Stereotypes-in-Large-Language-Models.pdf",
            "publicationVenue": "arXiv preprint arXiv:2307.00101",
            "citation_count": 12
        },
        {
            "title": "Surveying (dis) parities and concerns of compute hungry NLP research",
            "authors": "Ji-Ung Lee, Haritz Puerto, Betty van Aken, Yuki Arase, Jessica Zosa Forde, Leon Derczynski, Andreas R\u00fcckle, Iryna Gurevych, Roy Schwartz, Emma Strubell, Jesse Dodge",
            "year": "2023",
            "abstract": "Many recent improvements in NLP stem from the development and use of large pre-trained language models (PLMs) with billions of parameters. Large model sizes makes computational cost one of the main limiting factors for training and evaluating such models; and has raised severe concerns about the sustainability, reproducibility, and inclusiveness for researching PLMs. These concerns are often based on personal experiences and observations. However, there had not been any large-scale surveys that investigate them. In this work, we provide a first attempt to quantify these concerns regarding three topics, namely, environmental impact, equity, and impact on peer reviewing. By conducting a survey with 312 participants from the NLP community, we capture existing (dis)parities between different and within groups with respect to seniority, academia, and industry; and their impact on the peer reviewing process. For each topic, we provide an analysis and devise recommendations to mitigate found disparities, some of which already successfully implemented. Finally, we discuss additional concerns raised by many participants in free-text responses.",
            "url": "https://arxiv.org/pdf/2306.16900",
            "publicationVenue": "arXiv preprint arXiv:2306.16900",
            "citation_count": 1
        },
        {
            "title": "Large Language Model Distillation Doesn't Need a Teacher",
            "authors": "Ananya Harsh Jha, Dirk Groeneveld, Emma Strubell, Iz Beltagy",
            "year": "2023",
            "abstract": "Knowledge distillation trains a smaller student model to match the output distribution of a larger teacher to maximize the end-task performance under computational constraints. However, existing literature on language model distillation primarily focuses on compressing encoder-only models that are then specialized by task-specific supervised finetuning. We need to rethink this setup for more recent large language models with tens to hundreds of billions of parameters. Task-specific finetuning is impractical at this scale, and model performance is often measured using zero/few-shot prompting. Thus, in this work, we advocate for task-agnostic zero-shot evaluated distillation for large language models without access to end-task finetuning data. We propose a teacher-free task-agnostic distillation method, which uses a truncated version of the larger model for initialization, and continues pretraining this model using a language modeling objective. Our teacher-free method shines in a distillation regime where it is infeasible to fit both the student and teacher into the GPU memory. Despite its simplicity, our method can effectively reduce the model size by 50\\%, matching or outperforming the vanilla distillation method on perplexity and accuracy on 13 zero-shot end-tasks while being 1.5x computationally efficient.",
            "url": "https://arxiv.org/pdf/2305.14864",
            "publicationVenue": "arXiv preprint arXiv:2305.14864",
            "citation_count": 3
        },
        {
            "title": "Data-efficient Active Learning for Structured Prediction with Partial Annotation and Self-Training",
            "authors": "Zhisong Zhang, Emma Strubell, Eduard Hovy",
            "year": "2023",
            "abstract": "In this work we propose a pragmatic method that reduces the annotation cost for structured label spaces using active learning. Our approach leverages partial annotation, which reduces labeling costs for structured outputs by selecting only the most informative substructures for annotation. We also utilize selftraining to incorporate the current model's automatic predictions as pseudo-labels for unannotated sub-structures. A key challenge in effectively combining partial annotation with self-training to reduce annotation cost is determining which sub-structures to select to label. To address this challenge we adopt an error estimator to decide the partial selection ratio adaptively according to the current model's capability. In evaluations spanning four structured prediction tasks, we show that our combination of partial annotation and self-training using an adaptive selection ratio reduces annotation cost over strong full annotation baselines under a fair comparison scheme that takes reading time into consideration.",
            "url": "https://arxiv.org/pdf/2305.12634",
            "publicationVenue": "arXiv preprint arXiv:2305.12634",
            "citation_count": 1
        },
        {
            "title": "Regularizing Self-training for Unsupervised Domain Adaptation via Structural Constraints",
            "authors": "Rajshekhar Das, Jonathan Francis, Sanket Vaibhav Mehta, Jean Oh, Emma Strubell, Jose Moura",
            "year": "2023",
            "abstract": "Self-training based on pseudo-labels has emerged as a dominant approach for addressing conditional distribution shifts in unsupervised domain adaptation (UDA) for semantic segmentation problems. A notable drawback, however, is that this family of approaches is susceptible to erroneous pseudo labels that arise from confirmation biases in the source domain and that manifest as nuisance factors in the target domain. A possible source for this mismatch is the reliance on only photometric cues provided by RGB image inputs, which may ultimately lead to sub-optimal adaptation. To mitigate the effect of mismatched pseudo-labels, we propose to incorporate structural cues from auxiliary modalities, such as depth, to regularise conventional self-training objectives. Specifically, we introduce a contrastive pixel-level objectness constraint that pulls the pixel representations within a region of an object instance closer, while pushing those from different object categories apart. To obtain object regions consistent with the true underlying object, we extract information from both depth maps and RGB-images in the form of multimodal clustering. Crucially, the objectness constraint is agnostic to the ground-truth semantic labels and, hence, appropriate for unsupervised domain adaptation. In this work, we show that our regularizer significantly improves top performing self-training methods (by up topoints) in various UDA benchmarks for semantic segmentation. We include all code in the supplementary.",
            "url": "https://arxiv.org/pdf/2305.00131",
            "publicationVenue": "arXiv preprint arXiv:2305.00131",
            "citation_count": 1
        },
        {
            "title": "The Framework Tax: Disparities Between Inference Efficiency in Research and Deployment",
            "authors": "Jared Fernandez, Jacob Kahn, Clara Na, Yonatan Bisk, Emma Strubell",
            "year": "2023",
            "abstract": "Increased focus on the deployment of machine learning systems has led to rapid improvements in hardware accelerator performance and neural network model efficiency. However, the resulting reductions in floating point operations and increases in computational throughput of accelerators have not directly translated to improvements in real-world inference latency. We demonstrate that these discrepancies can be largely attributed to mis-alignments between model architectures and the capabilities of underlying hardware due to bottlenecks introduced by deep learning frameworks. We denote this phenomena as the \\textit{framework tax}, and observe that the disparity is growing as hardware speed increases over time. In this work, we examine this phenomena through a series of case studies analyzing the effects of model design decisions, framework paradigms, and hardware platforms on total model latency. Based on our findings, we provide actionable recommendations to ML researchers and practitioners aimed at narrowing the gap between efficient ML model research and practice.",
            "url": "https://arxiv.org/pdf/2302.06117",
            "publicationVenue": "arXiv preprint arXiv:2302.06117",
            "citation_count": 1
        },
        {
            "title": "Efficient and Equitable Natural Language Processing in the Age of Deep Learning (Dagstuhl Seminar 22232)",
            "authors": "Jesse Dodge, Iryna Gurevych, Roy Schwartz, Emma Strubell, Betty van Aken",
            "year": "2023",
            "abstract": "This report documents the program and the outcomes of Dagstuhl Seminar 22232\" Efficient and Equitable Natural Language Processing in the Age of Deep Learning\". Since 2012, the field of artificial intelligence (AI) has reported remarkable progress on a broad range of capabilities including object recognition, game playing, speech recognition, and machine translation. Much of this progress has been achieved by increasingly large and computationally intensive deep learning models: training costs for state-of-the-art deep learning models have increased 300,000 times between 2012 and 2018 [1]. Perhaps the epitome of this trend is the subfield of natural language processing (NLP) that over the past three years has experienced even sharper growth in model size and corresponding computational requirements in the word embedding approaches (eg ELMo, BERT, openGPT-2, Megatron-LM, T5, and GPT-3, one of the largest models ever trained with 175B dense parameters) that are now the basic building blocks of nearly all NLP models. Recent studies indicate that this trend is both environmentally unfriendly and prohibitively expensive, raising barriers to participation in NLP research [2, 3]. The goal of this seminar was to mitigate these concerns and promote equity of access in NLP.",
            "url": "https://drops.dagstuhl.de/opus/volltexte/2023/17454/pdf/dagrep_v012_i006_p014_22232.pdf",
            "publicationVenue": "Dagstuhl Reports",
            "citation_count": 2
        },
        {
            "title": "An empirical investigation of the role of pre-training in lifelong learning",
            "authors": "Sanket Vaibhav Mehta, Darshan Patil, Sarath Chandar, Emma Strubell",
            "year": "2023",
            "abstract": "The lifelong learning paradigm in machine learning is an attractive alternative to the more prominent isolated learning scheme not only due to its resemblance to biological learning but also its potential to reduce energy waste by obviating excessive model re-training. A key challenge to this paradigm is the phenomenon of catastrophic forgetting. With the increasing popularity and success of pre-trained models in machine learning, we pose the question: What role does pre-training play in lifelong learning, specifically with respect to catastrophic forgetting? We investigate existing methods in the context of large, pre-trained models and evaluate their performance on a variety of text and image classification tasks, including a large-scale study using a novel data set of 15 diverse NLP tasks. Across all settings, we observe that generic pre-training implicitly alleviates the effects of catastrophic forgetting when learning multiple tasks sequentially compared to randomly initialized models. We then further investigate why pre-training alleviates forgetting in this setting. We study this phenomenon by analyzing the loss landscape, finding that pre-trained weights appear to ease forgetting by leading to wider minima. Based on this insight, we propose jointly optimizing for current task loss and loss basin sharpness to explicitly encourage wider basins during sequential fine-tuning. We show that this optimization approach outperforms several state-of-the-art task-sequential continual learning algorithms across multiple settings, occasionally even without retaining a memory that scales in size with the number of tasks.",
            "url": "https://www.jmlr.org/papers/volume24/22-0496/22-0496.pdf",
            "publicationVenue": "Journal of Machine Learning Research",
            "citation_count": 58
        }
    ],
    "Alexander Waibel": [
        {
            "title": "Providing automated personal privacy during virtual meetings",
            "authors": null,
            "year": "2023",
            "abstract": "Systems and methods for providing automated personal privacy during virtual meetings are provided herein. The method may include establishing, by a video conference provider, a video conference having a plurality of participants. The method may also include receiving, from a first client device associated with one of the plurality of participants, a first audio stream and a first video stream, and recording responsive to an indication from one of the plurality of participants, one or more audio or video streams within a recording. The method may include receiving, from the first client device, a personal privacy request. In response to the personal privacy request, the method may include modifying, by the video conference provider, at least one of the first audio stream or the first video stream in the recording and storing the least one of the first audio stream or the first video stream as modified to the recording.",
            "url": null,
            "publicationVenue": null,
            "citation_count": 0
        },
        {
            "title": "Delta models for providing privatized speech-to-text during virtual meetings",
            "authors": null,
            "year": "2023",
            "abstract": "Provided herein are systems and methods for delta models for providing privatized speech-to-text during virtual meetings. In one embodiment, a system may include a non-transitory computer-readable medium; a communications interface; and a processor. The processor may be configured to execute processor-executable instructions to: join a virtual meeting. Each participant in the virtual meeting may exchange audio streams with other participants in the virtual meeting. The instructions may include receiving, from a video conference provider, a local model for speech recognition. The local model may be a copy of a centralized model. The instructions may include performing speech recognition using the local model on the audio streams. Performing speech recognition may include identifying audio feature data within the one or more audio streams, identifying, based on a vocabulary database, user-specific\u00a0\u2026",
            "url": null,
            "publicationVenue": null,
            "citation_count": 0
        },
        {
            "title": "Providing off-the-record functionality during virtual meetings",
            "authors": null,
            "year": "2023",
            "abstract": "A system for providing off-the-record functionality is provided herein. The system may include a processor configured to execute processor-executable instructions stored in non-transitory computer-readable medium to establish a video conference having a plurality of participants, each participant of the plurality of participants exchanging a plurality of audio or video streams via the video conference. The processor may also be configured to receive, from a first client device associated with one of the plurality of participants, a first audio stream or a first video stream of the plurality of audio or video streams, and record the plurality of audio or video streams within a recording. The processor may also be configured to receive an off-the-record request to begin an off-the-record time period, and in response to the off-the-record request, prevent at least one of the first audio stream or the first video stream from being included\u00a0\u2026",
            "url": null,
            "publicationVenue": null,
            "citation_count": 0
        },
        {
            "title": "Providing instant processing of virtual meeting recordings",
            "authors": null,
            "year": "2023",
            "abstract": "Systems and methods for providing instant processing of virtual meeting recordings are provided. A system may include a non-transitory computer-readable medium; a communications interface; and a processor configured to execute processor-executable instructions stored in non-transitory computer-readable medium to: establish a virtual meeting having a plurality of participants, each participant of the plurality of participants exchanging audio or video streams via the virtual meeting; record, responsive to an indication from one of the plurality of participants, the audio or video streams from the plurality of participants; generate a recording of one or more audio or video tracks corresponding to the audio or video streams exchanged during the virtual meeting; receive, from a first client device, a request to customize the recording, the request comprising an identification of customizable content and a modification\u00a0\u2026",
            "url": null,
            "publicationVenue": null,
            "citation_count": 0
        },
        {
            "title": "Enforcing consent requirements for sharing virtual meeting recordings",
            "authors": null,
            "year": "2023",
            "abstract": "Systems and methods for enforcing consent requirements for sharing virtual meeting recordings are provided herein. In an example, a method may include receiving, from a first client device, a recording privacy request associated with a virtual meeting, and receiving, from a second client device, a request to share a recording of the virtual meeting with one or more recipients. The method may also include modifying, by a video conference provider, at least one of a first audio stream or a first video stream associated with the first client device in the recording based on the recording privacy request, and generating, by the video conference provider, a privatized recording based on the modification of at least one of the first audio stream or the first video stream. The method may also include transmitting, by the video conference provider, the privatized recording to the one or more recipients.",
            "url": null,
            "publicationVenue": null,
            "citation_count": 0
        },
        {
            "title": "Modular Design of a Front-End and Back-End Speech-to-Speech Translation Application for Psychiatric Treatment of Refugees",
            "authors": "Enes Yavuz Ugan, Mohammed Mediani, Omar Al Jawabra, Aya Khader, Yining Liu, Alexander Waibel",
            "year": "2023",
            "abstract": "One of the inevitable impacts happening in areas with political conflicts is the significant influx of displaced individuals. The psychological consequences on individuals enduring such events are profound. Therefore, the imperative of providing adequate mental health care to refugees coming from conflict areas becomes apparent. However, providing this necessary care faces two obstacles. On the one hand, not all this target population is expected to have an acceptable level of proficiency of the hosting country\u2019s local language. On the other hand, finding enough number of suitable interpreters is a very challenging task. Moreover, even when the availability of the human interpreters is no problem, the refugees may hesitate to share their experiences with interpreters due to the associated stigma. To address these challenges and enhance mental health care for refugees, we propose the design of a modular front\u00a0\u2026",
            "url": "https://isl.anthropomatik.kit.edu/downloads/Modular_Design_of_a_Front-End_and_Back-End_Speech-to-Speech_Translation_Application_for_Psychiatric_Treatment_of_Refugees.pdf",
            "publicationVenue": "2023 IEEE Global Humanitarian Technology Conference (GHTC)",
            "citation_count": 0
        },
        {
            "title": "Comparison of Error Correction and Extraction Approaches",
            "authors": "Stefan Constantin, Alex Waibel",
            "year": "2023",
            "abstract": "We compare different approaches for error correction detection and error correction. For the error correction detection task, the inputs are the last two utterances of a user and the output is whether there is an error correction in the last utterance. The error correction task gets the same inputs, but the output is the correction of the second last utterance according to the error correction in the last utterance and the extracted pairs of reparandum and repair entity. There are two advantages when using the compared approaches as utility component for a dialog system. It can be avoided to collect corrections for every new domain, and the extraction of the reparandum and repair pairs offers the possibility to learn from them. As benchmark for our comparison, we use an adapted version of the EPIC-KITCHENS-100 dataset. The best approach, a pipeline approach with a fine-tuned sequence labeling BERT model for error\u00a0\u2026",
            "url": null,
            "publicationVenue": "Practical Solutions for Diverse Real-World NLP Applications",
            "citation_count": 0
        },
        {
            "title": "Incremental Blockwise Beam Search for Simultaneous Speech Translation with Controllable Quality-Latency Tradeoff",
            "authors": "Peter Pol\u00e1k, Brian Yan, Shinji Watanabe, Alex Waibel, Ond\u0159ej Bojar",
            "year": "2023",
            "abstract": "Blockwise self-attentional encoder models have recently emerged as one promising end-to-end approach to simultaneous speech translation. These models employ a blockwise beam search with hypothesis reliability scoring to determine when to wait for more input speech before translating further. However, this method maintains multiple hypotheses until the entire speech input is consumed -- this scheme cannot directly show a single \\textit{incremental} translation to users. Further, this method lacks mechanisms for \\textit{controlling} the quality vs. latency tradeoff. We propose a modified incremental blockwise beam search incorporating local agreement or hold-policies for quality-latency control. We apply our framework to models trained for online or offline translation and demonstrate that both types can be effectively used in online mode. Experimental results on MuST-C show 0.6-3.6 BLEU improvement without changing latency or 0.8-1.4 s latency improvement without changing quality.",
            "url": "https://arxiv.org/pdf/2309.11379",
            "publicationVenue": "arXiv preprint arXiv:2309.11379",
            "citation_count": 2
        },
        {
            "title": "Incremental Learning of Humanoid Robot Behavior from Natural Interaction and Large Language Models",
            "authors": "Leonard B\u00e4rmann, Rainer Kartmann, Fabian Peller-Konrad, Alex Waibel, Tamim Asfour",
            "year": "2023",
            "abstract": "Natural-language dialog is key for intuitive human-robot interaction. It can be used not only to express humans' intents, but also to communicate instructions for improvement if a robot does not understand a command correctly. Of great importance is to endow robots with the ability to learn from such interaction experience in an incremental way to allow them to improve their behaviors or avoid mistakes in the future. In this paper, we propose a system to achieve incremental learning of complex behavior from natural interaction, and demonstrate its implementation on a humanoid robot. Building on recent advances, we present a system that deploys Large Language Models (LLMs) for high-level orchestration of the robot's behavior, based on the idea of enabling the LLM to generate Python statements in an interactive console to invoke both robot perception and action. The interaction loop is closed by feeding back human instructions, environment observations, and execution results to the LLM, thus informing the generation of the next statement. Specifically, we introduce incremental prompt learning, which enables the system to interactively learn from its mistakes. For that purpose, the LLM can call another LLM responsible for code-level improvements of the current interaction based on human feedback. The improved interaction is then saved in the robot's memory, and thus retrieved on similar requests. We integrate the system in the robot cognitive architecture of the humanoid robot ARMAR-6 and evaluate our methods both quantitatively (in simulation) and qualitatively (in simulation and real-world) by demonstrating generalized incrementally\u00a0\u2026",
            "url": "https://arxiv.org/pdf/2309.04316",
            "publicationVenue": "arXiv preprint arXiv:2309.04316",
            "citation_count": 2
        },
        {
            "title": "Convoifilter: A case study of doing cocktail party speech recognition",
            "authors": "Thai-Binh Nguyen, Alexander Waibel",
            "year": "2023",
            "abstract": "This paper presents an end-to-end model designed to improve automatic speech recognition (ASR) for a particular speaker in a crowded, noisy environment. The model utilizes a single-channel speech enhancement module that isolates the speaker's voice from background noise, along with an ASR module. Through this approach, the model is able to decrease the word error rate (WER) of ASR from 80% to 26.4%. Typically, these two components are adjusted independently due to variations in data requirements. However, speech enhancement can create anomalies that decrease ASR efficiency. By implementing a joint fine-tuning strategy, the model can reduce the WER from 26.4% in separate tuning to 14.5% in joint tuning.",
            "url": "https://arxiv.org/pdf/2308.11380",
            "publicationVenue": "arXiv preprint arXiv:2308.11380",
            "citation_count": 0
        },
        {
            "title": "End-to-End Evaluation for Low-Latency Simultaneous Speech Translation",
            "authors": "Christian Huber, Tu Anh Dinh, Carlos Mullov, Ngoc Quan Pham, Thai Binh Nguyen, Fabian Retkowski, Stefan Constantin, Enes Yavuz Ugan, Danni Liu, Zhaolin Li, Sai Koneru, Jan Niehues, Alexander Waibel",
            "year": "2023",
            "abstract": "The challenge of low-latency speech translation has recently draw significant interest in the research community as shown by several publications and shared tasks. Therefore, it is essential to evaluate these different approaches in realistic scenarios. However, currently only specific aspects of the systems are evaluated and often it is not possible to compare different approaches. In this work, we propose the first framework to perform and evaluate the various aspects of low-latency speech translation under realistic conditions. The evaluation is carried out in an end-to-end fashion. This includes the segmentation of the audio as well as the run-time of the different components. Secondly, we compare different approaches to low-latency speech translation using this framework. We evaluate models with the option to revise the output as well as methods with fixed output. Furthermore, we directly compare state-of-the-art cascaded as well as end-to-end systems. Finally, the framework allows to automatically evaluate the translation quality as well as latency and also provides a web interface to show the low-latency model outputs to the user.",
            "url": "https://arxiv.org/pdf/2308.03415",
            "publicationVenue": "arXiv preprint arXiv:2308.03415",
            "citation_count": 0
        },
        {
            "title": "Plug the Leaks: Advancing Audio-driven Talking Face Generation by Preventing Unintended Information Flow",
            "authors": "Dogucan Yaman, Fevziye Irem Eyiokur, Leonard B\u00e4rmann, Hazim Kemal Ekenel, Alexander Waibel",
            "year": "2023",
            "abstract": "Audio-driven talking face generation is the task of creating a lip-synchronized, realistic face video from given audio and reference frames. This involves two major challenges: overall visual quality of generated images on the one hand, and audio-visual synchronization of the mouth part on the other hand. In this paper, we start by identifying several problematic aspects of synchronization methods in recent audio-driven talking face generation approaches. Specifically, this involves unintended flow of lip and pose information from the reference to the generated image, as well as instabilities during model training. Subsequently, we propose various techniques for obviating these issues: First, a silent-lip reference image generator prevents leaking of lips from the reference to the generated image. Second, an adaptive triplet loss handles the pose leaking problem. Finally, we propose a stabilized formulation of synchronization loss, circumventing aforementioned training instabilities while additionally further alleviating the lip leaking issue. Combining the individual improvements, we present state-of-the art performance on LRS2 and LRW in both synchronization and visual quality. We further validate our design in various ablation experiments, confirming the individual contributions as well as their complementary effects.",
            "url": "https://arxiv.org/pdf/2307.09368",
            "publicationVenue": "arXiv preprint arXiv:2307.09368",
            "citation_count": 0
        },
        {
            "title": "Towards Efficient Simultaneous Speech Translation: CUNI-KIT System for Simultaneous Track at IWSLT 2023",
            "authors": "Peter Pol\u00e1k, Danni Liu, Ngoc-Quan Pham, Jan Niehues, Alex Waibel, Ond\u0159ej Bojar",
            "year": "2023",
            "abstract": "In this paper, we describe our submission to the Simultaneous Track at IWSLT 2023. This year, we continue with the successful setup from the last year, however, we adopt the latest methods that further improve the translation quality. Additionally, we propose a novel online policy for attentional encoder-decoder models. The policy prevents the model to generate translation beyond the current speech input by using an auxiliary CTC output layer. We show that the proposed simultaneous policy can be applied to both streaming blockwise models and offline encoder-decoder models. We observe significant improvements in quality (up to 1.1 BLEU) and the computational footprint (up to 45% relative RTF).",
            "url": "https://aclanthology.org/2023.iwslt-1.37.pdf",
            "publicationVenue": "Proceedings of the 20th International Conference on Spoken Language Translation (IWSLT 2023)",
            "citation_count": 2
        },
        {
            "title": "Incremental post-editing and learning in speech transcription and translation services",
            "authors": null,
            "year": "2023",
            "abstract": "Computer systems and computer-implemented methods provide for interactive and incremental post-editing of real-time speech transcription and translation. A first component is automatic identification of potentially problematic regions in the output (eg, transcription or translation) that are either likely to be technically processed badly or risky in terms of their content or expression. A second component is intelligent, efficient interfaces that permit multiple editors to correct system output concurrently, collaboratively, efficiently, and simultaneously, so that corrections can be seamlessly inserted and become part of a running presentation. A third component is incremental learning and adaptation that allows the system to use the human corrective feedback to deliver instantaneous improvement of system behavior down-stream. A fourth component is transfer learning to transfer short-term learning into long term learning if\u00a0\u2026",
            "url": null,
            "publicationVenue": null,
            "citation_count": 0
        },
        {
            "title": "KIT's Multilingual Speech Translation System for IWSLT 2023",
            "authors": "Danni Liu, Thai Binh Nguyen, Sai Koneru, Enes Yavuz Ugan, Ngoc-Quan Pham, Tuan-Nam Nguyen, Tu Anh Dinh, Carlos Mullov, Alexander Waibel, Jan Niehues",
            "year": "2023",
            "abstract": "Many existing speech translation benchmarks focus on native-English speech in high-quality recording conditions, which often do not match the conditions in real-life use-cases. In this paper, we describe our speech translation system for the multilingual track of IWSLT 2023, which focuses on the translation of scientific conference talks. The test condition features accented input speech and terminology-dense contents. The tasks requires translation into 10 languages of varying amounts of resources. In absence of training data from the target domain, we use a retrieval-based approach (kNN-MT) for effective adaptation (+0.8 BLEU for speech translation). We also use adapters to easily integrate incremental training data from data augmentation, and show that it matches the performance of re-training. We observe that cascaded systems are more easily adaptable towards specific target domains, due to their separate modules. Our cascaded speech system substantially outperforms its end-to-end counterpart on scientific talk translation, although their performance remains similar on TED talks.",
            "url": "https://arxiv.org/pdf/2306.05320",
            "publicationVenue": "arXiv preprint arXiv:2306.05320",
            "citation_count": 2
        },
        {
            "title": "Voice agent for sidebars during virtual meetings",
            "authors": null,
            "year": "2023",
            "abstract": "Systems and methods for providing a voice agent for sidebars during virtual meetings are provided. In an Example, a system including a non-transitory computer-readable medium, a communications interface, and a processor is provided. The processor configured to execute processor-executable instructions stored in the non-transitory computer-readable medium to: establish a video conference, receive, from a first client device, a request for a sidebar meeting, and transmit to the first client device: a first set of audio and video streams corresponding to a main meeting, and a second set of audio and video streams corresponding to the sidebar meeting. The processor may be configured to identify, by a voice agent, an attention cue in an audio stream from the first set of audio and video streams, and generate, by the voice agent, an alert based on the attention cue identified in the audio stream.",
            "url": null,
            "publicationVenue": null,
            "citation_count": 0
        },
        {
            "title": "AdapITN: A Fast, Reliable, and Dynamic Adaptive Inverse Text Normalization",
            "authors": "Thai-Binh Nguyen, Quang Minh Nguyen, Quoc Truong Do, Chi Mai Luong, Alexander Waibel",
            "year": "2023",
            "abstract": "Inverse text normalization (ITN) is the task that transforms text in spoken-form into written-form. While automatic speech recognition (ASR) produces text in spoken-form, human and natural language understanding systems prefer to consume text in written-form. ITN generally deals with semiotic phrases (e.g., numbers, date, time). However, lack of studies to deal with phonetization phrases, which is ASR\u2019s output when it handles unseen data (e.g., foreign-named entities, domain names), although these exist in the same form in the spoken-form text. The reason is that phonetization phrases are infinite patterns and language-dependent. In this study, we introduce a novel end2end model that can handle both semiotic phrases (SEP) and phonetization phrases (PHP), named AdapITN. We call it \"Adap\" because it allows for handling unseen PHP. The model performs only when necessary by providing a mechanism to\u00a0\u2026",
            "url": "https://www.researchgate.net/profile/Thai-Binh-Nguyen-4/publication/371349109_AdapITN_A_Fast_Reliable_and_Dynamic_Adaptive_Inverse_Text_Normalization/links/64874453d702370600ef33ab/AdapITN-A-Fast-Reliable-and-Dynamic-Adaptive-Inverse-Text-Normalization.pdf",
            "publicationVenue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
            "citation_count": 1
        },
        {
            "title": "SYNTACC: Synthesizing Multi-Accent Speech By Weight Factorization",
            "authors": "Tuan-Nam Nguyen, Ngoc-Quan Pham, Alexander Waibel",
            "year": "2023",
            "abstract": "Conventional multi-speaker text-to-speech synthesis (TTS) is known to be capable of synthesizing speech for multiple voices, yet it cannot generate speech in different accents. This limitation has motivated us to develop SYNTACC (Synthesizing speech with accents) which adapts conventional multi-speaker TTS to produce multi-accent speech. Our method uses the YourTTS model and involves a novel multi-accent training mechanism. The method works by decomposing each weight matrix into a shared component and an accent-dependent component, with the former being initialized by the pretrained multi-speaker TTS model and the latter being factorized into vectors using rank-1 matrices to reduce the number of training parameters per accent. This weight factorization method proves to be effective in fine-tuning the SYNTACC on multi-accent data sets in a low-resource condition. Our SYNTACC model\u00a0\u2026",
            "url": null,
            "publicationVenue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
            "citation_count": 0
        },
        {
            "title": "Face-Dubbing++: Lip-synchronous, voice preserving translation of videos",
            "authors": "Alexander Waibel, Moritz Behr, Dogucan Yaman, Fevziye Irem Eyiokur, Tuan-Nam Nguyen, Carlos Mullov, Mehmet Arif Demirtas, Alperen Kantarci, Stefan Constantin, Hazim Kemal Ekenel",
            "year": "2023",
            "abstract": "In this paper, we propose a neural end-to-end system for voice preserving and lip-synchronous video translation. The system is designed to combine multiple component models and produces a video of the original speaker speaking in the target language that is lip-synchronous with the target speech, yet maintains emphases in speech, voice characteristics, and face video of the original speaker. The result is a video of a speaker speaking in another language without actually knowing it. For the evaluation, we present a user study of the complete system and separate evaluations of the single components. Since there is no available dataset to evaluate our whole system, we collect a test set to evaluate our system. The results indicate that our system is able to generate convincing videos of the original speaker speaking the target language while preserving the original speaker\u2019s characteristics.",
            "url": "https://arxiv.org/pdf/2206.04523",
            "publicationVenue": "2023 IEEE International Conference on Acoustics, Speech, and Signal Processing Workshops (ICASSPW)",
            "citation_count": 4
        },
        {
            "title": "Unconstrained face mask and face-hand interaction datasets: building a computer vision system to help prevent the transmission of COVID-19",
            "authors": "Fevziye Irem Eyiokur, Haz\u0131m Kemal Ekenel, Alexander Waibel",
            "year": "2023",
            "abstract": "Health organizations advise social distancing, wearing face mask, and avoiding touching face to prevent the spread of coronavirus. Based on these protective measures, we developed a computer vision system to help prevent the transmission of COVID-19. Specifically, the developed system performs face mask detection, face-hand interaction detection, and measures social distance. To train and evaluate the developed system, we collected and annotated images that represent face mask usage and face-hand interaction in the real world. Besides assessing the performance of the developed system on our own datasets, we also tested it on existing datasets in the literature without performing any adaptation on them. In addition, we proposed a module to track social distance between people. Experimental results indicate that our datasets represent the real-world\u2019s diversity well. The proposed system achieved very\u00a0\u2026",
            "url": "https://link.springer.com/article/10.1007/s11760-022-02308-x",
            "publicationVenue": "Signal, Image and Video Processing",
            "citation_count": 37
        },
        {
            "title": "Train Global, Tailor Local: Minimalist Multilingual Translation into Endangered Languages",
            "authors": "Zhong Zhou, Jan Niehues, Alex Waibel",
            "year": "2023",
            "abstract": "In many humanitarian scenarios, translation into severely low resource languages often does not require a universal translation engine, but a dedicated text-specific translation engine. For example, healthcare records, hygienic procedures, government communication, emergency procedures and religious texts are all limited texts. While generic translation engines for all languages do not exist, translation of multilingually known limited texts into new, endangered languages may be possible and reduce human translation effort. We attempt to leverage translation resources from many rich resource languages to efficiently produce best possible translation quality for a well known text, which is available in multiple languages, in a new, severely low resource language. We examine two approaches: 1. best selection of seed sentences to jump start translations in a new language in view of best generalization to the remainder of a larger targeted text(s), and 2. we adapt large general multilingual translation engines from many other languages to focus on a specific text in a new, unknown language. We find that adapting large pretrained multilingual models to the domain/text first and then to the severely low resource language works best. If we also select a best set of seed sentences, we can improve average chrF performance on new test languages from a baseline of 21.9 to 50.7, while reducing the number of seed sentences to only around 1,000 in the new, unknown language.",
            "url": "https://arxiv.org/pdf/2305.03873",
            "publicationVenue": "arXiv preprint arXiv:2305.03873",
            "citation_count": 1
        },
        {
            "title": "A survey on computer vision based human analysis in the COVID-19 era",
            "authors": "Fevziye Irem Eyiokur, Alperen Kantarc\u0131, Mustafa Ekrem Erak\u0131n, Naser Damer, Ferda Ofli, Muhammad Imran, Janez Kri\u017eaj, Albert Ali Salah, Alexander Waibel, Vitomir \u0160truc, Haz\u0131m Kemal Ekenel",
            "year": "2023",
            "abstract": "The emergence of COVID-19 has had a global and profound impact, not only on society as a whole, but also on the lives of individuals. Various prevention measures were introduced around the world to limit the transmission of the disease, including face masks, mandates for social distancing and regular disinfection in public spaces, and the use of screening applications. These developments also triggered the need for novel and improved computer vision techniques capable of (i) providing support to the prevention measures through an automated analysis of visual data, on the one hand, and (ii) facilitating normal operation of existing vision-based services, such as biometric authentication schemes, on the other. Especially important here, are computer vision techniques that focus on the analysis of people and faces in visual data and have been affected the most by the partial occlusions introduced by the\u00a0\u2026",
            "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9755265/",
            "publicationVenue": "Image and Vision Computing",
            "citation_count": 8
        },
        {
            "title": "Error-correction and extraction in request dialogs",
            "authors": null,
            "year": "2023",
            "abstract": "A system comprises a machine that is configured to act upon requests from a user and sensing means for sensing an operational-mode dialog stream from the user for the machine. The system also comprises a computing system that is configured to train a neural network through machine learning to output, for each training example in a training dialog stream dataset, a corrected request for the machine. The computing system is also configure to, in an operational mode, using the trained neural network, generate a corrected, operational-mode request for the machine based on the operational-mode dialog stream from the user for the machine, wherein the operational-mode dialog stream is sensed by the sensing means.",
            "url": null,
            "publicationVenue": null,
            "citation_count": 0
        },
        {
            "title": "Beyond Quantity: Research with Subsymbolic AI",
            "authors": "Andreas Sudmann, Anna Echterh\u00f6lter, Markus Ramsauer, Fabian Retkowski, Jens Schr\u00f6ter, Alexander Waibel",
            "year": "2023",
            "abstract": "How do artificial neural networks and other forms of artificial intelligence interfere with methods and practices in the sciences? Which interdisciplinary epistemological challenges arise when we think about the use of AI beyond its dependency on big data? Not only the natural sciences, but also the social sciences and the humanities seem to be increasingly affected by current approaches of subsymbolic AI, which master problems of quality (fuzziness, uncertainty) in a hitherto unknown way. But what are the conditions, implications, and effects of these (potential) epistemic transformations and how must research on AI be configured to address them adequately?",
            "url": "https://library.oapen.org/bitstream/handle/20.500.12657/85723/9783839467664.pdf?sequence=1",
            "publicationVenue": "transcript Verlag",
            "citation_count": 0
        },
        {
            "title": "Multimodal Error Correction with Natural Language and Pointing Gestures",
            "authors": "Stefan Constantin, Fevziye Irem Eyiokur, Dogucan Yaman, Leonard B\u00e4rmann, Alex Waibel",
            "year": "2023",
            "abstract": "Error correction is crucial in human-computer interaction, as it can provide supervision for incrementally learning artificial intelligence. If a system maps entities like objects or persons with unknown class to inappropriate existing classes, or misrecognizes entities from known classes when there is too high train-test discrepancy, error correction is a natural way for a user to improve the system. Provided an agent with visual perception, if such entity is in the view of the system, pointing gestures can dramatically simplify the error correction. Therefore, we propose a modularized system for multimodal error correction using natural language and pointing gestures. First, pointing line generation and region proposal detects whether there is a pointing gesture, and if yes, which candidate objects (ie RoIs) are on the pointing line. Second, these RoIs (if any) and the user's utterances are fed into a VL-T5 network to extract and link both the class name and the corresponding RoI of the referred entity, or to output that there is no error correction. In the latter case, the utterances can be passed to a downstream component for Natural Language Understanding. We use additional, challenging annotations for an existing real-world pointing gesture dataset to evaluate our proposed system. Furthermore, we demonstrate our approach by integrating it on a real-world steerable laser pointer robot, enabling interactive multimodal error correction and thus incremental learning of new objects.",
            "url": "https://openaccess.thecvf.com/content/ICCV2023W/ACVR/papers/Constantin_Multimodal_Error_Correction_with_Natural_Language_and_Pointing_Gestures_ICCVW_2023_paper.pdf",
            "publicationVenue": "Proceedings of the IEEE/CVF International Conference on Computer Vision",
            "citation_count": 0
        },
        {
            "title": "Findings of the IWSLT 2023 evaluation campaign",
            "authors": "Milind Agarwal, Sweta Agarwal, Antonios Anastasopoulos, Luisa Bentivogli, Ond\u0159ej Bojar, Claudia Borg, Marine Carpuat, Roldano Cattoni, Mauro Cettolo, Mingda Chen, William Chen, Khalid Choukri, Alexandra Chronopoulou, Anna Currey, Thierry Declerck, Qianqian Dong, Kevin Duh, Yannick Est\u00e8ve, Marcello Federico",
            "year": "2023",
            "abstract": "This paper reports on the shared tasks organized by the 20th IWSLT Conference. The shared tasks address 9 scientific challenges in spoken language translation: simultaneous and offline translation, automatic subtitling and dubbing, speech-to-speech translation, multilingual, dialect and low-resource speech translation, and formality control. The shared tasks attracted a total of 38 submissions by 31 teams. The growing interest towards spoken language translation is also witnessed by the constantly increasing number of shared task organizers and contributors to the overview paper, almost evenly distributed across industry and academia.",
            "url": "https://www.um.edu.mt/library/oar/bitstream/123456789/114765/1/2023.iwslt-1.1v2.pdf",
            "publicationVenue": "Association for Computational Linguistics",
            "citation_count": 21
        }
    ],
    "Shinji Watanabe": [
        {
            "title": "LV-CTC: Non-Autoregressive ASR With CTC and Latent Variable Models",
            "authors": "Yuya Fujita, Shinji Watanabe, Xuankai Chang, Takashi Maekaku",
            "year": "2023",
            "abstract": "Non-autoregressive (NAR) models for automatic speech recognition (ASR) aim to achieve high accuracy and fast inference by simplifying the autoregressive (AR) generation process of conventional models. Connectionist temporal classification (CTC) is one of the key techniques used in NAR ASR models. In this paper, we propose a new model combining CTC and a latent variable model, which is one of the state-of-the-art models in the neural machine translation research field. A new neural network architecture and formulation specialized for ASR application are introduced. In the proposed model, CTC alignment is assumed to be dependent on the latent variables that are expected to capture dependencies between tokens. Experimental results on a 100 hours subset of Librispeech corpus showed the best recognition accuracy among CTC-based NAR models. On the TED-LIUM2 corpus, the best recognition\u00a0\u2026",
            "url": null,
            "publicationVenue": "2023 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)",
            "citation_count": 0
        },
        {
            "title": "Summarize While Translating: Universal Model With Parallel Decoding for Summarization and Translation",
            "authors": "Takatomo Kano, Atsunori Ogawa, Marc Delcroix, Kohei Matsuura, Takanori Ashihara, William Chen, Shinji Watanabe",
            "year": "2023",
            "abstract": "Recently, multi-decoder and universal models have attracted increased interest in speech and language processing as they allow learning common representations across tasks. These models learn a common representation by sharing a part of or all network parameters. Moreover, such a universal model can handle tasks unseen during training (zero-shot tasks). However, these models do not fully exploit inter-dependencies between tasks during decoding since they usually perform decoding for each task independently. In this paper, we propose to address this issue by extending the universal model to perform multi-task parallel decoding with a cross-attention module between decoders to capture task inter-dependencies explicitly. We also introduce a novel multi-stream beam search algorithm to allow such parallel decoding. We test our proposed model on multi-lingual (English and Portuguese) text/speech\u00a0\u2026",
            "url": null,
            "publicationVenue": "2023 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)",
            "citation_count": 0
        },
        {
            "title": "Yodas: Youtube-Oriented Dataset for Audio and Speech",
            "authors": "Xinjian Li, Shinnosuke Takamichi, Takaaki Saeki, William Chen, Sayaka Shiota, Shinji Watanabe",
            "year": "2023",
            "abstract": "In this study, we introduce YODAS (YouTube-Oriented Dataset for Audio and Speech), a large-scale, multilingual dataset comprising currently over 500k hours of speech data in more than 100 languages, sourced from both labeled and unlabeled YouTube speech datasets. The labeled subsets, including manual or automatic subtitles, facilitate supervised model training. Conversely, the unlabeled subsets are apt for self-supervised learning applications. YODAS is distinctive as the first publicly available dataset of its scale, and it will be distributed under a Creative Commons license. We introduce the collection methodology utilized for YODAS, which contributes to the large-scale speech dataset construction. Subsequently, we provide a comprehensive analysis of speech, text contained within the dataset. Finally, we describe the speech recognition baselines over the top-15 languages.",
            "url": null,
            "publicationVenue": "2023 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)",
            "citation_count": 1
        },
        {
            "title": "Espnet-Summ: Introducing a Novel Large Dataset, Toolkit, and a Cross-Corpora Evaluation of Speech Summarization Systems",
            "authors": "Roshan Sharma, William Chen, Takatomo Kano, Ruchira Sharma, Siddhant Arora, Shinji Watanabe, Atsunori Ogawa, Marc Delcroix, Rita Singh, Bhiksha Raj",
            "year": "2023",
            "abstract": "Speech summarization has garnered significant interest and progressed rapidly over the past few years. In particular, end-to-end models have recently emerged as a competitive alternative to cascade systems for abstractive video summarization. This paper aims to establish progress in this rapidly evolving research field, by introducing ESPNet-SUMM, a new open-source toolkit that facilitates a comprehensive comparison of end-to-end and cascade speech summarization models on 4 different speech summarization tasks spanning diverse applications. Experiments demonstrate that end-to-end models perform better for larger corpora with shorter inputs. This work also introduces Interview, the largest public open-domain multiparty interview corpus withof conversations between radio hosts and guests. Finally, this work explores the use of multiple datasets to improve end-to-end summarization, and\u00a0\u2026",
            "url": null,
            "publicationVenue": "2023 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)",
            "citation_count": 0
        },
        {
            "title": "Domain Adaptation by Data Distribution Matching Via Submodularity For Speech Recognition",
            "authors": "Yusuke Shinohara, Shinji Watanabe",
            "year": "2023",
            "abstract": "We study the problem of building a domain-specific speech recognition model given some text from the target domain. One of the most popular approaches to this problem is shallow fusion, which incorporates a domain-specific language model build from the given text. However, shallow fusion significantly increases the model size and inference cost, which makes its deployment harder. In this paper, we propose domain adaptation by data distribution matching, where a subset is selected from an existing multi-domain training data to match the target-domain distribution, and a model is fine-tuned on the subset. A submodular optimization algorithm with a novel extension is employed for the subset selection. Experiments on LibriSpeech, a corpus of audiobooks, where we treat each book as a domain, show that the proposed distribution-matching approach achieves WERs equivalent with the conventional shallow\u00a0\u2026",
            "url": null,
            "publicationVenue": "2023 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)",
            "citation_count": 0
        },
        {
            "title": "TorchAudio 2.1: Advancing speech recognition, self-supervised learning, and audio processing components for PyTorch",
            "authors": "Jeff Hwang, Moto Hira, Caroline Chen, Xiaohui Zhang, Zhaoheng Ni, Guangzhi Sun, Pingchuan Ma, Ruizhe Huang, Vineel Pratap, Yuekai Zhang, Anurag Kumar, Chin-Yun Yu, Chuang Zhu, Chunxi Liu, Jacob Kahn, Mirco Ravanelli, Peng Sun, Shinji Watanabe, Yangyang Shi, Yumeng Tao",
            "year": "2023",
            "abstract": "TorchAudio is an open-source audio and speech processing library built for PyTorch. It aims to accelerate the research and development of audio and speech technologies by providing well-designed, easy-to-use, and performant PyTorch components. Its contributors routinely engage with users to understand their needs and fulfill them by developing impactful features. Here, we survey TorchAudio\u2019s development principles and contents and highlight key features we include in its latest version (2.1): self-supervised learning pre-trained pipelines and training recipes, high-performance CTC decoders, speech recognition models and training recipes, advanced media I/O capabilities, and tools for performing forced alignment, multi-channel speech enhancement, and reference-less speech assessment. For a selection of these features, through empirical studies, we demonstrate their efficacy and show that they achieve\u00a0\u2026",
            "url": "https://arxiv.org/pdf/2310.17864",
            "publicationVenue": "2023 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)",
            "citation_count": 2
        },
        {
            "title": "A Single Speech Enhancement Model Unifying Dereverberation, Denoising, Speaker Counting, Separation, And Extraction",
            "authors": "Kohei Saijo, Wangyou Zhang, Zhong-Qiu Wang, Shinji Watanabe, Tetsunori Kobayashi, Tetsuji Ogawa",
            "year": "2023",
            "abstract": "We propose a multi-task universal speech enhancement (MUSE) model that can perform five speech enhancement (SE) tasks: dereverberation, denoising, speech separation (SS), target speaker extraction (TSE), and speaker counting. This is achieved by integrating two modules into an SE model: 1) an internal separation module that does both speaker counting and separation; and 2) a TSE module that extracts the target speech from the internal separation outputs using target speaker cues. The model is trained to perform TSE if the target speaker cue is given and SS otherwise. By training the model to remove noise and reverberation, we allow the model to tackle the five tasks mentioned above with a single model, which has not been accomplished yet. Evaluation results demonstrate that the proposed MUSE model can successfully handle multiple tasks with a single model.",
            "url": "https://arxiv.org/pdf/2310.08277",
            "publicationVenue": "2023 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)",
            "citation_count": 1
        },
        {
            "title": "Findings of the 2023 ML-Superb Challenge: Pre-Training And Evaluation Over More Languages And Beyond",
            "authors": "Jiatong Shi, William Chen, Dan Berrebbi, Hsiu-Hsuan Wang, Wei-Ping Huang, En-Pei Hu, Ho-Lam Chuang, Xuankai Chang, Yuxun Tang, Shang-Wen Li, Abdelrahman Mohamed, Hung-yi Lee, Shinji Watanabe",
            "year": "2023",
            "abstract": "The 2023 Multilingual Speech Universal Performance Benchmark (ML-SUPERB) Challenge expands upon the acclaimed SUPERB framework, emphasizing self-supervised models in multilingual speech recognition and language identification. The challenge comprises a research track focused on applying ML-SUPERB to specific multilingual subjects, a Challenge Track for model submissions, and a New Language Track where language resource researchers can contribute and evaluate their low-resource language data in the context of the latest progress in multilingual speech recognition. The challenge garnered 12 model submissions and 54 language corpora, resulting in a comprehensive benchmark encompassing 154 languages. The findings indicate that merely scaling models is not the definitive solution for multilingual speech tasks, and a variety of speech/voice types present significant challenges in\u00a0\u2026",
            "url": "https://arxiv.org/pdf/2310.05513",
            "publicationVenue": "2023 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)",
            "citation_count": 0
        },
        {
            "title": "Toward universal speech enhancement for diverse input conditions",
            "authors": "Wangyou Zhang, Kohei Saijo, Zhong-Qiu Wang, Shinji Watanabe, Yanmin Qian",
            "year": "2023",
            "abstract": "The past decade has witnessed substantial growth of data-driven speech enhancement (SE) techniques thanks to deep learning. While existing approaches have shown impressive performance in some common datasets, most of them are designed only for a single condition (e.g., single-channel, multi-channel, or a fixed sampling frequency) or only consider a single task (e.g., denoising or dereverberation). Currently, there is no universal SE approach that can effectively handle diverse input conditions with a single model. In this paper, we make the first attempt to investigate this line of research. First, we devise a single SE model that is independent of microphone channels, signal lengths, and sampling frequencies. Second, we design a universal SE benchmark by combining existing public corpora with multiple conditions. Our experiments on a wide range of datasets show that the proposed single model can\u00a0\u2026",
            "url": "https://arxiv.org/pdf/2309.17384",
            "publicationVenue": "2023 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)",
            "citation_count": 3
        },
        {
            "title": "Joint Prediction and Denoising for Large-Scale Multilingual Self-Supervised Learning",
            "authors": "William Chen, Jiatong Shi, Brian Yan, Dan Berrebbi, Wangyou Zhang, Yifan Peng, Xuankai Chang, Soumi Maiti, Shinji Watanabe",
            "year": "2023",
            "abstract": "Multilingual self-supervised learning (SSL) has often lagged behind state-of-the-art (SOTA) methods due to the expenses and complexity required to handle many languages. This further harms the reproducibility of SSL, which is already limited to few research groups due to its resource usage. We show that more powerful techniques can actually lead to more efficient pre-training, opening SSL to more research groups. We propose WavLabLM, which extends WavLM\u2019s joint prediction and denoising to 40k hours of data across 136 languages. To build WavLabLM, we devise a novel multi-stage pre-training method, designed to address the language imbalance of multilingual data. WavLabLM achieves comparable performance to XLS-R on ML-SUPERB with less thanof the training data, making SSL realizable with academic compute. We show that further efficiency can be achieved with a vanilla HuBERT Base model\u00a0\u2026",
            "url": "https://arxiv.org/pdf/2309.15317",
            "publicationVenue": "2023 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)",
            "citation_count": 2
        },
        {
            "title": "Segment-Level Vectorized Beam Search Based on Partially Autoregressive Inference",
            "authors": "Masao Someki, Nicholas Eng, Yosuke Higuchi, Shinji Watanabe",
            "year": "2023",
            "abstract": "Attention-based encoder-decoder models with autoregressive (AR) decoding have proven to be the dominant approach for automatic speech recognition (ASR) due to their superior accuracy. However, they often suffer from slow inference. This is primarily attributed to the incremental calculation of the decoder. This work proposes a partially AR framework, which employs segment-level vectorized beam search for improving the inference speed of an ASR model based on the hybrid connectionist temporal classification (CTC) attention-based architecture. It first generates an initial hypothesis using greedy CTC decoding, identifying low-confidence tokens based on their output probabilities. We then utilize the decoder to perform segment-level vectorized beam search on these tokens, re-predicting in parallel with minimal decoder calculations. Experimental results show that our method is 12 to 13 times faster in\u00a0\u2026",
            "url": "https://arxiv.org/pdf/2309.14922",
            "publicationVenue": "2023 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)",
            "citation_count": 0
        },
        {
            "title": "Reproducing whisper-style training using an open-source toolkit and publicly available data",
            "authors": "Yifan Peng, Jinchuan Tian, Brian Yan, Dan Berrebbi, Xuankai Chang, Xinjian Li, Jiatong Shi, Siddhant Arora, William Chen, Roshan Sharma, Wangyou Zhang, Yui Sudo, Muhammad Shakeel, Jee-Weon Jung, Soumi Maiti, Shinji Watanabe",
            "year": "2023",
            "abstract": "Pre-training speech models on large volumes of data has achieved remarkable success. OpenAI Whisper is a multilingual multitask model trained on 680k hours of supervised speech data. It generalizes well to various speech recognition and translation benchmarks even in a zero-shot setup. However, the full pipeline for developing such models (from data collection to training) is not publicly accessible, which makes it difficult for researchers to further improve its performance and address training-related issues such as efficiency, robustness, fairness, and bias. This work presents an Open Whisper-style Speech Model (OWSM), which reproduces Whisperstyle training using an open-source toolkit and publicly available data. OWSM even supports more translation directions and can be more efficient to train. We will publicly release all scripts used for data preparation, training, inference, and scoring as well as\u00a0\u2026",
            "url": "https://arxiv.org/pdf/2309.13876",
            "publicationVenue": "2023 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)",
            "citation_count": 7
        },
        {
            "title": "Understanding probe behaviors through variational bounds of mutual information",
            "authors": "Kwanghee Choi, Jee-weon Jung, Shinji Watanabe",
            "year": "2023",
            "abstract": "With the success of self-supervised representations, researchers seek a better understanding of the information encapsulated within a representation. Among various interpretability methods, we focus on classification-based linear probing. We aim to foster a solid understanding and provide guidelines for linear probing by constructing a novel mathematical framework leveraging information theory. First, we connect probing with the variational bounds of mutual information (MI) to relax the probe design, equating linear probing with fine-tuning. Then, we investigate empirical behaviors and practices of probing through our mathematical framework. We analyze the layer-wise performance curve being convex, which seemingly violates the data processing inequality. However, we show that the intermediate representations can have the biggest MI estimate because of the tradeoff between better separability and decreasing MI. We further suggest that the margin of linearly separable representations can be a criterion for measuring the \"goodness of representation.\" We also compare accuracy with MI as the measuring criteria. Finally, we empirically validate our claims by observing the self-supervised speech models on retaining word and phoneme information.",
            "url": "https://arxiv.org/html/2312.10019v1",
            "publicationVenue": "arXiv preprint arXiv:2312.10019",
            "citation_count": 1
        },
        {
            "title": "Phoneme-aware Encoding for Prefix-tree-based Contextual ASR",
            "authors": "Hayato Futami, Emiru Tsunoo, Yosuke Kashiwagi, Hiroaki Ogawa, Siddhant Arora, Shinji Watanabe",
            "year": "2023",
            "abstract": "In speech recognition applications, it is important to recognize context-specific rare words, such as proper nouns. Tree-constrained Pointer Generator (TCPGen) has shown promise for this purpose, which efficiently biases such words with a prefix tree. While the original TCPGen relies on grapheme-based encoding, we propose extending it with phoneme-aware encoding to better recognize words of unusual pronunciations. As TCPGen handles biasing words as subword units, we propose obtaining subword-level phoneme-aware encoding by using alignment between phonemes and subwords. Furthermore, we propose injecting phoneme-level predictions from CTC into queries of TCPGen so that the model better interprets the phoneme-aware encodings. We conducted ASR experiments with TCPGen for RNN transducer. We observed that proposed phoneme-aware encoding outperformed ordinary grapheme-based encoding on both the English LibriSpeech and Japanese CSJ datasets, demonstrating the robustness of our approach across linguistically diverse languages.",
            "url": "https://arxiv.org/html/2312.09582v1",
            "publicationVenue": "arXiv preprint arXiv:2312.09582",
            "citation_count": 0
        },
        {
            "title": "Generative Context-aware Fine-tuning of Self-supervised Speech Models",
            "authors": "Suwon Shon, Kwangyoun Kim, Prashant Sridhar, Yi-Te Hsu, Shinji Watanabe, Karen Livescu",
            "year": "2023",
            "abstract": "When performing tasks like automatic speech recognition or spoken language understanding for a given utterance, access to preceding text or audio provides contextual information can improve performance. Considering the recent advances in generative large language models (LLM), we hypothesize that an LLM could generate useful context information using the preceding text. With appropriate prompts, LLM could generate a prediction of the next sentence or abstractive text like titles or topics. In this paper, we study the use of LLM-generated context information and propose an approach to distill the generated information during fine-tuning of self-supervised speech models, which we refer to as generative context-aware fine-tuning. This approach allows the fine-tuned model to make improved predictions without access to the true surrounding segments or to the LLM at inference time, while requiring only a very small additional context module. We evaluate the proposed approach using the SLUE and Libri-light benchmarks for several downstream tasks: automatic speech recognition, named entity recognition, and sentiment analysis. The results show that generative context-aware fine-tuning outperforms a context injection fine-tuning approach that accesses the ground-truth previous text, and is competitive with a generative context injection fine-tuning approach that requires the LLM at inference time.",
            "url": "https://arxiv.org/pdf/2312.09895",
            "publicationVenue": "arXiv preprint arXiv:2312.09895",
            "citation_count": 0
        },
        {
            "title": "End-to-end integration of online and offline encoders using auxiliary losses for automatic speech recognition",
            "authors": "Shakeel Muhammad, Sudo Yui, Peng Yifan, Watanabe Shinji",
            "year": "2023",
            "abstract": "End-to-end (E2E) automatic speech recognition (ASR) models have two desirable properties: online and offline modes. The online ASR mode, which operates under strict latency constraints, processes speech frames in real-time to provide transcription. Conversely, the offline ASR mode waits for the complete utterance of speech frames before generating a transcription. Recently, the integration of online and offline ASR for recurrent neural network transducers (RNNT) can be achieved through the joint training of online and offline encoders with a shared decoder. However, this integration comes at the cost of performance degradation in the offline ASR mode, as the shared decoder must handle features of varying contexts. Namely, with E2E integration framework of online and offline encoders, we explore two approaches to enhance the performance of both the ASR modes. First, we introduce separate RNN-T decoders for each ASR mode while maintaining shared encoders, thereby effectively managing features of different contexts. Second, we explore multiple auxiliary loss criteria to introduce additional regularization, thereby enhancing the overall stability and performance of the framework. Overall, evaluation results show 1.8%-2.5% relative character error rate reductions (CERR) on corpora of spontaneous Japanese (CSJ) for online ASR, and 4.4%-6.3% relative CERRs for offline ASR within a single model compared to separate online and offline models.",
            "url": "https://www.jstage.jst.go.jp/article/jsaisigtwo/2023/Challenge-063/2023_03/_pdf",
            "publicationVenue": "\u4eba\u5de5\u77e5\u80fd\u5b66\u4f1a\u7b2c\u4e8c\u7a2e\u7814\u7a76\u4f1a\u8cc7\u6599",
            "citation_count": 0
        },
        {
            "title": "Software Design and User Interface of ESPnet-SE++: Speech Enhancement for Robust Speech Processing",
            "authors": "Yen-Ju Lu, Xuankai Chang, Chenda Li, Wangyou Zhang, Samuele Cornell, Zhaoheng Ni, Yoshiki Masuyama, Brian Yan, Robin Scheibler, Zhong-Qiu Wang, Yu Tsao, Yanmin Qian, Shinji Watanabe",
            "year": "2023",
            "abstract": "This paper presents the software design and user interface of ESPnet-SE++, a new speech separation and enhancement (SSE) module of the ESPnet toolkit. ESPnet-SE++ significantly expands the functionality of ESPnet-SE (Li et al., 2021) with several new models (Chen et al., 2017; Dang et al., 2022; Hershey et al., 2016; Hu et al., 2020; Li et al., 2022; Lu, Cornell, et al., 2022; Luo et al., 2019; Takahashi et al., 2019; Tan et al., 2021), loss functions (Boeddeker et al., 2021; Le Roux et al., 2019; Luo & Mesgarani, 2018; Scheibler, 2022), and training recipes as shown in (Lu, Chang, et al., 2022). Crucially, it features a new, redesigned interface, which allows for a flexible combination of SSE front-ends with many downstream tasks, including automatic speech recognition (ASR), speaker diarization (SD), speech translation (ST), and spoken language understanding (SLU).",
            "url": "https://joss.theoj.org/papers/10.21105/joss.05403.pdf",
            "publicationVenue": "Journal of Open Source Software",
            "citation_count": 0
        },
        {
            "title": "Music ControlNet: Multiple time-varying controls for music generation",
            "authors": "Shih-Lun Wu, Chris Donahue, Shinji Watanabe, Nicholas J Bryan",
            "year": "2023",
            "abstract": "Text-to-music generation models are now capable of generating high-quality music audio in broad styles. However, text control is primarily suitable for the manipulation of global musical attributes like genre, mood, and tempo, and is less suitable for precise control over time-varying attributes such as the positions of beats in time or the changing dynamics of the music. We propose Music ControlNet, a diffusion-based music generation model that offers multiple precise, time-varying controls over generated audio. To imbue text-to-music models with time-varying control, we propose an approach analogous to pixel-wise control of the image-domain ControlNet method. Specifically, we extract controls from training audio yielding paired data, and fine-tune a diffusion-based conditional generative model over audio spectrograms given melody, dynamics, and rhythm controls. While the image-domain Uni-ControlNet method already allows generation with any subset of controls, we devise a new strategy to allow creators to input controls that are only partially specified in time. We evaluate both on controls extracted from audio and controls we expect creators to provide, demonstrating that we can generate realistic music that corresponds to control inputs in both settings. While few comparable music generation models exist, we benchmark against MusicGen, a recent model that accepts text and melody input, and show that our model generates music that is 49% more faithful to input melodies despite having 35x fewer parameters, training on 11x less data, and enabling two additional forms of time-varying control. Sound examples can be found at https\u00a0\u2026",
            "url": "https://arxiv.org/pdf/2311.07069",
            "publicationVenue": "arXiv preprint arXiv:2311.07069",
            "citation_count": 4
        },
        {
            "title": "Synthetic Data Augmentation for ASR with Domain Filtering",
            "authors": "Tuan Vu Ho, Shota Horiguchi, Shinji Watanabe, Paola Garcia, Takashi Sumiyoshi",
            "year": "2023",
            "abstract": "Recent studies have shown that synthetic speech can effectively serve as training data for automatic speech recognition models. Text data for synthetic speech is mostly obtained from in-domain text or generated text using augmentation. However, obtaining large amounts of in-domain text data with diverse lexical contexts is difficult, especially in low-resource scenarios. This paper proposes using text from a large generic-domain source and applying a domain filtering method to choose the relevant text data. This method involves two filtering steps: 1) selecting text based on its semantic similarity to the available in-domain text and 2) diversifying the vocabulary of the selected text using a greedy-search algorithm. Experimental results show that our proposed method outperforms the conventional text augmentation approach, with the relative reduction of word-error-rate ranging from 6% to 25% on the LibriSpeech\u00a0\u2026",
            "url": null,
            "publicationVenue": "2023 Asia Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC)",
            "citation_count": 0
        },
        {
            "title": "Exploring the Integration of Speech Separation and Recognition with Self-Supervised Learning Representation",
            "authors": "Yoshiki Masuyama, Xuankai Chang, Wangyou Zhang, Samuele Cornell, Zhong-Qiu Wang, Nobutaka Ono, Yanmin Qian, Shinji Watanabe",
            "year": "2023",
            "abstract": "Neural speech separation has made remarkable progress and its integration with automatic speech recognition (ASR) is an important direction towards realizing multi-speaker ASR. This work provides an insightful investigation of speech separation in reverberant and noisy-reverberant scenarios as an ASR front-end. In detail, we explore multi-channel separation methods, mask-based beamforming and complex spectral mapping, as well as the best features to use in the ASR back-end model. We employ the recent self-supervised learning representation (SSLR) as a feature and improve the recognition performance from the case with filterbank features. To further improve multi-speaker recognition performance, we present a carefully designed training strategy for integrating speech separation and recognition with SSLR. The proposed integration using TF-GridNet-based complex spectral mapping and WavLM\u00a0\u2026",
            "url": "https://arxiv.org/pdf/2307.12231",
            "publicationVenue": "2023 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA)",
            "citation_count": 1
        },
        {
            "title": "HuBERTopic: Enhancing Semantic Representation of HuBERT through Self-supervision Utilizing Topic Model",
            "authors": "Takashi Maekaku, Jiatong Shi, Xuankai Chang, Yuya Fujita, Shinji Watanabe",
            "year": "2023",
            "abstract": "Recently, the usefulness of self-supervised representation learning (SSRL) methods has been confirmed in various downstream tasks. Many of these models, as exemplified by HuBERT and WavLM, use pseudo-labels generated from spectral features or the model's own representation features. From previous studies, it is known that the pseudo-labels contain semantic information. However, the masked prediction task, the learning criterion of HuBERT, focuses on local contextual information and may not make effective use of global semantic information such as speaker, theme of speech, and so on. In this paper, we propose a new approach to enrich the semantic representation of HuBERT. We apply topic model to pseudo-labels to generate a topic label for each utterance. An auxiliary topic classification task is added to HuBERT by using topic labels as teachers. This allows additional global semantic information to be incorporated in an unsupervised manner. Experimental results demonstrate that our method achieves comparable or better performance than the baseline in most tasks, including automatic speech recognition and five out of the eight SUPERB tasks. Moreover, we find that topic labels include various information about utterance, such as gender, speaker, and its theme. This highlights the effectiveness of our approach in capturing multifaceted semantic nuances.",
            "url": "https://arxiv.org/pdf/2310.03975",
            "publicationVenue": "arXiv preprint arXiv:2310.03975",
            "citation_count": 0
        },
        {
            "title": "EFFUSE: Efficient self-supervised feature fusion for E2E ASR in multilingual and low resource scenarios",
            "authors": "Tejes Srivastava, Jiatong Shi, William Chen, Shinji Watanabe",
            "year": "2023",
            "abstract": "Self-Supervised Learning (SSL) models have demonstrated exceptional performance in various speech tasks, particularly in low-resource and multilingual domains. Recent works show that fusing SSL models could achieve superior performance compared to using one SSL model. However, fusion models have increased model parameter size, leading to longer inference times. In this paper, we propose a novel approach of predicting other SSL models' features from a single SSL model, resulting in a light-weight framework with competitive performance. Our experiments show that SSL feature prediction models outperform individual SSL models in multilingual speech recognition tasks. The leading prediction model achieves an average SUPERB score increase of 135.4 in ML-SUPERB benchmarks. Moreover, our proposed framework offers an efficient solution, as it reduces the resulting model parameter size and inference times compared to previous fusion models.",
            "url": "https://arxiv.org/pdf/2310.03938",
            "publicationVenue": "arXiv preprint arXiv:2310.03938",
            "citation_count": 1
        },
        {
            "title": "Universlu: Universal spoken language understanding for diverse classification and sequence generation tasks with a single network",
            "authors": "Siddhant Arora, Hayato Futami, Jee-weon Jung, Yifan Peng, Roshan Sharma, Yosuke Kashiwagi, Emiru Tsunoo, Shinji Watanabe",
            "year": "2023",
            "abstract": "Recent studies have demonstrated promising outcomes by employing large language models with multi-tasking capabilities. They utilize prompts to guide the model's behavior and surpass performance of task-specific models. Motivated by this, we ask: can we build a single model that jointly perform various spoken language understanding (SLU) tasks? To address this, we utilize pre-trained automatic speech recognition (ASR) models and employ various task and dataset specifiers as discrete prompts. We demonstrate efficacy of our single multi-task learning (MTL) model \"UniverSLU\" for 12 different speech classification and sequence generation tasks across 17 datasets and 9 languages. Results show that UniverSLU achieves competitive performance and even surpasses task-specific models. We also conduct preliminary investigations into enabling human-interpretable natural phrases instead of task specifiers as discrete prompts and test the model's generalization capabilities to new paraphrases.",
            "url": "https://arxiv.org/pdf/2310.02973",
            "publicationVenue": "arXiv preprint arXiv:2310.02973",
            "citation_count": 2
        },
        {
            "title": "One model to rule them all? Towards End-to-End Joint Speaker Diarization and Speech Recognition",
            "authors": "Samuele Cornell, Jee-weon Jung, Shinji Watanabe, Stefano Squartini",
            "year": "2023",
            "abstract": "This paper presents a novel framework for joint speaker diarization (SD) and automatic speech recognition (ASR), named SLIDAR (sliding-window diarization-augmented recognition). SLIDAR can process arbitrary length inputs and can handle any number of speakers, effectively solving ``who spoke what, when'' concurrently. SLIDAR leverages a sliding window approach and consists of an end-to-end diarization-augmented speech transcription (E2E DAST) model which provides, locally, for each window: transcripts, diarization and speaker embeddings. The E2E DAST model is based on an encoder-decoder architecture and leverages recent techniques such as serialized output training and ``Whisper-style\" prompting. The local outputs are then combined to get the final SD+ASR result by clustering the speaker embeddings to get global speaker identities. Experiments performed on monaural recordings from the AMI corpus confirm the effectiveness of the method in both close-talk and far-field speech scenarios.",
            "url": "https://arxiv.org/pdf/2310.01688",
            "publicationVenue": "arXiv preprint arXiv:2310.01688",
            "citation_count": 0
        },
        {
            "title": "Improving audio captioning models with fine-grained audio features, text embedding supervision, and llm mix-up augmentation",
            "authors": "Shih-Lun Wu, Xuankai Chang, Gordon Wichern, Jee-weon Jung, Fran\u00e7ois Germain, Jonathan Le Roux, Shinji Watanabe",
            "year": "2023",
            "abstract": "Automated audio captioning (AAC) aims to generate informative descriptions for various sounds from nature and/or human activities. In recent years, AAC has quickly attracted research interest, with state-of-the-art systems now relying on a sequence-to-sequence (seq2seq) backbone powered by strong models such as Transformers. Following the macro-trend of applied machine learning research, in this work, we strive to improve the performance of seq2seq AAC models by extensively leveraging pretrained models and large language models (LLMs). Specifically, we utilize BEATs to extract fine-grained audio features. Then, we employ Instructor LLM to fetch text embeddings of captions, and infuse their language-modality knowledge into BEATs audio features via an auxiliary InfoNCE loss function. Moreover, we propose a novel data augmentation method that uses ChatGPT to produce caption mix-ups (i.e., grammatical and compact combinations of two captions) which, together with the corresponding audio mixtures, increase not only the amount but also the complexity and diversity of training data. During inference, we propose to employ nucleus sampling and a hybrid reranking algorithm, which has not been explored in AAC research. Combining our efforts, our model achieves a new state-of-the-art 32.6 SPIDEr-FL score on the Clotho evaluation split, and wins the 2023 DCASE AAC challenge.",
            "url": "https://arxiv.org/pdf/2309.17352",
            "publicationVenue": "arXiv preprint arXiv:2309.17352",
            "citation_count": 4
        },
        {
            "title": "Cross-Modal Multi-Tasking for Speech-to-Text Translation via Hard Parameter Sharing",
            "authors": "Brian Yan, Xuankai Chang, Antonios Anastasopoulos, Yuya Fujita, Shinji Watanabe",
            "year": "2023",
            "abstract": "Recent works in end-to-end speech-to-text translation (ST) have proposed multi-tasking methods with soft parameter sharing which leverage machine translation (MT) data via secondary encoders that map text inputs to an eventual cross-modal representation. In this work, we instead propose a ST/MT multi-tasking framework with hard parameter sharing in which all model parameters are shared cross-modally. Our method reduces the speech-text modality gap via a pre-processing stage which converts speech and text inputs into two discrete token sequences of similar length -- this allows models to indiscriminately process both modalities simply using a joint vocabulary. With experiments on MuST-C, we demonstrate that our multi-tasking framework improves attentional encoder-decoder, Connectionist Temporal Classification (CTC), transducer, and joint CTC/attention models by an average of +0.5 BLEU without any external MT data. Further, we show that this framework incorporates external MT data, yielding +0.8 BLEU, and also improves transfer learning from pre-trained textual models, yielding +1.8 BLEU.",
            "url": "https://arxiv.org/pdf/2309.15826",
            "publicationVenue": "arXiv preprint arXiv:2309.15826",
            "citation_count": 0
        },
        {
            "title": "Enhancing End-to-End Conversational Speech Translation Through Target Language Context Utilization",
            "authors": "Amir Hussein, Brian Yan, Antonios Anastasopoulos, Shinji Watanabe, Sanjeev Khudanpur",
            "year": "2023",
            "abstract": "Incorporating longer context has been shown to benefit machine translation, but the inclusion of context in end-to-end speech translation (E2E-ST) remains under-studied. To bridge this gap, we introduce target language context in E2E-ST, enhancing coherence and overcoming memory constraints of extended audio segments. Additionally, we propose context dropout to ensure robustness to the absence of context, and further improve performance by adding speaker information. Our proposed contextual E2E-ST outperforms the isolated utterance-based E2E-ST approach. Lastly, we demonstrate that in conversational speech, contextual information primarily contributes to capturing context style, as well as resolving anaphora and named entities.",
            "url": "https://arxiv.org/pdf/2309.15686",
            "publicationVenue": "arXiv preprint arXiv:2309.15686",
            "citation_count": 0
        },
        {
            "title": "Exploring speech recognition, translation, and understanding with discrete speech units: A comparative study",
            "authors": "Xuankai Chang, Brian Yan, Kwanghee Choi, Jeeweon Jung, Yichen Lu, Soumi Maiti, Roshan Sharma, Jiatong Shi, Jinchuan Tian, Shinji Watanabe, Yuya Fujita, Takashi Maekaku, Pengcheng Guo, Yao-Fei Cheng, Pavel Denisov, Kohei Saijo, Hsiu-Hsuan Wang",
            "year": "2023",
            "abstract": "Speech signals, typically sampled at rates in the tens of thousands per second, contain redundancies, evoking inefficiencies in sequence modeling. High-dimensional speech features such as spectrograms are often used as the input for the subsequent model. However, they can still be redundant. Recent investigations proposed the use of discrete speech units derived from self-supervised learning representations, which significantly compresses the size of speech data. Applying various methods, such as de-duplication and subword modeling, can further compress the speech sequence length. Hence, training time is significantly reduced while retaining notable performance. In this study, we undertake a comprehensive and systematic exploration into the application of discrete units within end-to-end speech processing models. Experiments on 12 automatic speech recognition, 3 speech translation, and 1 spoken language understanding corpora demonstrate that discrete units achieve reasonably good results in almost all the settings. We intend to release our configurations and trained models to foster future research efforts.",
            "url": "https://arxiv.org/pdf/2309.15800",
            "publicationVenue": "arXiv preprint arXiv:2309.15800",
            "citation_count": 4
        },
        {
            "title": "Speech collage: code-switched audio generation by collaging monolingual corpora",
            "authors": "Amir Hussein, Dorsa Zeinali, Ond\u0159ej Klejch, Matthew Wiesner, Brian Yan, Shammur Chowdhury, Ahmed Ali, Shinji Watanabe, Sanjeev Khudanpur",
            "year": "2023",
            "abstract": "Designing effective automatic speech recognition (ASR) systems for Code-Switching (CS) often depends on the availability of the transcribed CS resources. To address data scarcity, this paper introduces Speech Collage, a method that synthesizes CS data from monolingual corpora by splicing audio segments. We further improve the smoothness quality of audio generation using an overlap-add approach. We investigate the impact of generated data on speech recognition in two scenarios: using in-domain CS text and a zero-shot approach with synthesized CS text. Empirical results highlight up to 34.4% and 16.2% relative reductions in Mixed-Error Rate and Word-Error Rate for in-domain and zero-shot scenarios, respectively. Lastly, we demonstrate that CS augmentation bolsters the model's code-switching inclination and reduces its monolingual bias.",
            "url": "https://arxiv.org/pdf/2309.15674",
            "publicationVenue": "arXiv preprint arXiv:2309.15674",
            "citation_count": 0
        },
        {
            "title": "Incremental Blockwise Beam Search for Simultaneous Speech Translation with Controllable Quality-Latency Tradeoff",
            "authors": "Peter Pol\u00e1k, Brian Yan, Shinji Watanabe, Alex Waibel, Ond\u0159ej Bojar",
            "year": "2023",
            "abstract": "Blockwise self-attentional encoder models have recently emerged as one promising end-to-end approach to simultaneous speech translation. These models employ a blockwise beam search with hypothesis reliability scoring to determine when to wait for more input speech before translating further. However, this method maintains multiple hypotheses until the entire speech input is consumed -- this scheme cannot directly show a single \\textit{incremental} translation to users. Further, this method lacks mechanisms for \\textit{controlling} the quality vs. latency tradeoff. We propose a modified incremental blockwise beam search incorporating local agreement or hold-policies for quality-latency control. We apply our framework to models trained for online or offline translation and demonstrate that both types can be effectively used in online mode. Experimental results on MuST-C show 0.6-3.6 BLEU improvement without changing latency or 0.8-1.4 s latency improvement without changing quality.",
            "url": "https://arxiv.org/pdf/2309.11379",
            "publicationVenue": "arXiv preprint arXiv:2309.11379",
            "citation_count": 2
        },
        {
            "title": "Av-superb: A multi-task evaluation benchmark for audio-visual representation models",
            "authors": "Yuan Tseng, Layne Berry, Yi-Ting Chen, I Chiu, Hsuan-Hao Lin, Max Liu, Puyuan Peng, Yi-Jen Shih, Hung-Yu Wang, Haibin Wu, Po-Yao Huang, Chun-Mao Lai, Shang-Wen Li, David Harwath, Yu Tsao, Shinji Watanabe, Abdelrahman Mohamed, Chi-Luen Feng, Hung-yi Lee",
            "year": "2023",
            "abstract": "Audio-visual representation learning aims to develop systems with human-like perception by utilizing correlation between auditory and visual information. However, current models often focus on a limited set of tasks, and generalization abilities of learned representations are unclear. To this end, we propose the AV-SUPERB benchmark that enables general-purpose evaluation of unimodal audio/visual and bimodal fusion representations on 7 datasets covering 5 audio-visual tasks in speech and audio processing. We evaluate 5 recent self-supervised models and show that none of these models generalize to all tasks, emphasizing the need for future study on improving universal model performance. In addition, we show that representations may be improved with intermediate-task fine-tuning and audio event classification with AudioSet serves as a strong intermediate task. We release our benchmark with evaluation code and a model submission platform to encourage further research in audio-visual learning.",
            "url": "https://arxiv.org/pdf/2309.10787",
            "publicationVenue": "arXiv preprint arXiv:2309.10787",
            "citation_count": 2
        },
        {
            "title": "Semi-Autoregressive Streaming ASR With Label Context",
            "authors": "Siddhant Arora, George Saon, Shinji Watanabe, Brian Kingsbury",
            "year": "2023",
            "abstract": "Non-autoregressive (NAR) modeling has gained significant interest in speech processing since these models achieve dramatically lower inference time than autoregressive (AR) models while also achieving good transcription accuracy. Since NAR automatic speech recognition (ASR) models must wait for the completion of the entire utterance before processing, some works explore streaming NAR models based on blockwise attention for low-latency applications. However, streaming NAR models significantly lag in accuracy compared to streaming AR and non-streaming NAR models. To address this, we propose a streaming \"semi-autoregressive\" ASR model that incorporates the labels emitted in previous blocks as additional context using a Language Model (LM) subnetwork. We also introduce a novel greedy decoding algorithm that addresses insertion and deletion errors near block boundaries while not significantly increasing the inference time. Experiments show that our method outperforms the existing streaming NAR model by 19% relative on Tedlium2, 16%/8% on Librispeech-100 clean/other test sets, and 19%/8% on the Switchboard(SWB) / Callhome(CH) test sets. It also reduced the accuracy gap with streaming AR and non-streaming NAR models while achieving 2.5x lower latency. We also demonstrate that our approach can effectively utilize external text data to pre-train the LM subnetwork to further improve streaming ASR accuracy.",
            "url": "https://arxiv.org/html/2309.10926v2",
            "publicationVenue": "arXiv preprint arXiv:2309.10926",
            "citation_count": 0
        },
        {
            "title": "Dynamic-superb: Towards a dynamic, collaborative, and comprehensive instruction-tuning benchmark for speech",
            "authors": "Chien-yu Huang, Ke-Han Lu, Shih-Heng Wang, Chi-Yuan Hsiao, Chun-Yi Kuan, Haibin Wu, Siddhant Arora, Kai-Wei Chang, Jiatong Shi, Yifan Peng, Roshan Sharma, Shinji Watanabe, Bhiksha Ramakrishnan, Shady Shehata, Hung-yi Lee",
            "year": "2023",
            "abstract": "Text language models have shown remarkable zero-shot capability in generalizing to unseen tasks when provided with well-formulated instructions. However, existing studies in speech processing primarily focus on limited or specific tasks. Moreover, the lack of standardized benchmarks hinders a fair comparison across different approaches. Thus, we present Dynamic-SUPERB, a benchmark designed for building universal speech models capable of leveraging instruction tuning to perform multiple tasks in a zero-shot fashion. To achieve comprehensive coverage of diverse speech tasks and harness instruction tuning, we invite the community to collaborate and contribute, facilitating the dynamic growth of the benchmark. To initiate, Dynamic-SUPERB features 55 evaluation instances by combining 33 tasks and 22 datasets. This spans a broad spectrum of dimensions, providing a comprehensive platform for evaluation. Additionally, we propose several approaches to establish benchmark baselines. These include the utilization of speech models, text language models, and the multimodal encoder. Evaluation results indicate that while these baselines perform reasonably on seen tasks, they struggle with unseen ones. We also conducted an ablation study to assess the robustness and seek improvements in the performance. We release all materials to the public and welcome researchers to collaborate on the project, advancing technologies in the field together.",
            "url": "https://arxiv.org/pdf/2309.09510",
            "publicationVenue": "arXiv preprint arXiv:2309.09510",
            "citation_count": 3
        },
        {
            "title": "Decoder-only architecture for speech recognition with ctc prompts and text data augmentation",
            "authors": "Emiru Tsunoo, Hayato Futami, Yosuke Kashiwagi, Siddhant Arora, Shinji Watanabe",
            "year": "2023",
            "abstract": "Collecting audio-text pairs is expensive; however, it is much easier to access text-only data. Unless using shallow fusion, end-to-end automatic speech recognition (ASR) models require architecture modifications or additional training schemes to use text-only data. Inspired by recent advances in decoder-only language models (LMs), such as GPT-3 and PaLM adopted for speech-processing tasks, we propose using a decoder-only architecture for ASR with simple text augmentation. To provide audio information, encoder features compressed by CTC prediction are used as prompts for the decoder, which can be regarded as refining CTC prediction using the decoder-only model. Because the decoder architecture is the same as an autoregressive LM, it is simple to enhance the model by leveraging external text data with LM training. An experimental comparison using LibriSpeech and Switchboard shows that our proposed models with text augmentation training reduced word error rates from ordinary CTC by 0.3% and 1.4% on LibriSpeech test-clean and testother set, respectively, and 2.9% and 5.0% on Switchboard and CallHome. The proposed model had advantage on computational efficiency compared with conventional encoder-decoder ASR models with a similar parameter setup, and outperformed them on the LibriSpeech 100h and Switchboard training scenarios.",
            "url": "https://arxiv.org/html/2309.08876v2",
            "publicationVenue": "arXiv preprint arXiv:2309.08876",
            "citation_count": 2
        },
        {
            "title": "The multimodal information based speech processing (misp) 2023 challenge: Audio-visual target speaker extraction",
            "authors": "Shilong Wu, Chenxi Wang, Hang Chen, Yusheng Dai, Chenyue Zhang, Ruoyu Wang, Hongbo Lan, Jun Du, Chin-Hui Lee, Jingdong Chen, Shinji Watanabe, Sabato Marco Siniscalchi, Odette Scharenborg, Zhong-Qiu Wang, Jia Pan, Jianqing Gao",
            "year": "2023",
            "abstract": "Previous Multimodal Information based Speech Processing (MISP) challenges mainly focused on audio-visual speech recognition (AVSR) with commendable success. However, the most advanced back-end recognition systems often hit performance limits due to the complex acoustic environments. This has prompted a shift in focus towards the Audio-Visual Target Speaker Extraction (AVTSE) task for the MISP 2023 challenge in ICASSP 2024 Signal Processing Grand Challenges. Unlike existing audio-visual speech enhance-ment challenges primarily focused on simulation data, the MISP 2023 challenge uniquely explores how front-end speech processing, combined with visual clues, impacts back-end tasks in real-world scenarios. This pioneering effort aims to set the first benchmark for the AVTSE task, offering fresh insights into enhancing the ac-curacy of back-end speech recognition systems through AVTSE in challenging and real acoustic environments. This paper delivers a thorough overview of the task setting, dataset, and baseline system of the MISP 2023 challenge. It also includes an in-depth analysis of the challenges participants may encounter. The experimental results highlight the demanding nature of this task, and we look forward to the innovative solutions participants will bring forward.",
            "url": "https://arxiv.org/pdf/2309.08348",
            "publicationVenue": "arXiv preprint arXiv:2309.08348",
            "citation_count": 1
        },
        {
            "title": "Visual Speech Recognition for Low-resource Languages with Automatic Labels From Whisper Model",
            "authors": "Jeong Hun Yeo, Minsu Kim, Shinji Watanabe, Yong Man Ro",
            "year": "2023",
            "abstract": "This paper proposes a powerful Visual Speech Recognition (VSR) method for multiple languages, especially for low-resource languages that have a limited number of labeled data. Different from previous methods that tried to improve the VSR performance for the target language by using knowledge learned from other languages, we explore whether we can increase the amount of training data itself for the different languages without human intervention. To this end, we employ a Whisper model which can conduct both language identification and audio-based speech recognition. It serves to filter data of the desired languages and transcribe labels from the unannotated, multilingual audio-visual data pool. By comparing the performances of VSR models trained on automatic labels and the human-annotated labels, we show that we can achieve similar VSR performance to that of human-annotated labels even without utilizing human annotations. Through the automated labeling process, we label large-scale unlabeled multilingual databases, VoxCeleb2 and AVSpeech, producing 1,002 hours of data for four low VSR resource languages, French, Italian, Spanish, and Portuguese. With the automatic labels, we achieve new state-of-the-art performance on mTEDx in four languages, significantly surpassing the previous methods. The automatic labels are available online: https://github.com/JeongHun0716/Visual-Speech-Recognition-for-Low-Resource-Languages",
            "url": "https://arxiv.org/html/2309.08535v2",
            "publicationVenue": "arXiv preprint arXiv:2309.08535",
            "citation_count": 2
        },
        {
            "title": "Towards practical and efficient image-to-speech captioning with vision-language pre-training and multi-modal tokens",
            "authors": "Minsu Kim, Jeongsoo Choi, Soumi Maiti, Jeong Hun Yeo, Shinji Watanabe, Yong Man Ro",
            "year": "2023",
            "abstract": "In this paper, we propose methods to build a powerful and efficient Image-to-Speech captioning (Im2Sp) model. To this end, we start with importing the rich knowledge related to image comprehension and language modeling from a large-scale pre-trained vision-language model into Im2Sp. We set the output of the proposed Im2Sp as discretized speech units, i.e., the quantized speech features of a self-supervised speech model. The speech units mainly contain linguistic information while suppressing other characteristics of speech. This allows us to incorporate the language modeling capability of the pre-trained vision-language model into the spoken language modeling of Im2Sp. With the vision-language pre-training strategy, we set new state-of-the-art Im2Sp performances on two widely used benchmark databases, COCO and Flickr8k. Then, we further improve the efficiency of the Im2Sp model. Similar to the speech unit case, we convert the original image into image units, which are derived through vector quantization of the raw image. With these image units, we can drastically reduce the required data storage for saving image data to just 0.8% when compared to the original image data in terms of bits. Demo page: https://ms-dot-k.github.io/Image-to-Speech-Captioning.",
            "url": "https://arxiv.org/pdf/2309.08531",
            "publicationVenue": "arXiv preprint arXiv:2309.08531",
            "citation_count": 3
        },
        {
            "title": "Voxtlm: unified decoder-only models for consolidating speech recognition/synthesis and speech/text continuation tasks",
            "authors": "Soumi Maiti, Yifan Peng, Shukjae Choi, Jee-weon Jung, Xuankai Chang, Shinji Watanabe",
            "year": "2023",
            "abstract": "We propose a decoder-only language model, VoxtLM, that can perform four tasks: speech recognition, speech synthesis, text generation, and speech continuation. VoxtLM integrates text vocabulary with discrete speech tokens from self-supervised speech features and uses special tokens to enable multitask learning. Compared to a single-task model, VoxtLM exhibits a significant improvement in speech synthesis, with improvements in both speech intelligibility from 28.9 to 5.6 and objective quality from 2.68 to 3.90. VoxtLM also improves speech generation and speech recognition performance over the single-task counterpart. VoxtLM is trained with publicly available data and training recipes and model checkpoints will be open-sourced to make fully reproducible work.",
            "url": "https://arxiv.org/html/2309.07937v3",
            "publicationVenue": "arXiv preprint arXiv:2309.07937",
            "citation_count": 8
        },
        {
            "title": "Multilingual TTS Accent Impressions for Accented ASR",
            "authors": "Georgios Karakasidis, Nathaniel Robinson, Yaroslav Getman, Atieno Ogayo, Ragheb Al-Ghezi, Ananya Ayasi, Shinji Watanabe, David R Mortensen, Mikko Kurimo",
            "year": "2023",
            "abstract": "Automatic Speech Recognition (ASR) for high-resource languages like English is often considered a solved problem. However, most high-resource ASR systems favor socioeconomically advantaged dialects. In the case of English, this leaves behind many L2 speakers and speakers of low-resource accents (a majority of English speakers). One way to mitigate this is to fine-tune a pre-trained English ASR model for a desired low-resource accent. However, collecting transcribed accented audio is costly and time-consuming. In this work, we present a method to produce synthetic L2-English speech via pre-trained text-to-speech (TTS) in an L1 language (target accent). This can be produced at a much larger scale and lower cost than authentic speech collection. We present initial experiments applying this augmentation method. Our results suggest that success of TTS augmentation relies on access to more than one\u00a0\u2026",
            "url": null,
            "publicationVenue": "International Conference on Text, Speech, and Dialogue",
            "citation_count": 0
        },
        {
            "title": "Bayes Risk Transducer: Transducer with Controllable Alignment Prediction",
            "authors": "Jinchuan Tian, Jianwei Yu, Hangting Chen, Brian Yan, Chao Weng, Dong Yu, Shinji Watanabe",
            "year": "2023",
            "abstract": "Automatic speech recognition (ASR) based on transducers is widely used. In training, a transducer maximizes the summed posteriors of all paths. The path with the highest posterior is commonly defined as the predicted alignment between the speech and the transcription. While the vanilla transducer does not have a prior preference for any of the valid paths, this work intends to enforce the preferred paths and achieve controllable alignment prediction. Specifically, this work proposes Bayes Risk Transducer (BRT), which uses a Bayes risk function to set lower risk values to the preferred paths so that the predicted alignment is more likely to satisfy specific desired properties. We further demonstrate that these predicted alignments with intentionally designed properties can provide practical advantages over the vanilla transducer. Experimentally, the proposed BRT saves inference cost by up to 46% for non-streaming ASR and reduces overall system latency by 41% for streaming ASR.",
            "url": "https://arxiv.org/pdf/2308.10107",
            "publicationVenue": "arXiv preprint arXiv:2308.10107",
            "citation_count": 0
        },
        {
            "title": "TF-GridNet: Integrating full-and sub-band modeling for speech separation",
            "authors": "Zhong-Qiu Wang, Samuele Cornell, Shukjae Choi, Younglo Lee, Byeong-Yeol Kim, Shinji Watanabe",
            "year": "2023",
            "abstract": "We propose TF-GridNet for speech separation. The model is a novel deep neural network (DNN) integrating full- and sub-band modeling in the time-frequency (T-F) domain. It stacks several blocks, each consisting of an intra-frame full-band module, a sub-band temporal module, and a cross-frame self-attention module. It is trained to perform complex spectral mapping, where the real and imaginary (RI) components of input signals are stacked as features to predict target RI components. We first evaluate it on monaural anechoic speaker separation. Without using data augmentation and dynamic mixing, it obtains a state-of-the-art 23.5 dB improvement in scale-invariant signal-to-distortion ratio (SI-SDR) on WSJ0-2mix, a standard dataset for two-speaker separation. To show its robustness to noise and reverberation, we evaluate it on monaural reverberant speaker separation using the SMS-WSJ dataset and on noisy\u00a0\u2026",
            "url": "https://arxiv.org/pdf/2211.12433",
            "publicationVenue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing",
            "citation_count": 28
        },
        {
            "title": "Integration of Frame-and Label-synchronous Beam Search for Streaming Encoder-decoder Speech Recognition",
            "authors": "Emiru Tsunoo, Hayato Futami, Yosuke Kashiwagi, Siddhant Arora, Shinji Watanabe",
            "year": "2023",
            "abstract": "Although frame-based models, such as CTC and transducers, have an affinity for streaming automatic speech recognition, their decoding uses no future knowledge, which could lead to incorrect pruning. Conversely, label-based attention encoder-decoder mitigates this issue using soft attention to the input, while it tends to overestimate labels biased towards its training domain, unlike CTC. We exploit these complementary attributes and propose to integrate the frame- and label-synchronous (F-/L-Sync) decoding alternately performed within a single beam-search scheme. F-Sync decoding leads the decoding for block-wise processing, while L-Sync decoding provides the prioritized hypotheses using look-ahead future frames within a block. We maintain the hypotheses from both decoding methods to perform effective pruning. Experiments demonstrate that the proposed search algorithm achieves lower error rates compared to the other search methods, while being robust against out-of-domain situations.",
            "url": "https://arxiv.org/pdf/2307.12767",
            "publicationVenue": "arXiv preprint arXiv:2307.12767",
            "citation_count": 0
        },
        {
            "title": "Integrating pretrained ASR and LM to perform sequence generation for spoken language understanding",
            "authors": "Siddhant Arora, Hayato Futami, Yosuke Kashiwagi, Emiru Tsunoo, Brian Yan, Shinji Watanabe",
            "year": "2023",
            "abstract": "There has been an increased interest in the integration of pretrained speech recognition (ASR) and language models (LM) into the SLU framework. However, prior methods often struggle with a vocabulary mismatch between pretrained models, and LM cannot be directly utilized as they diverge from its NLU formulation. In this study, we propose a three-pass end-to-end (E2E) SLU system that effectively integrates ASR and LM subnetworks into the SLU formulation for sequence generation tasks. In the first pass, our architecture predicts ASR transcripts using the ASR subnetwork. This is followed by the LM subnetwork, which makes an initial SLU prediction. Finally, in the third pass, the deliberation subnetwork conditions on representations from the ASR and LM subnetworks to make the final prediction. Our proposed three-pass SLU system shows improved performance over cascaded and E2E SLU models on two benchmark SLU datasets, SLURP and SLUE, especially on acoustically challenging utterances.",
            "url": "https://arxiv.org/pdf/2307.11005",
            "publicationVenue": "arXiv preprint arXiv:2307.11005",
            "citation_count": 1
        },
        {
            "title": "BASS: Block-wise Adaptation for Speech Summarization",
            "authors": "Roshan Sharma, Kenneth Zheng, Siddhant Arora, Shinji Watanabe, Rita Singh, Bhiksha Raj",
            "year": "2023",
            "abstract": "End-to-end speech summarization has been shown to improve performance over cascade baselines. However, such models are difficult to train on very large inputs (dozens of minutes or hours) owing to compute restrictions and are hence trained with truncated model inputs. Truncation leads to poorer models, and a solution to this problem rests in block-wise modeling, i.e., processing a portion of the input frames at a time. In this paper, we develop a method that allows one to train summarization models on very long sequences in an incremental manner. Speech summarization is realized as a streaming process, where hypothesis summaries are updated every block based on new acoustic information. We devise and test strategies to pass semantic context across the blocks. Experiments on the How2 dataset demonstrate that the proposed block-wise training method improves by 3 points absolute on ROUGE-L over a truncated input baseline.",
            "url": "https://arxiv.org/pdf/2307.08217",
            "publicationVenue": "arXiv preprint arXiv:2307.08217",
            "citation_count": 0
        },
        {
            "title": "Legonn: Building modular encoder-decoder models",
            "authors": "Siddharth Dalmia, Dmytro Okhonko, Mike Lewis, Sergey Edunov, Shinji Watanabe, Florian Metze, Luke Zettlemoyer, Abdelrahman Mohamed",
            "year": "2023",
            "abstract": "State-of-the-art encoder-decoder models (e.g. for machine translation (MT) or automatic speech recognition (ASR)) are constructed and trained end-to-end as an atomic unit. No component of the model can be (re-)used without the others, making it impossible to share parts, e.g. a high resourced decoder, across tasks. We describe LegoNN, a procedure for building encoder-decoder architectures in a way so that its parts can be applied to other tasks without the need for any fine-tuning. To achieve this reusability, the interface between encoder and decoder modules is grounded to a sequence of marginal distributions over a pre-defined discrete vocabulary. We present two approaches for ingesting these marginals; one is differentiable, allowing the flow of gradients across the entire network, and the other is gradient-isolating. To enable the portability of decoder modules between MT tasks for different source\u00a0\u2026",
            "url": "https://arxiv.org/pdf/2206.03318",
            "publicationVenue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing",
            "citation_count": 9
        },
        {
            "title": "Deep Speech Synthesis from MRI-Based Articulatory Representations",
            "authors": "Peter Wu, Tingle Li, Yijing Lu, Yubin Zhang, Jiachen Lian, Alan W Black, Louis Goldstein, Shinji Watanabe, Gopala K Anumanchipalli",
            "year": "2023",
            "abstract": "In this paper, we study articulatory synthesis, a speech synthesis method using human vocal tract information that offers a way to develop efficient, generalizable and interpretable synthesizers. While recent advances have enabled intelligible articulatory synthesis using electromagnetic articulography (EMA), these methods lack critical articulatory information like excitation and nasality, limiting generalization capabilities. To bridge this gap, we propose an alternative MRI-based feature set that covers a much more extensive articulatory space than EMA. We also introduce normalization and denoising procedures to enhance the generalizability of deep learning methods trained on MRI data. Moreover, we propose an MRI-to-speech model that improves both computational efficiency and speech fidelity. Finally, through a series of ablations, we show that the proposed MRI representation is more comprehensive than EMA and identify the most suitable MRI feature subset for articulatory synthesis.",
            "url": "https://arxiv.org/pdf/2307.02471",
            "publicationVenue": "arXiv preprint arXiv:2307.02471",
            "citation_count": 5
        },
        {
            "title": "SigMoreFun submission to the SIGMORPHON shared task on interlinear glossing",
            "authors": "Taiqi He, Lindia Tjuatja, Nathaniel Robinson, Shinji Watanabe, David R Mortensen, Graham Neubig, Lori Levin",
            "year": "2023",
            "abstract": "In our submission to the SIGMORPHON 2023 Shared Task on interlinear glossing (IGT), we explore approaches to data augmentation and modeling across seven low-resource languages. For data augmentation, we explore two approaches: creating artificial data from the provided training data and utilizing existing IGT resources in other languages. On the modeling side, we test an enhanced version of the provided token classification baseline as well as a pretrained multilingual seq2seq model. Additionally, we apply post-correction using a dictionary for Gitksan, the language with the smallest amount of data. We find that our token classification models are the best performing, with the highest word-level accuracy for Arapaho and highest morpheme-level accuracy for Gitksan out of all submissions. We also show that data augmentation is an effective strategy, though applying artificial data pretraining has very different effects across both models tested.",
            "url": "https://aclanthology.org/2023.sigmorphon-1.22.pdf",
            "publicationVenue": "Proceedings of the 20th SIGMORPHON workshop on Computational Research in Phonetics, Phonology, and Morphology",
            "citation_count": 2
        },
        {
            "title": "Cmu\u2019s iwslt 2023 simultaneous speech translation system",
            "authors": "Brian Yan, Jiatong Shi, Soumi Maiti, William Chen, Xinjian Li, Yifan Peng, Siddhant Arora, Shinji Watanabe",
            "year": "2023",
            "abstract": "This paper describes CMU\u2019s submission to the IWSLT 2023 simultaneous speech translation shared task for translating English speech to both German text and speech in a streaming fashion. We first build offline speech-to-text (ST) models using the joint CTC/attention framework. These models also use WavLM front-end features and mBART decoder initialization. We adapt our offline ST models for simultaneous speech-to-text translation (SST) by 1) incrementally encoding chunks of input speech, re-computing encoder states for each new chunk and 2) incrementally decoding output text, pruning beam search hypotheses to 1-best after processing each chunk. We then build text-to-speech (TTS) models using the VITS framework and achieve simultaneous speech-to-speech translation (SS2ST) by cascading our SST and TTS models.",
            "url": "https://aclanthology.org/2023.iwslt-1.20.pdf",
            "publicationVenue": "Proceedings of the 20th International Conference on Spoken Language Translation (IWSLT 2023)",
            "citation_count": 3
        },
        {
            "title": "Improving Speech Enhancement Performance by Leveraging Contextual Broad Phonetic Class Information",
            "authors": "Yen-Ju Lu, Chia-Yu Chang, Cheng Yu, Ching-Feng Liu, Jeih-weih Hung, Shinji Watanabe, Yu Tsao",
            "year": "2023",
            "abstract": "Previous studies have confirmed that by augmenting acoustic features with the place/manner of articulatory features, the speech enhancement (SE) process can be guided to consider the broad phonetic properties of the input speech when performing enhancement to attain performance improvements. In this article, we explore the contextual information of articulatory attributes as additional information to further benefit SE. More specifically, we propose to improve the SE performance by leveraging losses from an end-to-end automatic speech recognition (E2E-ASR) model that predicts the sequence of broad phonetic classes (BPCs). We also developed multi-objective training with ASR and perceptual losses to train the SE system based on a BPC-based E2E-ASR. Experimental results from speech denoising, speech dereverberation, and impaired speech enhancement tasks confirmed that contextual BPC\u00a0\u2026",
            "url": "https://ieeexplore.ieee.org/iel7/6570655/6633080/10164201.pdf",
            "publicationVenue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing",
            "citation_count": 0
        },
        {
            "title": "The CHiME-7 DASR Challenge: Distant Meeting Transcription with Multiple Devices in Diverse Scenarios",
            "authors": "Samuele Cornell, Matthew Wiesner, Shinji Watanabe, Desh Raj, Xuankai Chang, Paola Garcia, Yoshiki Masuyama, Zhong-Qiu Wang, Stefano Squartini, Sanjeev Khudanpur",
            "year": "2023",
            "abstract": "The CHiME challenges have played a significant role in the development and evaluation of robust speech recognition (ASR) systems. We introduce the CHiME-7 distant ASR (DASR) task, within the 7th CHiME challenge. This task comprises joint ASR and diarization in far-field settings with multiple, and possibly heterogeneous, recording devices. Different from previous challenges, we evaluate systems on 3 diverse scenarios: CHiME-6, DiPCo, and Mixer 6. The goal is for participants to devise a single system that can generalize across different array geometries and use cases with no a-priori information. Another departure from earlier CHiME iterations is that participants are allowed to use open-source pre-trained models and datasets. In this paper, we describe the challenge design, motivation, and fundamental research questions in detail. We also present the baseline system, which is fully array-topology agnostic and features multi-channel diarization, channel selection, guided source separation and a robust ASR model that leverages self-supervised speech representations (SSLR).",
            "url": "https://arxiv.org/pdf/2306.13734",
            "publicationVenue": "arXiv preprint arXiv:2306.13734",
            "citation_count": 20
        },
        {
            "title": "Reducing Barriers to Self-Supervised Learning: HuBERT Pre-training with Academic Compute",
            "authors": "William Chen, Xuankai Chang, Yifan Peng, Zhaoheng Ni, Soumi Maiti, Shinji Watanabe",
            "year": "2023",
            "abstract": "Self-supervised learning (SSL) has led to great strides in speech processing. However, the resources needed to train these models has become prohibitively large as they continue to scale. Currently, only a few groups with substantial resources are capable of creating SSL models, which harms reproducibility. In this work, we optimize HuBERT SSL to fit in academic constraints. We reproduce HuBERT independently from the original implementation, with no performance loss. Our code and training optimizations make SSL feasible with only 8 GPUs, instead of the 32 used in the original work. We also explore a semi-supervised route, using an ASR model to skip the first pre-training iteration. Within one iteration of pre-training, our models improve over HuBERT on several tasks. Furthermore, our HuBERT Large variant requires only 8 GPUs, achieving similar performance to the original trained on 128. As our contribution to the community, all models, configurations, and code are made open-source in ESPnet.",
            "url": "https://arxiv.org/pdf/2306.06672",
            "publicationVenue": "arXiv preprint arXiv:2306.06672",
            "citation_count": 12
        },
        {
            "title": "Multi-Channel Speaker Extraction with Adversarial Training: The Wavlab Submission to The Clarity ICASSP 2023 Grand Challenge",
            "authors": "Samuele Cornell, Zhong-Qiu Wang, Yoshiki Masuyama, Shinji Watanabe, Manuel Pariente, Nobutaka Ono, Stefano Squartini",
            "year": "2023",
            "abstract": "In this work we detail our submission to the Clarity ICASSP 2023 grand challenge, in which participants have to develop a strong target speech enhancement system for hearing-aid (HA) devices in noisy-reverberant environments. Our system builds on our previous submission at the Second Clarity Enhancement Challenge (CEC2): iNeuBe-X, which consists in an iterative neural/conventional beamforming enhancement pipeline, guided by an enrollment utterance from the target speaker. This model, which won by a large margin the CEC2, is an extension of the state-of-the-art TF-GridNet model for multi-channel, streamable target-speaker speech enhancement. Here, this approach is extended and further improved by leveraging generative adversarial training, which we show proves especially useful when the training data is limited. Using only the official 6k training scenes data, our best model achieves 0.80\u00a0\u2026",
            "url": "https://zqwang7.github.io/publications/Multi-Channel_Speaker_Extraction_with_Adversarial_Training_The_Wavlab_Submission_to_The_Clarity_ICASSP_2023_Grand_Challenge.pdf",
            "publicationVenue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
            "citation_count": 1
        },
        {
            "title": "FindAdaptNet: Find and Insert Adapters by Learned Layer Importance",
            "authors": "Junwei Huang, Karthik Ganesan, Soumi Maiti, Young Min Kim, Xuankai Chang, Paul Liang, Shinji Watanabe",
            "year": "2023",
            "abstract": "Adapters are lightweight bottleneck modules introduced to assist pre-trained self-supervised learning (SSL) models to be customized to new tasks. However, searching the appropriate layers to insert adapters on large models has become difficult due to the large number of possible layers and thus a vast search space (2Npossibilities for N layers). In this paper, we propose a technique that achieves automatic insertion of adapters for downstream automatic speech recognition (ASR) and spoken language understanding (SLU) tasks. Our approach is based on two-stage training. First, we train our model for a specific downstream task with additional shallow learnable layers and weight parameters to obtain the weighted summation over the output of each layer in SSL. This training method is established by the SUPERB baseline [1]. This first-stage training determines the most important layers given their respective\u00a0\u2026",
            "url": null,
            "publicationVenue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
            "citation_count": 3
        },
        {
            "title": "Fully Unsupervised Topic Clustering of Unlabelled Spoken Audio Using Self-Supervised Representation Learning and Topic Model",
            "authors": "Takashi Maekaku, Yuya Fujita, Xuankai Chang, Shinji Watanabe",
            "year": "2023",
            "abstract": "Unsupervised topic clustering of spoken audio is an important research topic for zero-resourced unwritten languages. A classical approach is to find a set of spoken terms from only the audio based on dynamic time warping or generative modeling (e.g., hidden Markov model), and apply a topic model to classify topics. The spoken term discovery is the most important and difficult part. In this paper, we propose to combine self-supervised representation learning (SSRL) methods as a component of spoken term discovery and probabilistic topic models. Most SSRL methods pre-train a model which predicts high-quality pseudo labels generated from an audio-only corpus. These pseudo labels can be used to produce a sequence of pseudo subwords by applying deduplication and a subword model. Then, we apply a topic model based on latent Dirichlet allocation for these pseudo-subword sequences in an unsupervised\u00a0\u2026",
            "url": null,
            "publicationVenue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
            "citation_count": 1
        },
        {
            "title": "Speech Summarization of Long Spoken Document: Improving Memory Efficiency of Speech/Text Encoders",
            "authors": "Takatomo Kano, Atsunori Ogawa, Marc Delcroix, Roshan Sharma, Kohei Matsuura, Shinji Watanabe",
            "year": "2023",
            "abstract": "Speech summarization requires processing several minute-long speech sequences to allow exploiting the whole context of a spoken document. A conventional approach is a cascade of automatic speech recognition (ASR) and text summarization (TS). However, the cascade systems are sensitive to ASR errors. Moreover, the cascade system cannot be optimized for input speech and utilize para-linguistic information. Recently, there has been an increased interest in end-to-end (E2E) approaches optimized to output summaries directly from speech. Such systems can thus mitigate the ASR errors of cascade approaches. However, E2E speech summarization requires massive computational resources because it needs to encode long speech sequences. We propose a speech summarization system that enables E2E summarization from 100 seconds, which is the limit of the conventional method, to up to 10 minutes (i\u00a0\u2026",
            "url": null,
            "publicationVenue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
            "citation_count": 3
        },
        {
            "title": "E-Branchformer-Based E2E SLU Toward Stop on-Device Challenge",
            "authors": "Yosuke Kashiwagi, Siddhant Arora, Hayato Futami, Jessica Huynh, Shih-Lun Wu, Yifan Peng, Brian Yan, Emiru Tsunoo, Shinji Watanabe",
            "year": "2023",
            "abstract": "In this paper, we report our team\u2019s study on track 2 of the Spoken Language Understanding Grand Challenge, which is a component of the ICASSP Signal Processing Grand Challenge 2023. The task is intended for on-device processing and involves estimating semantic parse labels from speech using a model with 15 million parameters. We use E2E E-Branchformer-based spoken language understanding model, which is more parameter controllable than cascade models, and reduced the parameter size through sequential distillation and tensor decomposition techniques. On the STOP dataset, we achieved an exact match accuracy of 70.9% under the tight constraint of 15 million parameters.",
            "url": null,
            "publicationVenue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
            "citation_count": 0
        },
        {
            "title": "A Study on the Integration of Pipeline and E2E SLU Systems for Spoken Semantic Parsing Toward Stop Quality Challenge",
            "authors": "Siddhant Arora, Hayato Futami, Shih-Lun Wu, Jessica Huynh, Yifan Peng, Yosuke Kashiwagi, Emiru Tsunoo, Brian Yan, Shinji Watanabe",
            "year": "2023",
            "abstract": "Recently there have been efforts to introduce new benchmark tasks for spoken language understanding (SLU), like semantic parsing. In this paper, we describe our proposed spoken semantic parsing system for the quality track (Track 1) in Spoken Language Understanding Grand Challenge which is part of ICASSP Signal Processing Grand Challenge 2023. We experiment with both end-to-end and pipeline systems for this task. Strong automatic speech recognition (ASR) models like Whisper and pretrained Language models (LM) like BART are utilized inside our SLU framework to boost performance. We also investigate the output level combination of various models to get an exact match accuracy of 80.8, which won the 1st place at the challenge.",
            "url": "https://arxiv.org/pdf/2305.01620",
            "publicationVenue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
            "citation_count": 3
        },
        {
            "title": "The Pipeline System of ASR and NLU with MLM-based data Augmentation Toward Stop Low-Resource Challenge",
            "authors": "Hayato Futami, Jessica Huynh, Siddhant Arora, Shih-Lun Wu, Yosuke Kashiwagi, Yifan Peng, Brian Yan, Emiru Tsunoo, Shinji Watanabe",
            "year": "2023",
            "abstract": "This paper describes our system for the low-resource domain adaptation track (Track 3) in Spoken Language Understanding Grand Challenge, which is a part of ICASSP Signal Processing Grand Challenge 2023. In the track, we adopt a pipeline approach of ASR and NLU. For ASR, we fine-tune Whisper for each domain with upsampling. For NLU, we fine-tune BART on all the Track3 data and then on low-resource domain data. We apply masked LM (MLM) -based data augmentation, where some of input tokens and corresponding target labels are replaced using MLM. We also apply a retrieval-based approach, where model input is augmented with similar training samples. As a result, we achieved exact match (EM) accuracy 63.3/75.0 (average: 69.15) for reminder/weather domain, and won the 1st place at the challenge.",
            "url": "https://arxiv.org/pdf/2305.01194",
            "publicationVenue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
            "citation_count": 1
        },
        {
            "title": "Joint Modelling of Spoken Language Understanding Tasks with Integrated Dialog History",
            "authors": "Siddhant Arora, Hayato Futami, Emiru Tsunoo, Brian Yan, Shinji Watanabe",
            "year": "2023",
            "abstract": "Most human interactions occur in the form of spoken conversations where the semantic meaning of a given utterance depends on the context. Each utterance in spoken conversation can be represented by many semantic and speaker attributes, and there has been an interest in building Spoken Language Understanding (SLU) systems for automatically predicting these attributes. Recent work has shown that incorporating dialogue history can help advance SLU performance. However, separate models are used for each SLU task, leading to an increase in inference time and computation cost. Motivated by this, we aim to ask: can we jointly model all the SLU tasks while incorporating context to facilitate low-latency and lightweight inference? To answer this, we propose a novel model architecture that learns dialog context to jointly predict the intent, dialog act, speaker role, and emotion for the spoken utterance. Note\u00a0\u2026",
            "url": "https://arxiv.org/pdf/2305.00926",
            "publicationVenue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
            "citation_count": 3
        },
        {
            "title": "I3D: Transformer architectures with input-dependent dynamic depth for speech recognition",
            "authors": "Yifan Peng, Jaesong Lee, Shinji Watanabe",
            "year": "2023",
            "abstract": "Transformer-based end-to-end speech recognition has achieved great success. However, the large footprint and computational overhead make it difficult to deploy these models in some real-world applications. Model compression techniques can reduce the model size and speed up inference, but the compressed model has a fixed architecture which might be suboptimal. We propose a novel Transformer encoder with Input-Dependent Dynamic Depth (I3D) to achieve strong performance-efficiency trade-offs. With a similar number of layers at inference time, I3D-based models outperform the vanilla Transformer and the static pruned model via iterative layer pruning. We also present interesting analysis on the gate probabilities and the input-dependency, which helps us better understand deep encoders.",
            "url": "https://arxiv.org/pdf/2303.07624",
            "publicationVenue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
            "citation_count": 7
        },
        {
            "title": "FNeural Speech Enhancement with Very Low Algorithmic Latency and Complexity via Integrated full-and sub-band Modeling",
            "authors": "Zhong-Qiu Wang, Samuele Cornell, Shukjae Choi, Younglo Lee, Byeong-Yeol Kim, Shinji Watanabe",
            "year": "2023",
            "abstract": "We propose FSB-LSTM, a novel long short-term memory (LSTM) based architecture that integrates full- and sub-band (FSB) modeling, for single- and multi-channel speech enhancement in the short-time Fourier transform (STFT) domain. The model maintains an information highway to flow an over-complete input representation through multiple FSB-LSTM modules. Each FSB-LSTM module consists of a full-band block to model spectro-temporal patterns at all frequencies and a sub-band block to model patterns within each sub-band, where each of the two blocks takes a down-sampled representation as input and returns an up-sampled discriminative representation to be added to the block input via a residual connection. The model is designed to have a low algorithmic complexity, a small run-time buffer and a very low algorithmic latency, at the same time producing a strong enhancement performance on a noisy\u00a0\u2026",
            "url": "https://arxiv.org/pdf/2304.08707",
            "publicationVenue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
            "citation_count": 7
        },
        {
            "title": "Enhancing Speech-To-Speech Translation with Multiple TTS Targets",
            "authors": "Jiatong Shi, Yun Tang, Ann Lee, Hirofumi Inaguma, Changhan Wang, Juan Pino, Shinji Watanabe",
            "year": "2023",
            "abstract": "It has been known that direct speech-to-speech translation (S2ST) models usually suffer from the data scarcity issue because of the limited existing parallel materials for both source and target speech. Therefore to train a direct S2ST system, previous works usually utilize text-to-speech (TTS) systems to generate samples in the target language by augmenting the data from speech-to-text translation (S2TT). However, there is a limited investigation into how the synthesized target speech would affect the S2ST models. In this work, we analyze the effect of changing synthesized target speech for direct S2ST models. We find that simply combining the target speech from different TTS systems can potentially improve the S2ST performances. Following that, we also propose a multi-task framework that jointly optimizes the S2ST system with multiple targets from different TTS systems. Extensive experiments demonstrate that\u00a0\u2026",
            "url": "https://arxiv.org/pdf/2304.04618",
            "publicationVenue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
            "citation_count": 1
        },
        {
            "title": "The multimodal information based speech processing (misp) 2022 challenge: Audio-visual diarization and recognition",
            "authors": "Zhe Wang, Shilong Wu, Hang Chen, Mao-Kui He, Jun Du, Chin-Hui Lee, Jingdong Chen, Shinji Watanabe, Sabato Siniscalchi, Odette Scharenborg, Diyuan Liu, Baocai Yin, Jia Pan, Jianqing Gao, Cong Liu",
            "year": "2023",
            "abstract": "The Multi-modal Information based Speech Processing (MISP) challenge aims to extend the application of signal processing technology in specific scenarios by promoting the research into wake-up words, speaker diarization, speech recognition, and other technologies. The MISP2022 challenge has two tracks: 1) audio-visual speaker diarization (AVSD), aiming to solve \"who spoken when\" using both audio and visual data; 2) a novel audio-visual diarization and recognition (AVDR) task that focuses on addressing \"who spoken what when\" with audio-visual speaker diarization results. Both tracks focus on the Chinese language, and use far-field audio and video in real home-tv scenarios: 2-6 people communicating each other with TV noise in the background. This paper introduces the dataset, track settings, and baselines of the MISP2022 challenge. Our analyses of experiments and examples indicate the good\u00a0\u2026",
            "url": "https://arxiv.org/pdf/2303.06326",
            "publicationVenue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
            "citation_count": 9
        },
        {
            "title": "Structured Pruning of Self-Supervised Pre-Trained Models for Speech Recognition and Understanding",
            "authors": "Yifan Peng, Kwangyoun Kim, Felix Wu, Prashant Sridhar, Shinji Watanabe",
            "year": "2023",
            "abstract": "Self-supervised speech representation learning (SSL) has shown to be effective in various downstream tasks, but SSL models are usually large and slow. Model compression techniques such as pruning aim to reduce the model size and computation without degradation in accuracy. Prior studies focus on the pruning of Transformers; however, speech models not only utilize a stack of Transformer blocks, but also combine a frontend network based on multiple convolutional layers for low-level feature representation learning. This frontend has a small size but a heavy computational cost. In this work, we propose three task-specific structured pruning methods to deal with such heterogeneous networks. Experiments on LibriSpeech and SLURP show that the proposed method is more accurate than the original wav2vec2-base with 10% to 30% less computation, and is able to reduce the computation by 40% to 50\u00a0\u2026",
            "url": "https://arxiv.org/pdf/2302.14132",
            "publicationVenue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
            "citation_count": 11
        },
        {
            "title": "Improving massively multilingual asr with auxiliary ctc objectives",
            "authors": "William Chen, Brian Yan, Jiatong Shi, Yifan Peng, Soumi Maiti, Shinji Watanabe",
            "year": "2023",
            "abstract": "Multilingual Automatic Speech Recognition (ASR) models have extended the usability of speech technologies to a wide variety of languages. With how many languages these models have to handle, however, a key to understanding their imbalanced performance across different languages is to examine if the model actually knows which language it should transcribe. In this paper, we introduce our work on improving performance on FLEURS, a 102-language open ASR benchmark, by conditioning the entire model on language identity (LID). We investigate techniques inspired from recent Connectionist Temporal Classification (CTC) studies to help the model handle the large number of languages, conditioning on the LID predictions of auxiliary tasks. Our experimental results demonstrate the effectiveness of our technique over standard CTC/Attention-based hybrid models. Furthermore, our state-of-the-art systems\u00a0\u2026",
            "url": "https://arxiv.org/pdf/2302.12829",
            "publicationVenue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
            "citation_count": 19
        },
        {
            "title": "Paaploss: A Phonetic-Aligned Acoustic Parameter Loss for Speech Enhancement",
            "authors": "Muqiao Yang, Joseph Konan, David Bick, Yunyang Zeng, Shuo Han, Anurag Kumar, Shinji Watanabe, Bhiksha Raj",
            "year": "2023",
            "abstract": "Despite rapid advancement in recent years, current speech enhancement models often produce speech that differs in perceptual quality from real clean speech. We propose a learning objective that formalizes differences in perceptual quality, by using domain knowledge of acoustic-phonetics. We identify temporal acoustic parameters \u2013 such as spectral tilt, spectral flux, shimmer, etc. \u2013 that are non-differentiable, and we develop a neural network estimator that can accurately predict their time-series values across an utterance. We also model phoneme-specific weights for each feature, as the acoustic parameters are known to show different behavior in different phonemes. We can add this criterion as an auxiliary loss to any model that produces speech, to optimize speech outputs to match the values of clean speech in these features. Experimentally we show that it improves speech enhancement workflows in both\u00a0\u2026",
            "url": "https://arxiv.org/pdf/2302.08095",
            "publicationVenue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
            "citation_count": 5
        },
        {
            "title": "Context-Aware Fine-Tuning of Self-Supervised Speech Models",
            "authors": "Suwon Shon, Felix Wu, Kwangyoun Kim, Prashant Sridhar, Karen Livescu, Shinji Watanabe",
            "year": "2023",
            "abstract": "Self-supervised pre-trained transformers have improved the state of the art on a variety of speech tasks. Due to the quadratic time and space complexity of self-attention, they usually operate at the level of relatively short (e.g., utterance) segments. In this paper, we study the use of context, i.e., surrounding segments, during fine-tuning and propose a new approach called context-aware fine-tuning. We attach a context module on top of the last layer of a pre-trained model to encode the whole segment into a context embedding vector which is then used as an additional feature for the final prediction. During the fine-tuning stage, we introduce an auxiliary loss that encourages this context embedding vector to be similar to context vectors of surrounding segments. This allows the model to make predictions without access to these surrounding segments at inference time and requires only a tiny overhead compared to\u00a0\u2026",
            "url": "https://arxiv.org/pdf/2212.08542",
            "publicationVenue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
            "citation_count": 6
        },
        {
            "title": "SpeechLMScore: Evaluating speech generation using speech language model",
            "authors": "Soumi Maiti, Yifan Peng, Takaaki Saeki, Shinji Watanabe",
            "year": "2023",
            "abstract": "While human evaluation is the most reliable metric for evaluating speech generation systems, it is generally costly and time-consuming. Previous studies on automatic speech quality assessment address the problem by predicting human evaluation scores with machine learning models. However, they rely on supervised learning and thus suffer from high annotation costs and domain-shift problems. We propose SpeechLMScore, an unsupervised metric to evaluate generated speech using a speech language model. SpeechLMScore computes the average log-probability of a speech signal by mapping it into discrete tokens and measures the average probability of generating the sequence of tokens. Therefore, it does not require human annotation and is a highly scalable framework. Evaluation results demonstrate that the proposed metric shows a promising correlation with human evaluation scores on different\u00a0\u2026",
            "url": "https://arxiv.org/pdf/2212.04559",
            "publicationVenue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
            "citation_count": 12
        },
        {
            "title": "Euro: Espnet Unsupervised ASR Open-Source Toolkit",
            "authors": "Dongji Gao, Jiatong Shi, Shun-Po Chuang, Leibny Paola Garcia, Hung-yi Lee, Shinji Watanabe, Sanjeev Khudanpur",
            "year": "2023",
            "abstract": "This paper describes the ESPnet Unsupervised ASR Open-source Toolkit (EURO), an end-to-end open-source toolkit for unsupervised automatic speech recognition (UASR). EURO adopts the state-of-the-art UASR learning method introduced by the Wav2vec-U, originally implemented at FAIRSEQ, which leverages self-supervised speech representations and adversarial training. In addition to wav2vec2, EURO extends the functionality and promotes reproducibility for UASR tasks by integrating S3PRL and k2, resulting in flexible frontends from 27 self-supervised models and various graph-based decoding strategies. EURO is implemented in ESPnet and follows its unified pipeline to provide UASR recipes with a complete setup. This improves the pipeline\u2019s efficiency and allows EURO to be easily applied to existing datasets in ESPnet. Extensive experiments on three mainstream self-supervised models\u00a0\u2026",
            "url": "https://arxiv.org/pdf/2211.17196",
            "publicationVenue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
            "citation_count": 4
        },
        {
            "title": "Streaming Joint Speech Recognition and Disfluency Detection",
            "authors": "Hayato Futami, Emiru Tsunoo, Kentaro Shibata, Yosuke Kashiwagi, Takao Okuda, Siddhant Arora, Shinji Watanabe",
            "year": "2023",
            "abstract": "Disfluency detection has mainly been solved in a pipeline approach, as post-processing of speech recognition. In this study, we propose Transformer-based encoder-decoder models that jointly solve speech recognition and disfluency detection, which work in a streaming manner. Compared to pipeline approaches, the joint models can leverage acoustic information that makes disfluency detection robust to recognition errors and provide non-verbal clues. Moreover, joint modeling results in low-latency and lightweight inference. We investigate two joint model variants for streaming disfluency detection: a transcript-enriched model and a multi-task model. The transcript- enriched model is trained on text with special tags indicating the starting and ending points of the disfluent part. However, it has problems with latency and standard language model adaptation, which arise from the additional disfluency tags. We\u00a0\u2026",
            "url": "https://arxiv.org/pdf/2211.08726",
            "publicationVenue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
            "citation_count": 3
        },
        {
            "title": "Avoid Overthinking in Self-Supervised Models for Speech Recognition",
            "authors": "Dan Berrebbi, Brian Yan, Shinji Watanabe",
            "year": "2023",
            "abstract": "Self-supervised learning (SSL) models reshaped our approach to speech, language and vision. However their huge size and the opaque relations between their layers and tasks result in slow inference and network overthinking, where predictions made from the last layer of large models is worse than those made from intermediate layers. Early exit (EE) strategies can solve both issues by dynamically reducing computations at inference time for certain samples. Although popular for classification tasks in vision and language, EE has seen less use for sequence-to-sequence speech recognition (ASR) tasks where outputs from early layers are often degenerate. This challenge is further compounded when speech SSL models are applied on out-of-distribution (OOD) data. This paper first shows that SSL models do overthinking in ASR. We then motivate further research in EE by computing an optimal bound for\u00a0\u2026",
            "url": "https://arxiv.org/pdf/2211.08989",
            "publicationVenue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
            "citation_count": 3
        },
        {
            "title": "A unified one-shot prosody and speaker conversion system with self-supervised discrete speech units",
            "authors": "Li-Wei Chen, Shinji Watanabe, Alexander Rudnicky",
            "year": "2023",
            "abstract": "We present a unified system to realize one-shot voice conversion (VC) on the pitch, rhythm, and speaker attributes. Existing works generally ignore the correlation between prosody and language content, leading to the degradation of naturalness in converted speech. Additionally, the lack of proper language features prevents these systems from accurately preserving language content after conversion. To address these issues, we devise a cascaded modular system leveraging self-supervised discrete speech units as language representation. These discrete units provide duration information essential for rhythm modeling. Our system first extracts utterance-level prosody and speaker representations from the raw waveform. Given the prosody representation, a prosody predictor estimates pitch, energy, and duration for each discrete unit in the utterance. A synthesizer further reconstructs speech based on the\u00a0\u2026",
            "url": "https://arxiv.org/pdf/2211.06535",
            "publicationVenue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
            "citation_count": 2
        },
        {
            "title": "Align, Write, Re-Order: Explainable End-to-End Speech Translation via Operation Sequence Generation",
            "authors": "Motoi Omachi, Brian Yan, Siddharth Dalmia, Yuya Fujita, Shinji Watanabe",
            "year": "2023",
            "abstract": "The black-box nature of end-to-end speech-to-text translation (E2E ST) makes it difficult to understand how source language inputs are being mapped to the target language. To solve this problem, we propose to simultaneously generate automatic speech recognition (ASR) and ST predictions such that each source language word is explicitly mapped to a target language word. A major challenge arises from the fact that translation is a non-monotonic sequence transduction task due to word ordering differences between languages \u2013 this clashes with the monotonic nature of ASR. Therefore, we propose to generate ST tokens out-of-order while remembering how to re-order them later. We achieve this by predicting a sequence of tuples consisting of a source word, the corresponding target words, and post-editing operations dictating the correct insertion points for the target word. We examine two variants of such\u00a0\u2026",
            "url": "https://arxiv.org/pdf/2211.05967",
            "publicationVenue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
            "citation_count": 2
        },
        {
            "title": "Bridging Speech and Textual Pre-Trained Models With Unsupervised ASR",
            "authors": "Jiatong Shi, Chan-Jan Hsu, Holam Chung, Dongji Gao, Paola Garcia, Shinji Watanabe, Ann Lee, Hung-yi Lee",
            "year": "2023",
            "abstract": "Spoken language understanding (SLU) is a task aiming to extract high-level semantics from spoken utterances. Previous works have investigated the use of speech self-supervised models and textual pre-trained models, which have shown reasonable improvements to various SLU tasks. However, because of the mismatched modalities between speech signals and text tokens, previous methods usually need complex designs of the frameworks. This work proposes a simple yet efficient unsupervised paradigm that connects speech and textual pre-trained models, resulting in an unsupervised speech-to-semantic pre-trained model for various tasks in SLU. To be specific, we propose to use unsupervised automatic speech recognition (ASR) as a connector that bridges different modalities used in speech and textual pre-trained models. Our experiments show that unsupervised ASR itself can improve the\u00a0\u2026",
            "url": "https://arxiv.org/pdf/2211.03025",
            "publicationVenue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
            "citation_count": 2
        },
        {
            "title": "Towards Zero-Shot Code-Switched Speech Recognition",
            "authors": "Brian Yan, Matthew Wiesner, Ond\u0159ej Klejch, Preethi Jyothi, Shinji Watanabe",
            "year": "2023",
            "abstract": "In this work, we seek to build effective code-switched (CS) automatic speech recognition systems (ASR) under the zero-shot set-ting where no transcribed CS speech data is available for training. Previously proposed frameworks which conditionally factorize the bilingual task into its constituent monolingual parts are a promising starting point for leveraging monolingual data efficiently. However, these methods require the monolingual modules to perform language segmentation. That is, each monolingual module has to simultaneously detect CS points and transcribe speech segments of one language while ignoring those of other languages \u2013 not a trivial task. We propose to simplify each monolingual module by allowing them to transcribe all speech segments indiscriminately with a monolingual script (i.e. transliteration). This simple modification passes the responsibility of CS point detection to subsequent bilingual\u00a0\u2026",
            "url": "https://arxiv.org/pdf/2211.01458",
            "publicationVenue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
            "citation_count": 7
        },
        {
            "title": "Bectra: Transducer-based end-to-end asr with bert-enhanced encoder",
            "authors": "Yosuke Higuchi, Tetsuji Ogawa, Tetsunori Kobayashi, Shinji Watanabe",
            "year": "2023",
            "abstract": "We present BERT-CTC-Transducer (BECTRA), a novel end-to-end automatic speech recognition (E2E-ASR) model formulated by the transducer with a BERT-enhanced encoder. Integrating a large-scale pre-trained language model (LM) into E2E-ASR has been actively studied, aiming to utilize versatile linguistic knowledge for generating accurate text. One crucial factor that makes this integration challenging lies in the vocabulary mismatch; the vocabulary constructed for a pre-trained LM is generally too large for E2E-ASR training and is likely to have a mismatch against a target ASR domain. To overcome such an issue, we propose BECTRA, an extended version of our previous BERT-CTC, that realizes BERT-based E2E-ASR using a vocabulary of interest. BECTRA is a transducer-based model, which adopts BERT-CTC for its encoder and trains an ASR-specific decoder using a vocabulary suitable for a target\u00a0\u2026",
            "url": "https://arxiv.org/pdf/2211.00792",
            "publicationVenue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
            "citation_count": 9
        },
        {
            "title": "Intermpl: Momentum Pseudo-Labeling With Intermediate CTC Loss",
            "authors": "Yosuke Higuchi, Tetsuji Ogawa, Tetsunori Kobayashi, Shinji Watanabe",
            "year": "2023",
            "abstract": "This paper presents InterMPL, a semi-supervised learning method of end-to-end automatic speech recognition (ASR) that performs pseudo-labeling (PL) with intermediate supervision. Momentum PL (MPL) trains a connectionist temporal classification (CTC)-based model on unlabeled data by continuously generating pseudo-labels on the fly and improving their quality. In contrast to autoregressive formulations, such as the attention-based encoder-decoder and transducer, CTC is well suited for MPL, or PL-based semi-supervised ASR in general, owing to its simple/fast inference algorithm and robustness against generating collapsed labels. However, CTC generally yields inferior performance than the autoregressive models due to the conditional independence assumption, thereby limiting the performance of MPL. We propose to enhance MPL by introducing intermediate loss, inspired by the recent advances in\u00a0\u2026",
            "url": "https://arxiv.org/pdf/2211.00795",
            "publicationVenue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
            "citation_count": 0
        },
        {
            "title": "Articulatory Representation Learning via Joint Factor Analysis and Neural Matrix Factorization",
            "authors": "Jiachen Lian, Alan W Black, Yijing Lu, Louis Goldstein, Shinji Watanabe, Gopala K Anumanchipalli",
            "year": "2023",
            "abstract": "Articulatory representation learning is the fundamental research in modeling neural speech production system. Our previous work has established a deep paradigm to decompose the articulatory kinematics data into gestures, which explicitly model the phonological and linguistic structure encoded with human speech production mechanism, and corresponding gestural scores. We continue with this line of work by raising two concerns: (1) The articulators are entangled together in the original algorithm such that some of the articulators do not leverage effective moving patterns, which limits the interpretability of both gestures and gestural scores; (2) The EMA data is sparsely sampled from articulators, which limits the intelligibility of learned representations. In this work, we propose a novel articulatory representation decomposition algorithm that takes the advantage of guided factor analysis to derive the articulatory\u00a0\u2026",
            "url": "https://arxiv.org/pdf/2210.16498",
            "publicationVenue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
            "citation_count": 3
        },
        {
            "title": "In search of strong embedding extractors for speaker diarisation",
            "authors": "Jee-weon Jung, Hee-Soo Heo, Bong-Jin Lee, Jaesung Huh, Andrew Brown, Youngki Kwon, Shinji Watanabe, Joon Son Chung",
            "year": "2023",
            "abstract": "Speaker embedding extractors (EEs), which map input audio to a speaker discriminant latent space, are of paramount importance in speaker diarisation. However, there are several challenges when adopting EEs for diarisation, from which we tackle two key problems. First, the evaluation is not straightforward because the required features differ between speaker verification and diarisation. We show that better performance on widely adopted speaker verification evaluation protocols does not lead to better diarisation performance. Second, embedding extractors have not seen utterances in which multiple speakers exist. These inputs are inevitably present in speaker diarisation because of overlapped speech and speaker changes; they degrade the performance. To mitigate the first problem, we generate speaker verification evaluation protocols that better mimic the diarisation scenario. We propose two data\u00a0\u2026",
            "url": "https://arxiv.org/pdf/2210.14682",
            "publicationVenue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
            "citation_count": 8
        },
        {
            "title": "TF-GridNet: Making time-frequency domain models great again for monaural speaker separation",
            "authors": "Zhong-Qiu Wang, Samuele Cornell, Shukjae Choi, Younglo Lee, Byeong-Yeol Kim, Shinji Watanabe",
            "year": "2023",
            "abstract": "We propose TF-GridNet, a novel multi-path deep neural network (DNN) operating in the time-frequency (T-F) domain, for monaural talker-independent speaker separation in anechoic conditions. The model stacks several multi-path blocks, each consisting of an intra-frame spectral module, a sub-band temporal module, and a full-band self-attention module, to leverage local and global spectro-temporal information for separation. The model is trained to perform complex spectral mapping, where the real and imaginary (RI) components of the input mixture are stacked as input features to predict target RI components. Besides using the scale-invariant signal-to-distortion ratio (SI-SDR) loss for model training, we include a novel loss term to encourage separated sources to add up to the input mixture. Without using dynamic mixing, we obtain 23.4 dB SI-SDR improvement (SI-SDRi) on the WSJ0-2mix dataset\u00a0\u2026",
            "url": null,
            "publicationVenue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
            "citation_count": 43
        },
        {
            "title": "Wav2seq: Pre-training speech-to-text encoder-decoder models using pseudo languages",
            "authors": "Felix Wu, Kwangyoun Kim, Shinji Watanabe, Kyu J Han, Ryan McDonald, Kilian Q Weinberger, Yoav Artzi",
            "year": "2023",
            "abstract": "We introduce Wav2Seq, the first self-supervised approach to pre-train both parts of encoder-decoder models for speech data. We induce a pseudo language as a compact discrete representation, and formulate a self-supervised pseudo speech recognition task \u2014 transcribing audio inputs into pseudo subword sequences. This process stands on its own, or can be applied as low-cost second-stage pre-training. We experiment with automatic speech recognition (ASR), spoken named entity recognition, and speech-to-text translation. We set new state-of-the-art results for end-to-end spoken named entity recognition, and show consistent improvements on 8 language pairs for speech-to-text translation, even when competing methods use additional text data for training. On ASR, our approach enables encoder-decoder methods to benefit from pre-training for all parts of the network, and shows comparable performance\u00a0\u2026",
            "url": "https://arxiv.org/pdf/2205.01086",
            "publicationVenue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
            "citation_count": 19
        },
        {
            "title": "Tensor decomposition for minimization of E2E SLU model toward on-device processing",
            "authors": "Yosuke Kashiwagi, Siddhant Arora, Hayato Futami, Jessica Huynh, Shih-Lun Wu, Yifan Peng, Brian Yan, Emiru Tsunoo, Shinji Watanabe",
            "year": "2023",
            "abstract": "Spoken Language Understanding (SLU) is a critical speech recognition application and is often deployed on edge devices. Consequently, on-device processing plays a significant role in the practical implementation of SLU. This paper focuses on the end-to-end (E2E) SLU model due to its small latency property, unlike a cascade system, and aims to minimize the computational cost. We reduce the model size by applying tensor decomposition to the Conformer and E-Branchformer architectures used in our E2E SLU models. We propose to apply singular value decomposition to linear layers and the Tucker decomposition to convolution layers, respectively. We also compare COMP/PARFAC decomposition and Tensor-Train decomposition to the Tucker decomposition. Since the E2E model is represented by a single neural network, our tensor decomposition can flexibly control the number of parameters without changing feature dimensions. On the STOP dataset, we achieved 70.9% exact match accuracy under the tight constraint of only 15 million parameters.",
            "url": "https://arxiv.org/pdf/2306.01247",
            "publicationVenue": "arXiv preprint arXiv:2306.01247",
            "citation_count": 1
        },
        {
            "title": "Exploration on HuBERT with Multiple Resolutions",
            "authors": "Jiatong Shi, Yun Tang, Hirofumi Inaguma, Hongyu GOng, Juan Pino, Shinji Watanabe",
            "year": "2023",
            "abstract": "Hidden-unit BERT (HuBERT) is a widely-used self-supervised learning (SSL) model in speech processing. However, we argue that its fixed 20ms resolution for hidden representations would not be optimal for various speech-processing tasks since their attributes (e.g., speaker characteristics and semantics) are based on different time scales. To address this limitation, we propose utilizing HuBERT representations at multiple resolutions for downstream tasks. We explore two approaches, namely the parallel and hierarchical approaches, for integrating HuBERT features with different resolutions. Through experiments, we demonstrate that HuBERT with multiple resolutions outperforms the original model. This highlights the potential of utilizing multiple resolutions in SSL models like HuBERT to capture diverse information from speech signals.",
            "url": "https://arxiv.org/pdf/2306.01084",
            "publicationVenue": "arXiv preprint arXiv:2306.01084",
            "citation_count": 3
        },
        {
            "title": "Exploration of Efficient End-to-End ASR using Discretized Input from Self-Supervised Learning",
            "authors": "Xuankai Chang, Brian Yan, Yuya Fujita, Takashi Maekaku, Shinji Watanabe",
            "year": "2023",
            "abstract": "Self-supervised learning (SSL) of speech has shown impressive results in speech-related tasks, particularly in automatic speech recognition (ASR). While most methods employ the output of intermediate layers of the SSL model as real-valued features for downstream tasks, there is potential in exploring alternative approaches that use discretized token sequences. This approach offers benefits such as lower storage requirements and the ability to apply techniques from natural language processing. In this paper, we propose a new protocol that utilizes discretized token sequences in ASR tasks, which includes de-duplication and sub-word modeling to enhance the input sequence. It reduces computational cost by decreasing the length of the sequence. Our experiments on the LibriSpeech dataset demonstrate that our proposed protocol performs competitively with conventional ASR systems using continuous input features, while reducing computational and storage costs.",
            "url": "https://arxiv.org/pdf/2305.18108",
            "publicationVenue": "arXiv preprint arXiv:2305.18108",
            "citation_count": 16
        },
        {
            "title": "DPHuBERT: Joint Distillation and Pruning of Self-Supervised Speech Models",
            "authors": "Yifan Peng, Yui Sudo, Shakeel Muhammad, Shinji Watanabe",
            "year": "2023",
            "abstract": "Self-supervised learning (SSL) has achieved notable success in many speech processing tasks, but the large model size and heavy computational cost hinder the deployment. Knowledge distillation trains a small student model to mimic the behavior of a large teacher model. However, the student architecture usually needs to be manually designed and will remain fixed during training, which requires prior knowledge and can lead to suboptimal performance. Inspired by recent success of task-specific structured pruning, we propose DPHuBERT, a novel task-agnostic compression method for speech SSL based on joint distillation and pruning. Experiments on SUPERB show that DPHuBERT outperforms pure distillation methods in almost all tasks. Moreover, DPHuBERT requires little training time and performs well with limited training data, making it suitable for resource-constrained applications. Our method can also be applied to various speech SSL models. Our code and models will be publicly available.",
            "url": "https://arxiv.org/pdf/2305.17651",
            "publicationVenue": "arXiv preprint arXiv:2305.17651",
            "citation_count": 17
        },
        {
            "title": "A New Benchmark of Aphasia Speech Recognition and Detection Based on E-Branchformer and Multi-task Learning",
            "authors": "Jiyang Tang, William Chen, Xuankai Chang, Shinji Watanabe, Brian MacWhinney",
            "year": "2023",
            "abstract": "Aphasia is a language disorder that affects the speaking ability of millions of patients. This paper presents a new benchmark for Aphasia speech recognition and detection tasks using state-of-the-art speech recognition techniques with the AphsiaBank dataset. Specifically, we introduce two multi-task learning methods based on the CTC/Attention architecture to perform both tasks simultaneously. Our system achieves state-of-the-art speaker-level detection accuracy (97.3%), and a relative WER reduction of 11% for moderate Aphasia patients. In addition, we demonstrate the generalizability of our approach by applying it to another disordered speech database, the DementiaBank Pitt corpus. We will make our all-in-one recipes and pre-trained model publicly available to facilitate reproducibility. Our standardized data preprocessing pipeline and open-source recipes enable researchers to compare results directly, promoting progress in disordered speech processing.",
            "url": "https://arxiv.org/pdf/2305.13331",
            "publicationVenue": "arXiv preprint arXiv:2305.13331",
            "citation_count": 1
        },
        {
            "title": "Prompting the Hidden Talent of Web-Scale Speech Models for Zero-Shot Task Generalization",
            "authors": "Puyuan Peng, Brian Yan, Shinji Watanabe, David Harwath",
            "year": "2023",
            "abstract": "We investigate the emergent abilities of the recently proposed web-scale speech model Whisper, by adapting it to unseen tasks with prompt engineering. We selected three tasks: audio-visual speech recognition (AVSR), code-switched speech recognition (CS-ASR), and speech translation (ST) on unseen language pairs. We design task-specific prompts, by either leveraging another large-scale model, or simply manipulating the special tokens in the default prompts. Experiments show that compared to the default prompts, our proposed prompts improve performance by 10% to 45% on the three zero-shot tasks, and even outperform SotA supervised models on some datasets. In addition, our experiments reveal many interesting properties of Whisper, including its robustness to prompts, bias on accents, and the multilingual understanding in its latent space. Code is available at https://github.com/jasonppy/PromptingWhisper",
            "url": "https://arxiv.org/pdf/2305.11095",
            "publicationVenue": "arXiv preprint arXiv:2305.11095",
            "citation_count": 15
        },
        {
            "title": "ML-SUPERB: Multilingual Speech Universal PERformance Benchmark",
            "authors": "Jiatong Shi, Dan Berrebbi, William Chen, Ho-Lam Chung, En-Pei Hu, Wei Ping Huang, Xuankai Chang, Shang-Wen Li, Abdelrahman Mohamed, Hung-yi Lee, Shinji Watanabe",
            "year": "2023",
            "abstract": "Speech processing Universal PERformance Benchmark (SUPERB) is a leaderboard to benchmark the performance of Self-Supervised Learning (SSL) models on various speech processing tasks. However, SUPERB largely considers English speech in its evaluation. This paper presents multilingual SUPERB (ML-SUPERB), covering 143 languages (ranging from high-resource to endangered), and considering both automatic speech recognition and language identification. Following the concept of SUPERB, ML-SUPERB utilizes frozen SSL features and employs a simple framework for multilingual tasks by learning a shallow downstream model. Similar to the SUPERB benchmark, we find speech SSL models can significantly improve performance compared to FBANK features. Furthermore, we find that multilingual models do not always perform better than their monolingual counterparts. We will release ML-SUPERB as a challenge with organized datasets and reproducible training scripts for future multilingual representation research.",
            "url": "https://arxiv.org/pdf/2305.10615",
            "publicationVenue": "arXiv preprint arXiv:2305.10615",
            "citation_count": 18
        },
        {
            "title": "A Comparative Study on E-Branchformer vs Conformer in Speech Recognition, Translation, and Understanding Tasks",
            "authors": "Yifan Peng, Kwangyoun Kim, Felix Wu, Brian Yan, Siddhant Arora, William Chen, Jiyang Tang, Suwon Shon, Prashant Sridhar, Shinji Watanabe",
            "year": "2023",
            "abstract": "Conformer, a convolution-augmented Transformer variant, has become the de facto encoder architecture for speech processing due to its superior performance in various tasks, including automatic speech recognition (ASR), speech translation (ST) and spoken language understanding (SLU). Recently, a new encoder called E-Branchformer has outperformed Conformer in the LibriSpeech ASR benchmark, making it promising for more general speech applications. This work compares E-Branchformer and Conformer through extensive experiments using different types of end-to-end sequence-to-sequence models. Results demonstrate that E-Branchformer achieves comparable or better performance than Conformer in almost all evaluation sets across 15 ASR, 2 ST, and 3 SLU benchmarks, while being more stable during training. We will release our training configurations and pre-trained models for reproducibility, which can benefit the speech community.",
            "url": "https://arxiv.org/pdf/2305.11073",
            "publicationVenue": "arXiv preprint arXiv:2305.11073",
            "citation_count": 8
        },
        {
            "title": "Improving Cascaded Unsupervised Speech Translation with Denoising Back-translation",
            "authors": "Yu-Kuan Fu, Liang-Hsuan Tseng, Jiatong Shi, Chen-An Li, Tsu-Yuan Hsu, Shinji Watanabe, Hung-yi Lee",
            "year": "2023",
            "abstract": "Most of the speech translation models heavily rely on parallel data, which is hard to collect especially for low-resource languages. To tackle this issue, we propose to build a cascaded speech translation system without leveraging any kind of paired data. We use fully unpaired data to train our unsupervised systems and evaluate our results on CoVoST 2 and CVSS. The results show that our work is comparable with some other early supervised methods in some language pairs. While cascaded systems always suffer from severe error propagation problems, we proposed denoising back-translation (DBT), a novel approach to building robust unsupervised neural machine translation (UNMT). DBT successfully increases the BLEU score by 0.7--0.9 in all three translation directions. Moreover, we simplified the pipeline of our cascaded system to reduce inference latency and conducted a comprehensive analysis of every part of our work. We also demonstrate our unsupervised speech translation results on the established website.",
            "url": "https://arxiv.org/pdf/2305.07455",
            "publicationVenue": "arXiv preprint arXiv:2305.07455",
            "citation_count": 2
        },
        {
            "title": "BEATs-based audio captioning model with INSTRUCTOR embedding supervision and ChatGPT mix-up",
            "authors": "Shih-Lun Wu, Xuankai Chang, Gordon Wichern, Jee-weon Jung, Fran\u00e7ois Germain, Jonathan Le Roux, Shinji Watanabe",
            "year": "2023",
            "abstract": "DCASE 2023 Task 6A, automated audio captioning (AAC), aims at generating informative descriptions for various sounds from nature and/or human activities. Our AAC system follows the sequence-tosequence (seq2seq) architecture. The audio encoder stack is comprised of a frozen BEATS Transformer followed by a 2-layer Conformer. The BEATS module, which has been pretrained on both masked audio token prediction and audio event classification, extracts fine-grained (ie,\u2248 50 Hz) audio features, while the Conformer downsamples and summarizes the audio features before they are cross-attended by the BART text decoder. Besides the autoregressive negative log-likelihood (NLL) loss computed on decoder outputs, we simultaneously apply an audio-text contrastive loss on our encoder output to infuse language modality knowledge into it. Specifically, we feed ground-truth captions into INSTRUCTOR Transformer, a state-of-the-art text embedding model, and teach our audio encoder to predict the INSTRUCTOR text embeddings through InfoNCE loss. In addition, we leverage ChatGPT to produce caption mix-ups (ie, grammatical and compact combinations of two captions) which, together with the corresponding audio mixtures, increases not only the amount but also the complexity and diversity of our training data. During inference, we employ nucleus sampling and a hybrid reranking algorithm that considers both likelihood and audio-caption representation similarity. Combining our efforts, our best single model and ensemble system achieve 0.326 and 0.336 SPIDEr-FL scores, respectively, on the Clotho (V2) evaluation split.",
            "url": "https://dcase.community/documents/challenge2023/technical_reports/DCASE2023_Wu_31_t6a.pdf",
            "publicationVenue": "Detection Classification Acoust. Scenes Events Challenge, Tech. Rep",
            "citation_count": 12
        },
        {
            "title": "Audiogpt: Understanding and generating speech, music, sound, and talking head",
            "authors": "Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xuankai Chang, Zhenhui Ye, Yuning Wu, Zhiqing Hong, Jiawei Huang, Jinglin Liu, Yi Ren, Zhou Zhao, Shinji Watanabe",
            "year": "2023",
            "abstract": "Large language models (LLMs) have exhibited remarkable capabilities across a variety of domains and tasks, challenging our understanding of learning and cognition. Despite the recent success, current LLMs are not capable of processing complex audio information or conducting spoken conversations (like Siri or Alexa). In this work, we propose a multi-modal AI system named AudioGPT, which complements LLMs (i.e., ChatGPT) with 1) foundation models to process complex audio information and solve numerous understanding and generation tasks; and 2) the input/output interface (ASR, TTS) to support spoken dialogue. With an increasing demand to evaluate multi-modal LLMs of human intention understanding and cooperation with foundation models, we outline the principles and processes and test AudioGPT in terms of consistency, capability, and robustness. Experimental results demonstrate the capabilities of AudioGPT in solving AI tasks with speech, music, sound, and talking head understanding and generation in multi-round dialogues, which empower humans to create rich and diverse audio content with unprecedented ease. Our system is publicly available at \\url{https://github.com/AIGC-Audio/AudioGPT}.",
            "url": "https://arxiv.org/pdf/2304.12995",
            "publicationVenue": "arXiv preprint arXiv:2304.12995",
            "citation_count": 80
        },
        {
            "title": "Efficient Sequence Transduction by Jointly Predicting Tokens and Durations",
            "authors": "Hainan Xu, Fei Jia, Somshubra Majumdar, He Huang, Shinji Watanabe, Boris Ginsburg",
            "year": "2023",
            "abstract": "This paper introduces a novel Token-and-Duration Transducer (TDT) architecture for sequence-to-sequence tasks. TDT extends conventional RNN-Transducer architectures by jointly predicting both a token and its duration, i.e. the number of input frames covered by the emitted token. This is achieved by using a joint network with two outputs which are independently normalized to generate distributions over tokens and durations. During inference, TDT models can skip input frames guided by the predicted duration output, which makes them significantly faster than conventional Transducers which process the encoder output frame by frame. TDT models achieve both better accuracy and significantly faster inference than conventional Transducers on different sequence transduction tasks. TDT models for Speech Recognition achieve better accuracy and up to 2.82X faster inference than RNN-Transducers. TDT models for Speech Translation achieve an absolute gain of over 1 BLEU on the MUST-C test compared with conventional Transducers, and its inference is 2.27X faster. In Speech Intent Classification and Slot Filling tasks, TDT models improve the intent accuracy up to over 1% (absolute) over conventional Transducers, while running up to 1.28X faster.",
            "url": "https://arxiv.org/pdf/2304.06795",
            "publicationVenue": "arXiv preprint arXiv:2304.06795",
            "citation_count": 3
        },
        {
            "title": "ESPnet-ST-v2: Multipurpose Spoken Language Translation Toolkit",
            "authors": "Brian Yan, Jiatong Shi, Yun Tang, Hirofumi Inaguma, Yifan Peng, Siddharth Dalmia, Peter Pol\u00e1k, Patrick Fernandes, Dan Berrebbi, Tomoki Hayashi, Xiaohui Zhang, Zhaoheng Ni, Moto Hira, Soumi Maiti, Juan Pino, Shinji Watanabe",
            "year": "2023",
            "abstract": "ESPnet-ST-v2 is a revamp of the open-source ESPnet-ST toolkit necessitated by the broadening interests of the spoken language translation community. ESPnet-ST-v2 supports 1) offline speech-to-text translation (ST), 2) simultaneous speech-to-text translation (SST), and 3) offline speech-to-speech translation (S2ST) -- each task is supported with a wide variety of approaches, differentiating ESPnet-ST-v2 from other open source spoken language translation toolkits. This toolkit offers state-of-the-art architectures such as transducers, hybrid CTC/attention, multi-decoders with searchable intermediates, time-synchronous blockwise CTC/attention, Translatotron models, and direct discrete unit models. In this paper, we describe the overall design, example models for each task, and performance benchmarking behind ESPnet-ST-v2, which is publicly available at https://github.com/espnet/espnet.",
            "url": "https://arxiv.org/pdf/2304.04596",
            "publicationVenue": "arXiv preprint arXiv:2304.04596",
            "citation_count": 5
        },
        {
            "title": "EVERYDAY CONVERSATION SPEECH RECOGNITION WITH END-TO-END NEURAL NETWORKS",
            "authors": "Xuankai Chang, Shinji Watanabe, Bhiksha Ramakrishnan, Rita Singh",
            "year": "2023",
            "abstract": "Automatic speech recognition (ASR) is an essential technology which facilitates effective humancomputer interaction. With the rapid progress in deep learning techniques, end-to-end (E2E) neural network-based ASR has brought significant advancements with remarkable performance. The success of ASR models have inspired various applications such as virtual assistants and automatic transcription services. Despite these achievements, recognizing conversational speech remains a challenging task, especially in the presence of environmental noise, room reverberations and speech overlaps.This thesis aims to address the challenges of recognizing everyday conversation speech in ASR systems using E2E neural networks. The proposed research will explore techniques and methodologies to enhance the performance of ASR in challenging real-word conversational scenarios. We divide the problem into several sub-problems focusing on speech overlaps, noise, and reverberations, where each of these factors will be individually analyzed and addressed. In addition, we conduct diverse investigations on E2E neural network architectures to leverage the benefits of joint training to handle these challenges. Specifically, we build E2E ASR models by integrating ad-hoc modules, including speech enhancement, feature extraction and speech recognition. We start from tackling the speech overlaps, which has not been sufficiently explored. The presence of speech overlaps poses difficulties in accurately decoding and aligning the individual utterances. We first propose several E2E models to recognize the overlapping speech within a single-channel\u00a0\u2026",
            "url": "http://cvis.cs.cmu.edu/cvis/docs/Xuankai_ThesisProposal.pdf",
            "publicationVenue": null,
            "citation_count": 0
        },
        {
            "title": "End-to-End Speech Recognition: A Survey",
            "authors": "Rohit Prabhavalkar, Takaaki Hori, Tara N Sainath, Ralf Schl\u00fcter, Shinji Watanabe",
            "year": "2023",
            "abstract": "In the last decade of automatic speech recognition (ASR) research, the introduction of deep learning brought considerable reductions in word error rate of more than 50% relative, compared to modeling without deep learning. In the wake of this transition, a number of all-neural ASR architectures were introduced. These so-called end-to-end (E2E) models provide highly integrated, completely neural ASR models, which rely strongly on general machine learning knowledge, learn more consistently from data, while depending less on ASR domain-specific experience. The success and enthusiastic adoption of deep learning accompanied by more generic model architectures lead to E2E models now becoming the prominent ASR approach. The goal of this survey is to provide a taxonomy of E2E ASR models and corresponding improvements, and to discuss their properties and their relation to the classical hidden Markov model (HMM) based ASR architecture. All relevant aspects of E2E ASR are covered in this work: modeling, training, decoding, and external language model integration, accompanied by discussions of performance and deployment opportunities, as well as an outlook into potential future developments.",
            "url": "https://arxiv.org/pdf/2303.03329",
            "publicationVenue": "arXiv preprint arXiv:2303.03329",
            "citation_count": 44
        },
        {
            "title": "Multi-channel target speaker extraction with refinement: The wavlab submission to the second clarity enhancement challenge",
            "authors": "Samuele Cornell, Zhong-Qiu Wang, Yoshiki Masuyama, Shinji Watanabe, Manuel Pariente, Nobutaka Ono",
            "year": "2023",
            "abstract": "This paper describes our submission to the Second Clarity Enhancement Challenge (CEC2), which consists of target speech enhancement for hearing-aid (HA) devices in noisy-reverberant environments with multiple interferers such as music and competing speakers. Our approach builds upon the powerful iterative neural/beamforming enhancement (iNeuBe) framework introduced in our recent work, and this paper extends it for target speaker extraction. We therefore name the proposed approach as iNeuBe-X, where the X stands for extraction. To address the challenges encountered in the CEC2 setting, we introduce four major novelties: (1) we extend the state-of-the-art TF-GridNet model, originally designed for monaural speaker separation, for multi-channel, causal speech enhancement, and large improvements are observed by replacing the TCNDenseNet used in iNeuBe with this new architecture; (2) we leverage a recent dual window size approach with future-frame prediction to ensure that iNueBe-X satisfies the 5 ms constraint on algorithmic latency required by CEC2; (3) we introduce a novel speaker-conditioning branch for TF-GridNet to achieve target speaker extraction; (4) we propose a fine-tuning step, where we compute an additional loss with respect to the target speaker signal compensated with the listener audiogram. Without using external data, on the official development set our best model reaches a hearing-aid speech perception index (HASPI) score of 0.942 and a scale-invariant signal-to-distortion ratio improvement (SI-SDRi) of 18.8 dB. These results are promising given the fact that the CEC2 data is extremely challenging\u00a0\u2026",
            "url": "https://arxiv.org/pdf/2302.07928",
            "publicationVenue": "arXiv preprint arXiv:2302.07928",
            "citation_count": 5
        },
        {
            "title": "A vector quantized approach for text to speech synthesis on real-world spontaneous speech",
            "authors": "Li-Wei Chen, Shinji Watanabe, Alexander Rudnicky",
            "year": "2023",
            "abstract": "Recent Text-to-Speech (TTS) systems trained on reading or acted corpora have achieved near human-level naturalness. The diversity of human speech, however, often goes beyond the coverage of these corpora. We believe the ability to handle such diversity is crucial for AI systems to achieve human-level communication. Our work explores the use of more abundant real-world data for building speech synthesizers. We train TTS systems using real-world speech from YouTube and podcasts. We observe the mismatch between training and inference alignments in mel-spectrogram based autoregressive models, leading to unintelligible synthesis, and demonstrate that learned discrete codes within multiple code groups effectively resolves this issue. We introduce our MQTTS system whose architecture is designed for multiple code generation and monotonic alignment, along with the use of a clean silence prompt to improve synthesis quality. We conduct ablation analyses to identify the efficacy of our methods. We show that MQTTS outperforms existing TTS systems in several objective and subjective measures.",
            "url": "https://arxiv.org/pdf/2302.04215",
            "publicationVenue": "arXiv preprint arXiv:2302.04215",
            "citation_count": 16
        },
        {
            "title": "Learning to Speak from Text: Zero-Shot Multilingual Text-to-Speech with Unsupervised Text Pretraining",
            "authors": "Takaaki Saeki, Soumi Maiti, Xinjian Li, Shinji Watanabe, Shinnosuke Takamichi, Hiroshi Saruwatari",
            "year": "2023",
            "abstract": "While neural text-to-speech (TTS) has achieved human-like natural synthetic speech, multilingual TTS systems are limited to resource-rich languages due to the need for paired text and studio-quality audio data. This paper proposes a method for zero-shot multilingual TTS using text-only data for the target language. The use of text-only data allows the development of TTS systems for low-resource languages for which only textual resources are available, making TTS accessible to thousands of languages. Inspired by the strong cross-lingual transferability of multilingual language models, our framework first performs masked language model pretraining with multilingual text-only data. Then we train this model with a paired data in a supervised manner, while freezing a language-aware embedding layer. This allows inference even for languages not included in the paired data but present in the text-only data. Evaluation results demonstrate highly intelligible zero-shot TTS with a character error rate of less than 12% for an unseen language. All experiments were conducted using public datasets and the implementation will be made available for reproducibility.",
            "url": "https://arxiv.org/pdf/2301.12596",
            "publicationVenue": "arXiv preprint arXiv:2301.12596",
            "citation_count": 5
        },
        {
            "title": "Unsupervised data selection for tts: Using arabic broadcast news as a case study",
            "authors": "Massa Baali, Tomoki Hayashi, Hamdy Mubarak, Soumi Maiti, Shinji Watanabe, Wassim El-Hajj, Ahmed Ali",
            "year": "2023",
            "abstract": "Several high-resource Text to Speech (TTS) systems currently produce natural, well-established human-like speech. In contrast, low-resource languages, including Arabic, have very limited TTS systems due to the lack of resources. We propose a fully unsupervised method for building TTS, including automatic data selection and pre-training/fine-tuning strategies for TTS training, using broadcast news as a case study. We show how careful selection of data, yet smaller amounts, can improve the efficiency of TTS system in generating more natural speech than a system trained on a bigger dataset. We adopt to propose different approaches for the: 1) data: we applied automatic annotations using DNSMOS, automatic vowelization, and automatic speech recognition (ASR) for fixing transcriptions' errors; 2) model: we used transfer learning from high-resource language in TTS model and fine-tuned it with one hour broadcast recording then we used this model to guide a FastSpeech2-based Conformer model for duration. Our objective evaluation shows 3.9% character error rate (CER), while the groundtruth has 1.3% CER. As for the subjective evaluation, where 1 is bad and 5 is excellent, our FastSpeech2-based Conformer model achieved a mean opinion score (MOS) of 4.4 for intelligibility and 4.2 for naturalness, where many annotators recognized the voice of the broadcaster, which proves the effectiveness of our proposed unsupervised method.",
            "url": "https://arxiv.org/pdf/2301.09099",
            "publicationVenue": "arXiv preprint arXiv:2301.09099",
            "citation_count": 6
        },
        {
            "title": "A study on the integration of pre-trained ssl, asr, lm and slu models for spoken language understanding",
            "authors": "Yifan Peng, Siddhant Arora, Yosuke Higuchi, Yushi Ueda, Sujay Kumar, Karthik Ganesan, Siddharth Dalmia, Xuankai Chang, Shinji Watanabe",
            "year": "2023",
            "abstract": "Collecting sufficient labeled data for spoken language understanding (SLU) is expensive and time-consuming. Recent studies achieved promising results by using pre-trained models in low-resource scenarios. Inspired by this, we aim to ask: which (if any) pre-training strategies can improve performance across SLU benchmarks? To answer this question, we employ four types of pre-trained models and their combinations for SLU. We leverage self-supervised speech and language models (LM) pre-trained on large quantities of un-paired data to extract strong speech and text representations. We also explore using supervised models pre-trained on larger external automatic speech recognition (ASR) or SLU corpora. We conduct extensive experiments on the SLU Evaluation (SLUE) benchmark and observe self-supervised pre-trained models to be more powerful, with pre-trained LM and speech models being most\u00a0\u2026",
            "url": "https://arxiv.org/pdf/2211.05869",
            "publicationVenue": "2022 IEEE Spoken Language Technology Workshop (SLT)",
            "citation_count": 15
        },
        {
            "title": "End-to-end integration of speech recognition, dereverberation, beamforming, and self-supervised learning representation",
            "authors": "Yoshiki Masuyama, Xuankai Chang, Samuele Cornell, Shinji Watanabe, Nobutaka Ono",
            "year": "2023",
            "abstract": "Self-supervised learning representation (SSLR) has demonstrated its significant effectiveness in automatic speech recognition (ASR), mainly with clean speech. Recent work pointed out the strength of integrating SSLR with single-channel speech enhancement for ASR in noisy environments. This paper further advances this integration by dealing with multi-channel input. We propose a novel end-to-end architecture by integrating dereverberation, beamforming, SSLR, and ASR within a single neural network. Our system achieves the best performance reported in the literature on the CHiME-4 6-channel track with a word error rate (WER) of 1.77%. While the WavLM-based strong SSLR demonstrates promising results by itself, the end-to-end integration with the weighted power minimization distortionless response beamformer, which simultaneously performs dereverberation and denoising, improves WER\u00a0\u2026",
            "url": "https://arxiv.org/pdf/2210.10742",
            "publicationVenue": "2022 IEEE Spoken Language Technology Workshop (SLT)",
            "citation_count": 11
        },
        {
            "title": "Superb@ slt 2022: Challenge on generalization and efficiency of self-supervised speech representation learning",
            "authors": "Tzu-hsun Feng, Annie Dong, Ching-Feng Yeh, Shu-wen Yang, Tzu-Quan Lin, Jiatong Shi, Kai-Wei Chang, Zili Huang, Haibin Wu, Xuankai Chang, Shinji Watanabe, Abdelrahman Mohamed, Shang-Wen Li, Hung-yi Lee",
            "year": "2023",
            "abstract": "We present the SUPERB challenge at SLT 2022, which aims at learning self-supervised speech representation for better performance, generalization, and efficiency. The challenge builds upon the SUPERB benchmark and implements metrics to measure the computation requirements of self-supervised learning (SSL) representation and to evaluate its generalizability and performance across the diverse SUPERB tasks. The SUPERB benchmark provides comprehensive coverage of popular speech processing tasks, from speech and speaker recognition to audio generation and semantic understanding. As SSL has gained interest in the speech community and showed promising outcomes, we envision the challenge to uplevel the impact of SSL techniques by motivating more practical designs of techniques beyond task performance. We summarize the results of 14 submitted models in this paper. We also discuss\u00a0\u2026",
            "url": "https://arxiv.org/pdf/2210.08634",
            "publicationVenue": "2022 IEEE Spoken Language Technology Workshop (SLT)",
            "citation_count": 21
        },
        {
            "title": "On compressing sequences for self-supervised speech models",
            "authors": "Yen Meng, Hsuan-Jui Chen, Jiatong Shi, Shinji Watanabe, Paola Garcia, Hung-yi Lee, Hao Tang",
            "year": "2023",
            "abstract": "Compressing self-supervised models has become increasingly necessary, as self-supervised models become larger. While previous approaches have primarily focused on compressing the model size, shortening sequences is also effective in reducing the computational cost. In this work, we study fixed-length and variable-length subsampling along the time axis in self-supervised learning. We explore how individual downstream tasks are sensitive to input frame rates. Subsampling while training self-supervised models not only improves the overall performance on downstream tasks under certain frame rates, but also brings significant speed-up in inference. Variable-length subsampling performs particularly well under low frame rates. In addition, if we have access to phonetic boundaries, we find no degradation in performance for an average frame rate as low as 10 Hz.",
            "url": "https://arxiv.org/pdf/2210.07189",
            "publicationVenue": "2022 IEEE Spoken Language Technology Workshop (SLT)",
            "citation_count": 12
        },
        {
            "title": "Mutual Learning of Single-and Multi-Channel End-to-End Neural Diarization",
            "authors": "Shota Horiguchi, Yuki Takashima, Shinji Watanabe, Paola Garcia",
            "year": "2023",
            "abstract": "Due to the high performance of multi-channel speech processing, we can use the outputs from a multi-channel model as teacher labels when training a single-channel model with knowledge distillation. To the contrary, it is also known that single-channel speech data can benefit multi-channel models by mixing it with multi-channel speech data during training or by using it for model pretraining. This paper focuses on speaker diarization and proposes to conduct the above bi-directional knowledge transfer alternately. We first introduce an end-to-end neural diarization model that can handle both single- and multi-channel inputs. Using this model, we alternately conduct i) knowledge distillation from a multi-channel model to a single-channel model and ii) finetuning from the distilled single-channel model to a multi-channel model. Experimental results on two-speaker data show that the proposed method mutually\u00a0\u2026",
            "url": "https://arxiv.org/pdf/2210.03459",
            "publicationVenue": "2022 IEEE Spoken Language Technology Workshop (SLT)",
            "citation_count": 1
        },
        {
            "title": "E-branchformer: Branchformer with enhanced merging for speech recognition",
            "authors": "Kwangyoun Kim, Felix Wu, Yifan Peng, Jing Pan, Prashant Sridhar, Kyu J Han, Shinji Watanabe",
            "year": "2023",
            "abstract": "Conformer, combining convolution and self-attention sequentially to capture both local and global information, has shown remarkable performance and is currently regarded as the state-of-the-art for automatic speech recognition (ASR). Several other studies have explored integrating convolution and self-attention but they have not managed to match Conformer's performance. The recently introduced Branchformer achieves comparable performance to Conformer by using dedicated branches of convolution and self-attention and merging local and global context from each branch. In this paper, we propose E-Branchformer, which enhances Branchformer by applying an effective merging method and stacking additional point-wise modules. E-Branchformer sets new state-of-the-art word error rates (WERs) 1.81% and 3.65% on LibriSpeech test-clean and test-other sets without using any external training data.",
            "url": "https://arxiv.org/pdf/2210.00077",
            "publicationVenue": "2022 IEEE Spoken Language Technology Workshop (SLT)",
            "citation_count": 44
        },
        {
            "title": "End-to-End Multi-Speaker ASR with Independent Vector Analysis",
            "authors": "Robin Scheibler, Wangyou Zhang, Xuankai Chang, Shinji Watanabe, Yanmin Qian",
            "year": "2023",
            "abstract": "We develop an end-to-end system for multi-channel, multi-speaker automatic speech recognition. We propose a frontend for joint source separation and dereverberation based on the independent vector analysis (IVA) paradigm. It uses the fast and stable iterative source steering algorithm together with a neural source model. Unlike conventional neural beamforming, the number of speakers can be dynamically changed during or after training. The parameters from the ASR module and the neural source model are optimized jointly from the ASR loss itself. We demonstrate competitive performance with previous systems using neural beamforming frontends with only one-ninth of the trainable parameter. First, we explore the trade-offs when using various number of channels for training and testing. Second, we demonstrate that the proposed IVA frontend performs well on noisy data, even when trained on clean\u00a0\u2026",
            "url": "https://arxiv.org/pdf/2204.00218",
            "publicationVenue": "2022 IEEE Spoken Language Technology Workshop (SLT)",
            "citation_count": 1
        },
        {
            "title": "EEND-SS: Joint end-to-end neural speaker diarization and speech separation for flexible number of speakers",
            "authors": "Soumi Maiti, Yushi Ueda, Shinji Watanabe, Chunlei Zhang, Meng Yu, Shi-Xiong Zhang, Yong Xu",
            "year": "2023",
            "abstract": "In this paper, we present a novel framework that jointly performs three tasks: speaker diarization, speech separation, and speaker counting. Our proposed framework integrates speaker diarization based on end-to-end neural diarization (EEND) models, speaker counting with encoder-decoder based attractors (EDA), and speech separation using Conv-TasNet. In addition, we propose a multipleconvolutional layer architecture for estimating the separation masks corresponding to a flexible number of speakers and a fusion technique for refining the separated speech signal with obtained speaker diarization information to improve the joint framework. Experiments using the LibriMix dataset show that our proposed method outperforms the single-task baselines in both diarization and separation metrics for fixed and flexible numbers of speakers and improves speaker counting performance for flexible numbers of\u00a0\u2026",
            "url": "https://arxiv.org/pdf/2203.17068",
            "publicationVenue": "2022 IEEE Spoken Language Technology Workshop (SLT)",
            "citation_count": 15
        },
        {
            "title": "Time-synchronous one-pass beam search for parallel online and offline transducers with dynamic block training",
            "authors": "Yui Sudo, Muhammad Shakeel, Yifan Peng, Shinji Watanabe",
            "year": "2023",
            "abstract": "End-to-end automatic speech recognition (ASR) has become an increasingly popular area of research, with two main models being online and offline ASR. Online models aim to provide real-time transcription with minimal latency, whereas offline models wait until the end of the speech utterance before generating a transcription. In this work, we explore three techniques to maximize the performance of each model by 1) proposing a joint parallel online and offline architecture for transducers; 2) introducing dynamic block (DB) training, which allows flexible block size selection and improves the robustness for the offline mode; and, 3) proposing a novel timesynchronous one-pass beam search using the online and offline decoders to further improve the performance of the offline mode. Experimental results show that the proposed method consistently improves the character/word error rates on the CSJ and LibriSpeech datasets.",
            "url": "https://www.researchgate.net/profile/Yui-Sudo/publication/373119488_Time-synchronous_one-pass_Beam_Search_for_Parallel_Online_and_Offline_Transducers_with_Dynamic_Block_Training/links/64db4229ad846e288293d437/Time-synchronous-one-pass-Beam-Search-for-Parallel-Online-and-Offline-Transducers-with-Dynamic-Block-Training.pdf",
            "publicationVenue": "Proc. Interspeech",
            "citation_count": 3
        },
        {
            "title": "Findings of the IWSLT 2023 evaluation campaign",
            "authors": "Milind Agarwal, Sweta Agarwal, Antonios Anastasopoulos, Luisa Bentivogli, Ond\u0159ej Bojar, Claudia Borg, Marine Carpuat, Roldano Cattoni, Mauro Cettolo, Mingda Chen, William Chen, Khalid Choukri, Alexandra Chronopoulou, Anna Currey, Thierry Declerck, Qianqian Dong, Kevin Duh, Yannick Est\u00e8ve, Marcello Federico",
            "year": "2023",
            "abstract": "This paper reports on the shared tasks organized by the 20th IWSLT Conference. The shared tasks address 9 scientific challenges in spoken language translation: simultaneous and offline translation, automatic subtitling and dubbing, speech-to-speech translation, multilingual, dialect and low-resource speech translation, and formality control. The shared tasks attracted a total of 38 submissions by 31 teams. The growing interest towards spoken language translation is also witnessed by the constantly increasing number of shared task organizers and contributors to the overview paper, almost evenly distributed across industry and academia.",
            "url": "https://www.um.edu.mt/library/oar/bitstream/123456789/114765/1/2023.iwslt-1.1v2.pdf",
            "publicationVenue": "Association for Computational Linguistics",
            "citation_count": 21
        },
        {
            "title": "A dilemma of ground truth in noisy speech separation and an approach to lessen the impact of imperfect training data",
            "authors": "Matthew Maciejewski, Jing Shi, Shinji Watanabe, Sanjeev Khudanpur",
            "year": "2023",
            "abstract": "As the performance of single-channel speech separation systems has improved, there has been a shift in the research community towards tackling more challenging conditions that are more representative of many real-world applications, including the addition of noise and reverberation. The need for ground truth in training state-of-the-art separation systems leads to a requirement of training on artificial mixtures, where single-speaker recordings are summed digitally. However, this leads to two separate approaches for creating noisy mixtures: one in which noise has been artificially added, maintaining perfect ground truth information, and one in which the noise is already present in the single-speaker recordings, allowing for in-domain training. In this work, we document a severe negative impact in both training and evaluation of models in the latter paradigm. We provide an explanation for this \u2013 the implicit task of\u00a0\u2026",
            "url": null,
            "publicationVenue": "Computer Speech & Language",
            "citation_count": 0
        }
    ],
    "Sean J Welleck": [
        {
            "title": "STEER: Unified Style Transfer with Expert Reinforcement",
            "authors": "Skyler Hallinan, Faeze Brahman, Ximing Lu, Jaehun Jung, Sean Welleck, Yejin Choi",
            "year": "2023",
            "abstract": "While text style transfer has many applications across natural language processing, the core premise of transferring from a single source style is unrealistic in a real-world setting. In this work, we focus on arbitrary style transfer: rewriting a text from an arbitrary, unknown style to a target style. We propose STEER: Unified Style Transfer with Expert Reinforcement, a unified frame-work developed to overcome the challenge of limited parallel data for style transfer. STEER involves automatically generating a corpus of style-transfer pairs using a product of experts during decoding. The generated offline data is then used to pre-train an initial policy before switching to online, off-policy reinforcement learning for further improvements via fine-grained reward signals. STEER is unified and can transfer to multiple target styles from an arbitrary, unknown source style, making it particularly flexible and efficient. Experimental results on a challenging dataset with text from a diverse set of styles demonstrate state-of-the-art results compared to competitive baselines. Remarkably, STEER outperforms the 175B parameter instruction-tuned GPT-3 on overall style transfer quality, despite being 226 times smaller in size. We also show STEER is robust, maintaining its style transfer capabilities on out-of-domain data, and surpassing nearly all baselines across various styles. The success of our method highlights the potential of RL algorithms when augmented with controllable decoding to overcome the challenge of limited data supervision.",
            "url": "https://arxiv.org/pdf/2311.07167",
            "publicationVenue": "arXiv preprint arXiv:2311.07167",
            "citation_count": 0
        },
        {
            "title": "llmstep: LLM proofstep suggestions in lean",
            "authors": "Sean Welleck, Rahul Saha",
            "year": "2023",
            "abstract": "We present LLMSTEP, a tool for integrating a language model into the Lean proof assistant. LLMSTEP is a Lean 4 tactic that sends a user's proof state to a server hosting a language model. The language model generates suggestions, which are checked in Lean and displayed to a user in their development environment. We provide a baseline language model, along with code for fine-tuning and evaluation to support further development. We provide server implementations that run on CPU, a CUDA GPU, or a Google Colab notebook, as a step towards fast, effective language model suggestions for any user.",
            "url": "https://arxiv.org/pdf/2310.18457",
            "publicationVenue": "The 3rd Workshop on Mathematical Reasoning and AI at NeurIPS'23",
            "citation_count": 3
        },
        {
            "title": "Electronic device for obtaining sentence corresponding to context information and operating method thereof",
            "authors": null,
            "year": "2023",
            "abstract": "A method of obtaining, by an electronic device, a sentence corresponding to context information, including obtaining first output information including at least one word output by decoding the context information based on at least one data; based on detecting that a first token is not included in the first output information, determining whether a number of words included in the first output information is greater than or equal to a reference value; based on a result of the determining, replacing the at least one data with other data; and obtaining the sentence corresponding to the context information based on at least one output information obtained by decoding the context information based on the other data.",
            "url": "https://patentimages.storage.googleapis.com/63/31/5c/051a13c7126df6/US11669694.pdf",
            "publicationVenue": null,
            "citation_count": 1
        },
        {
            "title": "Faith and Fate: Limits of Transformers on Compositionality",
            "authors": "Nouha Dziri, Ximing Lu, Melanie Sclar, Xiang Lorraine Li, Liwei Jian, Bill Yuchen Lin, Peter West, Chandra Bhagavatula, Ronan Le Bras, Jena D Hwang, Soumya Sanyal, Sean Welleck, Xiang Ren, Allyson Ettinger, Zaid Harchaoui, Yejin Choi",
            "year": "2023",
            "abstract": "Transformer large language models (LLMs) have sparked admiration for their exceptional performance on tasks that demand intricate multi-step reasoning. Yet, these models simultaneously show failures on surprisingly trivial problems. This begs the question: Are these errors incidental, or do they signal more substantial limitations? In an attempt to demystify transformer LLMs, we investigate the limits of these models across three representative compositional tasks---multi-digit multiplication, logic grid puzzles, and a classic dynamic programming problem. These tasks require breaking problems down into sub-steps and synthesizing these steps into a precise answer. We formulate compositional tasks as computation graphs to systematically quantify the level of complexity, and break down reasoning steps into intermediate sub-procedures. Our empirical findings suggest that transformer LLMs solve compositional tasks by reducing multi-step compositional reasoning into linearized subgraph matching, without necessarily developing systematic problem-solving skills. To round off our empirical study, we provide theoretical arguments on abstract multi-step reasoning problems that highlight how autoregressive generations' performance can rapidly decay with increased task complexity.",
            "url": "https://proceedings.neurips.cc/paper_files/paper/2023/file/deb3c28192f979302c157cb653c15e90-Paper-Conference.pdf",
            "publicationVenue": "NeurIPS 2023",
            "citation_count": 108
        },
        {
            "title": "Inference-Time Policy Adapters (IPA): Tailoring Extreme-Scale LMs without Fine-tuning",
            "authors": "Ximing Lu, Faeze Brahman, Peter West, Jaehun Jang, Khyathi Chandu, Abhilasha Ravichander, Lianhui Qin, Prithviraj Ammanabrolu, Liwei Jiang, Sahana Ramnath, Nouha Dziri, Jillian Fisher, Bill Yuchen Lin, Skyler Hallinan, Xiang Ren, Sean Welleck, Yejin Choi",
            "year": "2023",
            "abstract": "Large language models excel at a variety of language tasks when prompted with examples or instructions. Yet controlling these models through prompting alone is limited. Tailoring language models through fine-tuning (e.g., via reinforcement learning) can be effective, but it is expensive and requires model access. We propose Inference-time Policy Adapters (IPA), which efficiently tailors a language model such as GPT-3 without fine-tuning it. IPA guides a large base model during decoding time through a lightweight policy adaptor trained to optimize an arbitrary user objective with reinforcement learning. On five challenging text generation tasks, such as toxicity reduction and open-domain generation, IPA consistently brings significant improvements over off-the-shelf language models. It outperforms competitive baseline methods, sometimes even including expensive fine-tuning. In particular, tailoring GPT-2 with IPA can outperform GPT-3, while tailoring GPT- 3 with IPA brings a major performance boost over GPT-3 (and sometimes even over GPT-4). Our promising results highlight the potential of IPA as a lightweight alternative to tailoring extreme-scale language models.",
            "url": "https://arxiv.org/pdf/2305.15065",
            "publicationVenue": "arXiv preprint arXiv:2305.15065",
            "citation_count": 3
        },
        {
            "title": "Self-refine: Iterative refinement with self-feedback",
            "authors": "Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Sean Welleck, Bodhisattwa Prasad Majumder, Shashank Gupta, Amir Yazdanbakhsh, Peter Clark",
            "year": "2023",
            "abstract": "Like humans, large language models (LLMs) do not always generate the best output on their first try. Motivated by how humans refine their written text, we introduce Self-Refine, an approach for improving initial outputs from LLMs through iterative feedback and refinement. The main idea is to generate an initial output using an LLMs; then, the same LLMs provides* feedback* for its output and uses it to* refine* itself, iteratively. Self-Refine does not require any supervised training data, additional training, or reinforcement learning, and instead uses a single LLM as the generator, refiner and the feedback provider. We evaluate Self-Refine across 7 diverse tasks, ranging from dialog response generation to mathematical reasoning, using state-of-the-art (GPT-3.5, ChatGPT, and GPT-4) LLMs. Across all evaluated tasks, outputs generated with Self-Refine are preferred by humans and automatic metrics over those generated with the same LLM using conventional one-step generation, improving by20\\% absolute on average in task performance. Our work demonstrates that even state-of-the-art LLMs like GPT-4 can be further improved at test-time using our simple, standalone approach.",
            "url": "https://proceedings.neurips.cc/paper_files/paper/2023/file/91edff07232fb1b55a505a9e9f6c0ff3-Paper-Conference.pdf",
            "publicationVenue": "NeurIPS 2023",
            "citation_count": 367
        },
        {
            "title": "Neural theorem proving tutorial",
            "authors": "Sean Welleck",
            "year": "2023",
            "abstract": null,
            "url": null,
            "publicationVenue": null,
            "citation_count": 2
        },
        {
            "title": "Mauve scores for generative models: Theory and practice",
            "authors": "Krishna Pillutla, Lang Liu, John Thickstun, Sean Welleck, Swabha Swayamdipta, Rowan Zellers, Sewoong Oh, Yejin Choi, Zaid Harchaoui",
            "year": "2023",
            "abstract": "Generative artificial intelligence has made significant strides, producing text indistinguishable from human prose and remarkably photorealistic images. Automatically measuring how close the generated data distribution is to the target distribution is central to diagnosing existing models and developing better ones. We present MAUVE, a family of comparison measures between pairs of distributions such as those encountered in the generative modeling of text or images. These scores are statistical summaries of divergence frontiers capturing two types of errors in generative modeling. We explore three approaches to statistically estimate these scores: vector quantization, non-parametric estimation, and classifier-based estimation. We provide statistical bounds for the vector quantization approach. Empirically, we find that the proposed scores paired with a range of-divergences and statistical estimation methods can quantify the gaps between the distributions of human-written text and those of modern neural language models by correlating with human judgments and identifying known properties of the generated texts. We demonstrate in the vision domain that MAUVE can identify known properties of generated images on par with or better than existing metrics. In conclusion, we present practical recommendations for using MAUVE effectively with language and image modalities.",
            "url": "https://www.jmlr.org/papers/volume24/23-0023/23-0023.pdf",
            "publicationVenue": "Journal of Machine Learning Research",
            "citation_count": 8
        }
    ],
    "Eric Xing": [
        {
            "title": "Llm360: Towards fully transparent open-source llms",
            "authors": "Zhengzhong Liu, Aurick Qiao, Willie Neiswanger, Hongyi Wang, Bowen Tan, Tianhua Tao, Junbo Li, Yuqi Wang, Suqi Sun, Omkar Pangarkar, Richard Fan, Yi Gu, Victor Miller, Yonghao Zhuang, Guowei He, Haonan Li, Fajri Koto, Liping Tang, Nikhil Ranjan, Zhiqiang Shen, Xuguang Ren, Roberto Iriondo, Cun Mu, Zhiting Hu, Mark Schulze, Preslav Nakov, Tim Baldwin, Eric P Xing",
            "year": "2023",
            "abstract": "The recent surge in open-source Large Language Models (LLMs), such as LLaMA, Falcon, and Mistral, provides diverse options for AI practitioners and researchers. However, most LLMs have only released partial artifacts, such as the final model weights or inference code, and technical reports increasingly limit their scope to high-level design choices and surface statistics. These choices hinder progress in the field by degrading transparency into the training of LLMs and forcing teams to rediscover many details in the training process. We present LLM360, an initiative to fully open-source LLMs, which advocates for all training code and data, model checkpoints, and intermediate results to be made available to the community. The goal of LLM360 is to support open and collaborative AI research by making the end-to-end LLM training process transparent and reproducible by everyone. As a first step of LLM360, we release two 7B parameter LLMs pre-trained from scratch, Amber and CrystalCoder, including their training code, data, intermediate checkpoints, and analyses (at https://www.llm360.ai). We are committed to continually pushing the boundaries of LLMs through this open-source effort. More large-scale and stronger models are underway and will be released in the future.",
            "url": "https://arxiv.org/pdf/2312.06550",
            "publicationVenue": "arXiv preprint arXiv:2312.06550",
            "citation_count": 12
        },
        {
            "title": "A Study on the Calibration of In-context Learning",
            "authors": "Hanlin Zhang, Yi-Fan Zhang, Yaodong Yu, Dhruv Madeka, Dean Foster, Eric Xing, Hima Lakkaraju, Sham Kakade",
            "year": "2023",
            "abstract": "Modern auto-regressive language models are trained to minimize log loss on broad data by predicting the next token so they are expected to get calibrated answers when framing a problem as a next-token prediction task. We study this for in-context learning (ICL), a widely used way to adapt frozen large language models (LLMs) via crafting prompts, and investigate the trade-offs between performance and calibration on a wide range of natural language understanding and reasoning tasks. We conduct extensive experiments to show that such trade-offs may get worse as we increase model size, incorporate more ICL examples, and fine-tune models using instruction, dialog, or reinforcement learning from human feedback (RLHF) on carefully curated datasets. Furthermore, we find that common recalibration techniques that are widely effective such as temperature scaling provide limited gains in calibration errors, suggesting that new methods may be required for settings where models are expected to be reliable.",
            "url": "https://arxiv.org/html/2312.04021v3",
            "publicationVenue": "arXiv preprint arXiv:2312.04021",
            "citation_count": 1
        },
        {
            "title": "Linker-Tuning: Optimizing Continuous Prompts for Heterodimeric Protein Prediction",
            "authors": "Shuxian Zou, Hui Li, Shentong Mo, Xingyi Cheng, Eric Xing, Le Song",
            "year": "2023",
            "abstract": "Predicting the structure of interacting chains is crucial for understanding biological systems and developing new drugs. Large-scale pre-trained Protein Language Models (PLMs), such as ESM2, have shown impressive abilities in extracting biologically meaningful representations for protein structure prediction. In this paper, we show that ESMFold, which has been successful in computing accurate atomic structures for single-chain proteins, can be adapted to predict the heterodimer structures in a lightweight manner. We propose Linker-tuning, which learns a continuous prompt to connect the two chains in a dimer before running it as a single sequence in ESMFold. Experiment results show that our method successfully predicts 56.98% of interfaces on the i.i.d. heterodimer test set, with an absolute improvement of +12.79% over the ESMFold-Linker baseline. Furthermore, our model can generalize well to the out-of-distribution (OOD) test set HeteroTest2 and two antibody test sets Fab and Fv while beingfaster than AF-Multimer.",
            "url": "https://arxiv.org/html/2312.01186v1",
            "publicationVenue": "arXiv preprint arXiv:2312.01186",
            "citation_count": 0
        },
        {
            "title": "Lq-lora: Low-rank plus quantized matrix decomposition for efficient language model finetuning",
            "authors": "Han Guo, Philip Greengard, Eric P Xing, Yoon Kim",
            "year": "2023",
            "abstract": "We propose a simple approach for memory-efficient adaptation of pretrained language models. Our approach uses an iterative algorithm to decompose each pretrained matrix into a high-precision low-rank component and a memory-efficient quantized component. During finetuning, the quantized component remains fixed and only the low-rank component is updated. We present an integer linear programming formulation of the quantization component which enables dynamic configuration of quantization parameters (e.g., bit-width, block size) for each matrix given an overall target memory budget. We further explore a data-aware version of the algorithm which uses an approximation of the Fisher information matrix to weight the reconstruction objective during matrix decomposition. Experiments on adapting RoBERTa and LLaMA-2 (7B and 70B) demonstrate that our low-rank plus quantized matrix decomposition approach (LQ-LoRA) outperforms strong QLoRA and GPTQ-LoRA baselines and moreover enables more aggressive quantization. For example, on the OpenAssistant benchmark LQ-LoRA is able to learn a 2.5-bit LLaMA-2 model that is competitive with a model finetuned with 4-bit QLoRA. When finetuned on a language modeling calibration dataset, LQ-LoRA can also be used for model compression; in this setting our 2.75-bit LLaMA-2-70B model (which has 2.85 bits on average when including the low-rank components and requires 27GB of GPU memory) is competitive with the original model in full precision.",
            "url": "https://arxiv.org/html/2311.12023v2",
            "publicationVenue": "arXiv preprint arXiv:2311.12023",
            "citation_count": 4
        },
        {
            "title": "SegMix: A Simple Structure-Aware Data Augmentation Method",
            "authors": "Yuxin Pei, Pushkar Bhuse, Zhengzhong Liu, Eric Xing",
            "year": "2023",
            "abstract": "Interpolation-based Data Augmentation (DA) methods (Mixup) linearly interpolate the inputs and labels of two or more training examples. Mixup has more recently been adapted to the field of Natural Language Processing (NLP), mainly for sequence labeling tasks. However, such a simple adoption yields mixed or unstable improvements over the baseline models. We argue that the direct-adoption methods do not account for structures in NLP tasks. To this end, we propose SegMix, a collection of interpolation-based DA algorithms that can adapt to task-specific structures. SegMix poses fewer constraints on data structures, is robust to various hyperparameter settings, applies to more task settings, and adds little computational overhead. In the algorithm's core, we apply interpolation methods on task-specific meaningful segments, in contrast to applying them on sequences as in prior work. We find SegMix to be a flexible framework that combines rule-based DA methods with interpolation-based methods, creating interesting mixtures of DA techniques. We show that SegMix consistently improves performance over strong baseline models in Named Entity Recognition (NER) and Relation Extraction (RE) tasks, especially under data-scarce settings. Furthermore, this method is easy to implement and adds negligible training overhead.",
            "url": "https://arxiv.org/pdf/2311.09505",
            "publicationVenue": "arXiv preprint arXiv:2311.09505",
            "citation_count": 0
        },
        {
            "title": "Promptagent: Strategic planning with language models enables expert-level prompt optimization",
            "authors": "Xinyuan Wang, Chenxi Li, Zhen Wang, Fan Bai, Haotian Luo, Jiayou Zhang, Nebojsa Jojic, Eric P Xing, Zhiting Hu",
            "year": "2023",
            "abstract": "Highly effective, task-specific prompts are often heavily engineered by experts to integrate detailed instructions and domain insights based on a deep understanding of both instincts of large language models (LLMs) and the intricacies of the target task. However, automating the generation of such expert-level prompts remains elusive. Existing prompt optimization methods tend to overlook the depth of domain knowledge and struggle to efficiently explore the vast space of expert-level prompts. Addressing this, we present PromptAgent, an optimization method that autonomously crafts prompts equivalent in quality to those handcrafted by experts. At its core, PromptAgent views prompt optimization as a strategic planning problem and employs a principled planning algorithm, rooted in Monte Carlo tree search, to strategically navigate the expert-level prompt space. Inspired by human-like trial-and-error exploration, PromptAgent induces precise expert-level insights and in-depth instructions by reflecting on model errors and generating constructive error feedback. Such a novel framework allows the agent to iteratively examine intermediate prompts (states), refine them based on error feedbacks (actions), simulate future rewards, and search for high-reward paths leading to expert prompts. We apply PromptAgent to 12 tasks spanning three practical domains: BIG-Bench Hard (BBH), as well as domain-specific and general NLP tasks, showing it significantly outperforms strong Chain-of-Thought and recent prompt optimization baselines. Extensive analyses emphasize its capability to craft expert-level, detailed, and domain-insightful prompts with great\u00a0\u2026",
            "url": "https://arxiv.org/pdf/2310.16427",
            "publicationVenue": "arXiv preprint arXiv:2310.16427",
            "citation_count": 11
        },
        {
            "title": "Redco: A Lightweight Tool to Automate Distributed Training of LLMs on Any GPU/TPUs",
            "authors": "Bowen Tan, Yun Zhu, Lijuan Liu, Hongyi Wang, Yonghao Zhuang, Jindong Chen, Eric Xing, Zhiting Hu",
            "year": "2023",
            "abstract": "The recent progress of AI can be largely attributed to large language models (LLMs). However, their escalating memory requirements introduce challenges for machine learning (ML) researchers and engineers. Addressing this requires developers to partition a large model to distribute it across multiple GPUs or TPUs. This necessitates considerable coding and intricate configuration efforts with existing model parallel tools, such as Megatron-LM, DeepSpeed, and Alpa. These tools require users' expertise in machine learning systems (MLSys), creating a bottleneck in LLM development, particularly for developers without MLSys background. In this work, we present Redco, a lightweight and user-friendly tool crafted to automate distributed training and inference for LLMs, as well as to simplify ML pipeline development. The design of Redco emphasizes two key aspects. Firstly, to automate model parallism, our study identifies two straightforward rules to generate tensor parallel strategies for any given LLM. Integrating these rules into Redco facilitates effortless distributed LLM training and inference, eliminating the need of additional coding or complex configurations. We demonstrate the effectiveness by applying Redco on a set of LLM architectures, such as GPT-J, LLaMA, T5, and OPT, up to the size of 66B. Secondly, we propose a mechanism that allows for the customization of diverse ML pipelines through the definition of merely three functions, eliminating redundant and formulaic code like multi-host related processing. This mechanism proves adaptable across a spectrum of ML algorithms, from foundational language modeling to complex\u00a0\u2026",
            "url": "https://arxiv.org/pdf/2310.16355",
            "publicationVenue": "arXiv preprint arXiv:2310.16355",
            "citation_count": 0
        },
        {
            "title": "Contextualized machine learning",
            "authors": "Benjamin Lengerich, Caleb N Ellington, Andrea Rubbi, Manolis Kellis, Eric P Xing",
            "year": "2023",
            "abstract": "We examine Contextualized Machine Learning (ML), a paradigm for learning heterogeneous and context-dependent effects. Contextualized ML estimates heterogeneous functions by applying deep learning to the meta-relationship between contextual information and context-specific parametric models. This is a form of varying-coefficient modeling that unifies existing frameworks including cluster analysis and cohort modeling by introducing two reusable concepts: a context encoder which translates sample context into model parameters, and sample-specific model which operates on sample predictors. We review the process of developing contextualized models, nonparametric inference from contextualized models, and identifiability conditions of contextualized models. Finally, we present the open-source PyTorch package ContextualizedML.",
            "url": "https://arxiv.org/pdf/2310.11340",
            "publicationVenue": "arXiv preprint arXiv:2310.11340",
            "citation_count": 2
        },
        {
            "title": "RealChat-1M: A Large-Scale Real-World LLM Conversation Dataset",
            "authors": "Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Tianle Li, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zhuohan Li, Zi Lin, Eric Xing, Joseph E Gonzalez, Ion Stoica, Hao Zhang",
            "year": "2023",
            "abstract": "Studying how people interact with large language models (LLMs) in real-world scenarios is increasingly important due to their widespread use in various applications. In this paper, we introduce RealChat-1M, a large-scale dataset containing one million real-world conversations with 25 state-of-the-art LLMs. This dataset is collected from 210K unique IP addresses in the wild on our chat demo website. We offer an overview of the dataset's content, including its curation process, basic statistics, and topic distribution, highlighting its diversity, originality, and scale. We demonstrate its versatility through four use cases: developing content moderation models that perform similarly to GPT-4, building a safety benchmark, training instruction-following models that perform similarly to Vicuna, and creating challenging benchmark questions. We believe that this dataset will serve as a valuable resource for understanding and advancing LLM capabilities. The dataset will be publicly available.",
            "url": "https://openreview.net/pdf?id=BOfDKxfwt0",
            "publicationVenue": null,
            "citation_count": 0
        },
        {
            "title": "Contextualized Policy Recovery: Modeling and Interpreting Medical Decisions with Adaptive Imitation Learning",
            "authors": "Caleb Ellington, Jannik Deuschel, Ben Lengerich, Yingtao Luo, Pascal Friederich, Eric Xing",
            "year": "2023",
            "abstract": "Interpretable policy learning seeks to estimate intelligible decision policies from observed actions; however, existing models fall short by forcing a tradeoff between accuracy and interpretability.  This tradeoff limits data-driven interpretations of human decision-making process. e.g. to audit medical decisions for biases and suboptimal practices, we require models of decision processes which provide concise descriptions of complex behaviors.  Fundamentally, existing approaches are burdened by this tradeoff because they represent the underlying decision process as a universal policy, when in fact human decisions are dynamic and can change drastically with contextual information.  Thus, we propose Contextualized Policy Recovery (CPR), which re-frames the problem of modeling complex decision processes as a multi-task learning problem in which complex decision policies are comprised of context-specific policies.  CPR models each context-specific policy as a linear observation-to-action mapping,  and generates new decision models \\textit{on-demand} as contexts are updated with new observations. CPR is compatible with fully offline and partially observable decision environments, and can be tailored to incorporate any recurrent black-box model or interpretable decision model.  We assess CPR through studies on simulated and real data, achieving state-of-the-art performance on the canonical tasks of predicting antibiotic prescription in intensive care units (% AUROC vs. previous SOTA) and predicting MRI prescription for Alzheimer's patients (% AUROC vs. previous SOTA). With this improvement in predictive performance, CPR\u00a0\u2026",
            "url": "https://openreview.net/pdf?id=rNvyMAV8Aw",
            "publicationVenue": null,
            "citation_count": 0
        },
        {
            "title": "Memoization-Aware Bayesian Optimization for AI Pipelines with Unknown Costs",
            "authors": "Abdelmajid Essofi, Ridwan Salahuddeen, Munachiso S Nwadike, Navish Kumar, Kun Zhang, Eric Xing, Willie Neiswanger, Qirong Ho",
            "year": "2023",
            "abstract": "Bayesian optimization (BO) is an effective approach for optimizing expensive black-box functions via potentially noisy function evaluations. However, few BO techniques address the cost-aware setting, in which different samples impose different costs on the optimizer, particularly when costs are initially unknown. This cost-aware BO setting is of special interest in tuning multi-stage AI pipelines, in which we could apply caching techniques to store and reuse early-stage outputs in favor of optimizing later stages, without incurring the costs of re-running the full pipeline. In this paper, we propose the Expected-Expected Improvement Per Unit Cost (EEIPU), a novel extension to the Expected Improvement (EI) acquisition function that adapts to unknown costs in multi-stage pipelines. EEIPU fits individual Gaussian Process (GP) models for each stage's cost data and manages the different cost regions of the search space, while balancing exploration-exploitation trade-offs. Additionally, EEIPU incorporates early-stage memoization, reducing redundant computations and costs by reusing the results of earlier stages, allowing for more iterations than existing approaches within the specified budget. In the cost-aware setting, EEIPU significantly outperforms comparable methods when tested on both synthetic and real pipelines, returning higher objective function values at lower total execution costs. This offers a significant advancement in cost-aware BO for optimizing multi-stage machine learning pipelines.",
            "url": "https://openreview.net/pdf?id=fLWqIWPMDH",
            "publicationVenue": null,
            "citation_count": 0
        },
        {
            "title": "Liteformer: Lightweight Evoformer for Protein Structure Prediction",
            "authors": "Ning Sun, Xingyi Cheng, Shentong Mo, Chiming Liu, Hui Li, Eric Xing, Le Song",
            "year": "2023",
            "abstract": "AlphaFold2 has achieved seminal success in predicting structures from amino acid sequences with remarkable atomic accuracy. However, its Evoformer module faces a critical challenge in terms of high memory consumption, particularly concerning the computational complexity associated with sequence lengthand the number of Multiple Sequence Alignments (MSA), denoted as. This challenge arises from the attention mechanism involving third-order MSA and pair-wise tensors, leading to a complexity of. This memory bottleneck poses difficulties when working with lengthy protein sequences. To tackle this problem, we introduce a novel and lightweight variant of Evoformer named Liteformer. Liteformer employs an innovative attention linearization mechanism, reducing complexity tothrough the implementation of a bias-aware flow attention mechanism, which seamlessly integrates MSA sequences and pair-wise information. Our extensive experiments, conducted on both monomeric and multimeric benchmark datasets, showcase the efficiency gains of our framework.  Specifically, compared with Evoformer, Liteformer achieves up to a 44\\% reduction in memory usage and a 23\\% acceleration in training speed, all while maintaining competitive accuracy in protein structure prediction.",
            "url": "https://openreview.net/pdf?id=t0m0DdCCQ2",
            "publicationVenue": null,
            "citation_count": 0
        },
        {
            "title": "Contextualized Policy Recovery: Modeling and Interpreting Medical Decisions with Adaptive Imitation Learning",
            "authors": "Jannik Deuschel, Caleb N Ellington, Benjamin J Lengerich, Yingtao Luo, Pascal Friederich, Eric P Xing",
            "year": "2023",
            "abstract": "Interpretable policy learning seeks to estimate intelligible decision policies from observed actions; however, existing models fall short by forcing a tradeoff between accuracy and interpretability. This tradeoff limits data-driven interpretations of human decision-making process. e.g. to audit medical decisions for biases and suboptimal practices, we require models of decision processes which provide concise descriptions of complex behaviors. Fundamentally, existing approaches are burdened by this tradeoff because they represent the underlying decision process as a universal policy, when in fact human decisions are dynamic and can change drastically with contextual information. Thus, we propose Contextualized Policy Recovery (CPR), which re-frames the problem of modeling complex decision processes as a multi-task learning problem in which complex decision policies are comprised of context-specific policies. CPR models each context-specific policy as a linear observation-to-action mapping, and generates new decision modelsas contexts are updated with new observations. CPR is compatible with fully offline and partially observable decision environments, and can be tailored to incorporate any recurrent black-box model or interpretable decision model. We assess CPR through studies on simulated and real data, achieving state-of-the-art performance on the canonical tasks of predicting antibiotic prescription in intensive care units (AUROC vs. previous SOTA) and predicting MRI prescription for Alzheimer's patients (AUROC vs. previous SOTA). With this improvement in predictive performance, CPR closes the\u00a0\u2026",
            "url": "https://arxiv.org/html/2310.07918v3",
            "publicationVenue": "arXiv preprint arXiv:2310.07918",
            "citation_count": 1
        },
        {
            "title": "Lightseq: Sequence level parallelism for distributed training of long context transformers",
            "authors": "Dacheng Li, Rulin Shao, Anze Xie, Eric P Xing, Joseph E Gonzalez, Ion Stoica, Xuezhe Ma, Hao Zhang",
            "year": "2023",
            "abstract": "Increasing the context length of large language models (LLMs) unlocks fundamentally new capabilities, but also significantly increases the memory footprints of training. Previous model-parallel systems such as Megatron-LM partition and compute different attention heads in parallel, resulting in large communication volumes, so they cannot scale beyond the number of attention heads, thereby hindering its adoption. In this paper, we introduce a new approach, LightSeq, for long-context LLMs training. LightSeq has many notable advantages. First, LightSeq partitions over the sequence dimension, hence is agnostic to model architectures and readily applicable for models with varying numbers of attention heads, such as Multi-Head, Multi-Query and Grouped-Query attention. Second, LightSeq not only requires up to 4.7x less communication than Megatron-LM on popular LLMs but also overlaps the communication with computation. To further reduce the training time, LightSeq features a novel gradient checkpointing scheme to bypass an forward computation for memory-efficient attention. We evaluate LightSeq on Llama-7B and its variants with sequence lengths from 32K to 512K. Through comprehensive experiments on single and cross-node training, we show that LightSeq achieves up to 1.24-2.01x end-to-end speedup, and a 2-8x longer sequence length on models with fewer heads, compared to Megatron-LM. Codes will be available at https://github.com/RulinShao/LightSeq.",
            "url": "https://arxiv.org/pdf/2310.03294",
            "publicationVenue": "arXiv preprint arXiv:2310.03294",
            "citation_count": 3
        },
        {
            "title": "Fusing Models with Complementary Expertise",
            "authors": "Hongyi Wang, Felipe Maia Polo, Yuekai Sun, Souvik Kundu, Eric Xing, Mikhail Yurochkin",
            "year": "2023",
            "abstract": "Training AI models that generalize across tasks and domains has long been among the open problems driving AI research. The emergence of Foundation Models made it easier to obtain expert models for a given task, but the heterogeneity of data that may be encountered at test time often means that any single expert is insufficient. We consider the Fusion of Experts (FoE) problem of fusing outputs of expert models with complementary knowledge of the data distribution and formulate it as an instance of supervised learning. Our method is applicable to both discriminative and generative tasks and leads to significant performance improvements in image and text classification, text summarization, multiple-choice QA, and automatic evaluation of generated text. We also extend our method to the \"frugal\" setting where it is desired to reduce the number of expert model evaluations at test time.",
            "url": "https://arxiv.org/pdf/2310.01542",
            "publicationVenue": "arXiv preprint arXiv:2310.01542",
            "citation_count": 1
        },
        {
            "title": "Lmsys-chat-1m: A large-scale real-world llm conversation dataset",
            "authors": "Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Tianle Li, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zhuohan Li, Zi Lin, Eric Xing, Joseph E Gonzalez, Ion Stoica, Hao Zhang",
            "year": "2023",
            "abstract": "Studying how people interact with large language models (LLMs) in real-world scenarios is increasingly important due to their widespread use in various applications. In this paper, we introduce LMSYS-Chat-1M, a large-scale dataset containing one million real-world conversations with 25 state-of-the-art LLMs. This dataset is collected from 210K unique IP addresses in the wild on our Vicuna demo and Chatbot Arena website. We offer an overview of the dataset's content, including its curation process, basic statistics, and topic distribution, highlighting its diversity, originality, and scale. We demonstrate its versatility through four use cases: developing content moderation models that perform similarly to GPT-4, building a safety benchmark, training instruction-following models that perform similarly to Vicuna, and creating challenging benchmark questions. We believe that this dataset will serve as a valuable resource for understanding and advancing LLM capabilities. The dataset is publicly available at https://huggingface.co/datasets/lmsys/lmsys-chat-1m.",
            "url": "https://arxiv.org/pdf/2309.11998",
            "publicationVenue": "arXiv preprint arXiv:2309.11998",
            "citation_count": 22
        },
        {
            "title": "Slimpajama-dc: Understanding data combinations for llm training",
            "authors": "Zhiqiang Shen, Tianhua Tao, Liqun Ma, Willie Neiswanger, Joel Hestness, Natalia Vassilieva, Daria Soboleva, Eric Xing",
            "year": "2023",
            "abstract": "This paper aims to understand the impacts of various data combinations (e.g., web text, wikipedia, github, books) on the training of large language models using SlimPajama. SlimPajama is a rigorously deduplicated, multi-source dataset, which has been refined and further deduplicated to 627B tokens from the extensive 1.2T tokens RedPajama dataset contributed by Together. We've termed our research as SlimPajama-DC, an empirical analysis designed to uncover fundamental characteristics and best practices associated with employing SlimPajama in the training of large language models. During our research with SlimPajama, two pivotal observations emerged: (1) Global deduplication vs. local deduplication. We analyze and discuss how global (across different sources of datasets) and local (within the single source of dataset) deduplications affect the performance of trained models. (2) Proportions of high-quality/highly-deduplicated multi-source datasets in the combination. To study this, we construct six configurations of SlimPajama dataset and train individual ones using 1.3B Cerebras-GPT model with Alibi and SwiGLU. Our best configuration outperforms the 1.3B model trained on RedPajama using the same number of training tokens by a significant margin. All our 1.3B models are trained on Cerebras 16CS-2 cluster with a total of 80 PFLOP/s in bf16 mixed precision. We further extend our discoveries (such as increasing data diversity is crucial after global deduplication) on a 7B model with large batch-size training. Our models and the separate SlimPajama-DC datasets are available at: https://huggingface.co/MBZUAI-LLM and https\u00a0\u2026",
            "url": "https://arxiv.org/pdf/2309.10818",
            "publicationVenue": "arXiv preprint arXiv:2309.10818",
            "citation_count": 9
        },
        {
            "title": "Jais and jais-chat: Arabic-centric foundation and instruction-tuned open generative large language models",
            "authors": "Neha Sengupta, Sunil Kumar Sahu, Bokang Jia, Satheesh Katipomu, Haonan Li, Fajri Koto, Osama Mohammed Afzal, Samta Kamboj, Onkar Pandit, Rahul Pal, Lalit Pradhan, Zain Muhammad Mujahid, Massa Baali, Alham Fikri Aji, Zhengzhong Liu, Andy Hock, Andrew Feldman, Jonathan Lee, Andrew Jackson, Preslav Nakov, Timothy Baldwin, Eric Xing",
            "year": "2023",
            "abstract": "We introduce Jais and Jais-chat, new state-of-the-art Arabic-centric foundation and instruction-tuned open generative large language models (LLMs). The models are based on the GPT-3 decoder-only architecture and are pretrained on a mixture of Arabic and English texts, including source code in various programming languages. With 13 billion parameters, they demonstrate better knowledge and reasoning capabilities in Arabic than any existing open Arabic and multilingual models by a sizable margin, based on extensive evaluation. Moreover, the models are competitive in English compared to English-centric open models of similar size, despite being trained on much less English data. We provide a detailed description of the training, the tuning, the safety alignment, and the evaluation of the models. We release two open versions of the model -- the foundation Jais model, and an instruction-tuned Jais-chat variant -- with the aim of promoting research on Arabic LLMs. Available at https://huggingface.co/inception-mbzuai/jais-13b-chat",
            "url": "https://arxiv.org/pdf/2308.16149",
            "publicationVenue": "arXiv preprint arXiv:2308.16149",
            "citation_count": 13
        },
        {
            "title": "Multimodal image synthesis and editing: A survey and taxonomy",
            "authors": "Fangneng Zhan, Yingchen Yu, Rongliang Wu, Jiahui Zhang, Shijian Lu, Lingjie Liu, Adam Kortylewski, Christian Theobalt, Eric Xing",
            "year": "2023",
            "abstract": "As information exists in various modalities in real world, effective interaction and fusion among multimodal information plays a key role for the creation and perception of multimodal data in computer vision and deep learning research. With superb power in modeling the interaction among multimodal information, multimodal image synthesis and editing has become a hot research topic in recent years. Instead of providing explicit guidance for network training, multimodal guidance offers intuitive and flexible means for image synthesis and editing. On the other hand, this field is also facing several challenges in alignment of multimodal features, synthesis of high-resolution images, faithful evaluation metrics, etc. In this survey, we comprehensively contextualize the advance of the recent multimodal image synthesis and editing and formulate taxonomies according to data modalities and model types. We start with an\u00a0\u2026",
            "url": "https://www.researchgate.net/profile/Fangneng-Zhan/publication/362225295_Multimodal_Image_Synthesis_and_Editing_A_Survey_and_Taxonomy/links/64cd94c191fb036ba6c93a7f/Multimodal-Image-Synthesis-and-Editing-A-Survey-and-Taxonomy.pdf",
            "publicationVenue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "citation_count": 73
        },
        {
            "title": "Iterative graph self-distillation",
            "authors": "Hanlin Zhang, Shuai Lin, Weiyang Liu, Pan Zhou, Jian Tang, Xiaodan Liang, Eric P Xing",
            "year": "2023",
            "abstract": "Recently, there has been increasing interest in the challenge of how to discriminatively vectorize graphs. To address this, we propose a method called Iterative Graph Self-Distillation (IGSD) which learns graph-level representation in an unsupervised manner through instance discrimination using a self-supervised contrastive learning approach. IGSD involves a teacher-student distillation process that uses graph diffusion augmentations and constructs the teacher model using an exponential moving average of the student model. The intuition behind IGSD is to predict the teacher network representation of the graph pairs under different augmented views. As a natural extension, we also apply IGSD to semi-supervised scenarios by jointly regularizing the network with both supervised and self-supervised contrastive loss. Finally, we show that fine-tuning the IGSD-trained models with self-training can further improve\u00a0\u2026",
            "url": "https://arxiv.org/pdf/2010.12609",
            "publicationVenue": "IEEE Transactions on Knowledge and Data Engineering",
            "citation_count": 29
        },
        {
            "title": "Autonomous industrial process control system and method that provides autonomous retraining of forecast model",
            "authors": null,
            "year": "2023",
            "abstract": "The current disclosure is directed towards system and method for controlling industrial process. In one example, a method comprising deploying a forecast model for controlling an industrial process with training configurations that can be used as a single point of truth for guiding training and retraining versions of the forecast model using a model training algorithm without human input. The retraining and redeployment of the forecast model may be triggered when the performance of the forecast model degrades.",
            "url": "https://patentimages.storage.googleapis.com/84/3a/cc/5b83d01af642d0/US20210208545A1.pdf",
            "publicationVenue": null,
            "citation_count": 12
        },
        {
            "title": "Neural-symbolic interaction and co-evolving",
            "authors": "Bowen Tan, Shibo Hao, Eric Xing, Zhiting Hu",
            "year": "2023",
            "abstract": "Deep neural networks provide a powerful mechanism for learning patterns from massive data, achieving new levels of performance on image classification [24], speech recognition [25], machine translation [26], playing strategic board games [27], and so forth. Despite the impressive advances, the widely-used DNN methods still have limitations. The high predictive accuracy has heavily relied on large amounts of labeled data; and the purely data-driven learning can lead to uninterpretable and sometimes counterintuitive results [28, 29]. It is also difficult to encode human intention to guide the models to capture desired patterns, without expensive direct supervision or ad-hoc initialization. On the other hand, the cognitive process of human beings have indicated that people learn not only from concrete examples (as DNNs do) but also from different forms of general knowledge and rich experiences [30, 31]. Logic rules provide a flexible declarative language for communicating high-level cognition and expressing structured knowledge. It is therefore desirable to integrate logic rules into DNNs, to transfer human intention and domain knowledge to neural models, and regulate the learning process. In this section, we present a framework capable of enhancing general types of neural networks, such as convolutional networks (CNNs) and recurrent networks (RNNs), on various tasks, with logic rule knowledge. Combining symbolic representations with neural methods have been considered in different contexts. Neural-symbolic systems [32] construct a network from a given rule set to execute reasoning. To exploit a priori knowledge in general neural\u00a0\u2026",
            "url": null,
            "publicationVenue": "Compendium of Neurosymbolic Artificial Intelligence",
            "citation_count": 0
        },
        {
            "title": "Defending Against Malicious Behaviors in Federated Learning with Blockchain",
            "authors": "Nanqing Dong, Zhipeng Wang, Jiahao Sun, Michael Kampffmeyer, Yizhe Wen, Shuoying Zhang, William Knottenbelt, Eric Xing",
            "year": "2023",
            "abstract": "In the era of deep learning, federated learning (FL) presents a promising approach that allows multi-institutional data owners, or clients, to collaboratively train machine learning models without compromising data privacy. However, most existing FL approaches rely on a centralized server for global model aggregation, leading to a single point of failure. This makes the system vulnerable to malicious attacks when dealing with dishonest clients. In this work, we address this problem by proposing a secure and reliable FL system based on blockchain and distributed ledger technology. Our system incorporates a peer-to-peer voting mechanism and a reward-and-slash mechanism, which are powered by on-chain smart contracts, to detect and deter malicious behaviors. Both theoretical and empirical analyses are presented to demonstrate the effectiveness of the proposed approach, showing that our framework is robust against malicious client-side behaviors.",
            "url": "https://arxiv.org/pdf/2307.00543",
            "publicationVenue": "arXiv preprint arXiv:2307.00543",
            "citation_count": 3
        },
        {
            "title": "BertNet: Harvesting knowledge graphs with arbitrary relations from pretrained language models",
            "authors": "Shibo Hao, Bowen Tan, Kaiwen Tang, Bin Ni, Xiyan Shao, Hengzhe Zhang, Eric Xing, Zhiting Hu",
            "year": "2023",
            "abstract": "It is crucial to automatically construct knowledge graphs (KGs) of diverse new relations to support knowledge discovery and broad applications. Previous KG construction methods, based on either crowdsourcing or text mining, are often limited to a small predefined set of relations due to manual cost or restrictions in text corpus. Recent research proposed to use pretrained language models (LMs) as implicit knowledge bases that accept knowledge queries with prompts. Yet, the implicit knowledge lacks many desirable properties of a full-scale symbolic KG, such as easy access, navigation, editing, and quality assurance. In this paper, we propose a new approach of harvesting massive KGs of arbitrary relations from pretrained LMs. With minimal input of a relation definition (a prompt and a few shot of example entity pairs), the approach efficiently searches in the vast entity pair space to extract diverse accurate knowledge of the desired relation. We develop an effective search-and-rescore mechanism for improved efficiency and accuracy. We deploy the approach to harvest KGs of over 400 new relations, from LMs of varying capacities such as RoBERTaNet. Extensive human and automatic evaluations show our approach manages to extract diverse accurate knowledge, including tuples of complex relations (eg,\u201cA is capable of but not good at B\u201d). The resulting KGs as a symbolic interpretation of the source LMs also reveal new insights into the LMs\u2019 knowledge capacities.",
            "url": "https://aclanthology.org/2023.findings-acl.309.pdf",
            "publicationVenue": "Findings of the Association for Computational Linguistics: ACL 2023",
            "citation_count": 38
        },
        {
            "title": "One-for-All: Generalized LoRA for Parameter-Efficient Fine-tuning",
            "authors": "Arnav Chavan, Zhuang Liu, Deepak Gupta, Eric Xing, Zhiqiang Shen",
            "year": "2023",
            "abstract": "We present Generalized LoRA (GLoRA), an advanced approach for universal parameter-efficient fine-tuning tasks. Enhancing Low-Rank Adaptation (LoRA), GLoRA employs a generalized prompt module to optimize pre-trained model weights and adjust intermediate activations, providing more flexibility and capability across diverse tasks and datasets. Moreover, GLoRA facilitates efficient parameter adaptation by employing a scalable, modular, layer-wise structure search that learns individual adapter of each layer. Originating from a unified mathematical formulation, GLoRA exhibits strong transfer learning, few-shot learning and domain generalization abilities, as it adjusts to new tasks through additional dimensions on weights and activations. Comprehensive experiments demonstrate that GLoRA outperforms all previous methods in natural, specialized, and structured benchmarks, achieving superior accuracy with fewer parameters and computations on various datasets. Furthermore, our structural re-parameterization design ensures that GLoRA incurs no extra inference cost, rendering it a practical solution for resource-limited applications. Code is available at: https://github.com/Arnav0400/ViT-Slim/tree/master/GLoRA.",
            "url": "https://arxiv.org/pdf/2306.07967",
            "publicationVenue": "arXiv preprint arXiv:2306.07967",
            "citation_count": 27
        },
        {
            "title": "3D Open-vocabulary Segmentation with Foundation Models",
            "authors": "Kunhao Liu, Fangneng Zhan, Jiahui Zhang, Muyu Xu, Yingchen Yu, Abdulmotaleb El Saddik, Christian Theobalt, Eric Xing, Shijian Lu",
            "year": "2023",
            "abstract": "Open-vocabulary segmentation of 3D scenes is a fundamental function of human perception and thus a crucial objective in computer vision research. However, this task is heavily impeded by the lack of large-scale and diverse 3D open-vocabulary segmentation datasets for training robust and generalizable models. Distilling knowledge from pre-trained 2D open-vocabulary segmentation models helps but it compromises the open-vocabulary feature significantly as the 2D models are mostly finetuned with close-vocabulary datasets. We tackle the challenges in 3D open-vocabulary segmentation by exploiting the open-vocabulary multimodal knowledge and object reasoning capability of pre-trained foundation models CLIP and DINO, without necessitating any fine-tuning. Specifically, we distill open-vocabulary visual and textual knowledge from CLIP into a neural radiance field (NeRF) which effectively lifts 2D features into view-consistent 3D segmentation. Furthermore, we introduce the Relevancy-Distribution Alignment loss and Feature-Distribution Alignment loss to respectively mitigate the ambiguities of CLIP features and distill precise object boundaries from DINO features, eliminating the need for segmentation annotations during training. Extensive experiments show that our method even outperforms fully supervised models trained with segmentation annotations, suggesting that 3D open-vocabulary segmentation can be effectively learned from 2D images and text-image pairs.",
            "url": "https://arxiv.org/pdf/2305.14093",
            "publicationVenue": "arXiv preprint arXiv:2305.14093",
            "citation_count": 5
        },
        {
            "title": "Improved logical reasoning of language models via differentiable symbolic programming",
            "authors": "Hanlin Zhang, Jiani Huang, Ziyang Li, Mayur Naik, Eric Xing",
            "year": "2023",
            "abstract": "Pre-trained large language models (LMs) struggle to perform logical reasoning reliably despite advances in scale and compositionality. In this work, we tackle this challenge through the lens of symbolic programming. We propose DSR-LM, a Differentiable Symbolic Reasoning framework where pre-trained LMs govern the perception of factual knowledge, and a symbolic module performs deductive reasoning. In contrast to works that rely on hand-crafted logic rules, our differentiable symbolic reasoning framework efficiently learns weighted rules and applies semantic loss to further improve LMs. DSR-LM is scalable, interpretable, and allows easy integration of prior knowledge, thereby supporting extensive symbolic programming to robustly derive a logical conclusion. The results of our experiments suggest that DSR-LM improves the logical reasoning abilities of pre-trained language models, resulting in a significant increase in accuracy of over 20% on deductive reasoning benchmarks. Furthermore, DSR-LM outperforms a variety of competitive baselines when faced with systematic changes in sequence length.",
            "url": "https://arxiv.org/pdf/2305.03742",
            "publicationVenue": "arXiv preprint arXiv:2305.03742",
            "citation_count": 14
        },
        {
            "title": "Cuttlefish: Low-rank Model Training without All The Tuning",
            "authors": "Hongyi Wang, Saurabh Agarwal, Yoshiki Tanaka, Eric Xing, Dimitris Papailiopoulos",
            "year": "2023",
            "abstract": "Recent research has shown that training low-rank neural networks can effectively reduce the total number of trainable parameters without sacrificing predictive accuracy, resulting in end-to-end speedups. However, low-rank model training necessitates adjusting several additional factorization hyperparameters, such as the rank of the factorization at each layer. In this paper, we tackle this challenge by introducing Cuttlefish, an automated low-rank training approach that eliminates the need for tuning factorization hyperparameters. Cuttlefish leverages the observation that after a few epochs of full-rank training, the stable rank (ie, an approximation of the true rank) of each layer stabilizes at a constant value. Cuttlefish switches from full-rank to low-rank training once the stable ranks of all layers have converged, setting the dimension of each factorization to its corresponding stable rank. Our results show that Cuttlefish generates models up to 5.6 times smaller than full-rank models, and attains up to a 1.2 times faster end-to-end training process while preserving comparable accuracy. Moreover, Cuttlefish outperforms state-of-the-art low-rank model training methods and other prominent baselines. The source code for our implementation can be found at: https://github. com/hwang595/Cuttlefish.",
            "url": "https://proceedings.mlsys.org/paper_files/paper/2023/file/b38f833dd45718cc414bd9b2a1d86c9a-Paper-mlsys2023.pdf",
            "publicationVenue": "Proceedings of Machine Learning and Systems",
            "citation_count": 3
        },
        {
            "title": "On optimizing the communication of model parallelism",
            "authors": "Yonghao Zhuang, Hexu Zhao, Lianmin Zheng, Zhuohan Li, Eric Xing, Qirong Ho, Joseph Gonzalez, Ion Stoica, Hao Zhang",
            "year": "2023",
            "abstract": "We study a novel and important communication pattern in large-scale model-parallel deep learning (DL), which we call cross-mesh resharding. This pattern emerges when the two paradigms of model parallelism\u2013intra-operator and inter-operator parallelism\u2013are combined to support large models on large clusters. In cross-mesh resharding, a sharded tensor needs to be sent from a source device mesh to a destination device mesh, on which the tensor may be distributed with the same or different layouts. We formalize this as a many-to-many multicast communication problem, and show that existing approaches either are sub-optimal or do not generalize to different network topologies or tensor layouts, which result from different model architectures and parallelism strategies. We then propose two contributions to address cross-mesh resharding: an efficient broadcast-based communication system, and an \u201coverlapping-friendly\" pipeline schedule. On microbenchmarks, our overall system outperforms existing ones by up to 10x across various tensor and mesh layouts. On end-to-end training of two large models, GPT-3 and U-Transformer, we improve throughput by 10% and 50%, respectively.",
            "url": "https://proceedings.mlsys.org/paper_files/paper/2023/file/a42cbafcabb6dc7ce77bfe2e80f5c772-Paper-mlsys2023.pdf",
            "publicationVenue": "Proceedings of Machine Learning and Systems",
            "citation_count": 14
        },
        {
            "title": "Memory-adaptive Depth-wise Heterogenous Federated Learning",
            "authors": "Kai Zhang, Yutong Dai, Hongyi Wang, Eric Xing, Xun Chen, Lichao Sun",
            "year": "2023",
            "abstract": "Federated learning is a promising paradigm that allows multiple clients to collaboratively train a model without sharing the local data. However, the presence of heterogeneous devices in federated learning, such as mobile phones and IoT devices with varying memory capabilities, would limit the scale and hence the performance of the model could be trained. The mainstream approaches to address memory limitations focus on width-slimming techniques, where different clients train subnetworks with reduced widths locally and then the server aggregates the subnetworks. The global model produced from these methods suffers from performance degradation due to the negative impact of the actions taken to handle the varying subnetwork widths in the aggregation phase. In this paper, we introduce a memory-adaptive depth-wise learning solution in FL called FeDepth, which adaptively decomposes the full model into blocks according to the memory budgets of each client and trains blocks sequentially to obtain a full inference model. Our method outperforms state-of-the-art approaches, achieving 5% and more than 10% improvements in top-1 accuracy on CIFAR-10 and CIFAR-100, respectively. We also demonstrate the effectiveness of depth-wise fine-tuning on ViT. Our findings highlight the importance of memory-aware techniques for federated learning with heterogeneous devices and the success of depth-wise training strategy in improving the global model's performance.",
            "url": "https://arxiv.org/pdf/2303.04887",
            "publicationVenue": "arXiv preprint arXiv:2303.04887",
            "citation_count": 3
        },
        {
            "title": "Federated Learning as Variational Inference: A Scalable Expectation Propagation Approach",
            "authors": "Han Guo, Philip Greengard, Hongyi Wang, Andrew Gelman, Yoon Kim, Eric P Xing",
            "year": "2023",
            "abstract": "The canonical formulation of federated learning treats it as a distributed optimization problem where the model parameters are optimized against a global loss function that decomposes across client loss functions. A recent alternative formulation instead treats federated learning as a distributed inference problem, where the goal is to infer a global posterior from partitioned client data (Al-Shedivat et al., 2021). This paper extends the inference view and describes a variational inference formulation of federated learning where the goal is to find a global variational posterior that well-approximates the true posterior. This naturally motivates an expectation propagation approach to federated learning (FedEP), where approximations to the global posterior are iteratively refined through probabilistic message-passing between the central server and the clients. We conduct an extensive empirical study across various algorithmic considerations and describe practical strategies for scaling up expectation propagation to the modern federated setting. We apply FedEP on standard federated learning benchmarks and find that it outperforms strong baselines in terms of both convergence speed and accuracy.",
            "url": "https://arxiv.org/pdf/2302.04228",
            "publicationVenue": "arXiv preprint arXiv:2302.04228",
            "citation_count": 4
        },
        {
            "title": "Does compressing activations help model parallel training?",
            "authors": "Song Bian, Dacheng Li, Hongyi Wang, Eric P Xing, Shivaram Venkataraman",
            "year": "2023",
            "abstract": "Large-scale Transformer models are known for their exceptional performance in a range of tasks, but training them can be difficult due to the requirement for communication-intensive model parallelism. One way to improve training speed is to compress the message size in communication. Previous approaches have primarily focused on compressing gradients in a data parallelism setting, but compression in a model-parallel setting is an understudied area. We have discovered that model parallelism has fundamentally different characteristics than data parallelism. In this work, we present the first empirical study on the effectiveness of compression methods for model parallelism. We implement and evaluate three common classes of compression algorithms - pruning-based, learning-based, and quantization-based - using a popular Transformer training framework. We evaluate these methods across more than 160 settings and 8 popular datasets, taking into account different hyperparameters, hardware, and both fine-tuning and pre-training stages. We also provide analysis when the model is scaled up. Finally, we provide insights for future development of model parallelism compression algorithms.",
            "url": "https://arxiv.org/pdf/2301.02654",
            "publicationVenue": "arXiv preprint arXiv:2301.02654",
            "citation_count": 1
        },
        {
            "title": "Contextualized Networks Reveal Heterogeneous Transcriptomic Regulation in Tumors at Sample-Specific Resolution",
            "authors": "Caleb N Ellington, Benjamin J Lengerich, Thomas BK Watkins, Jiekun Yang, Hanxi Xiao, Manolis Kellis, Eric P Xing",
            "year": "2023",
            "abstract": "Cancers are shaped by somatic mutations, microenvironment, and patient background, each altering gene expression and regulation in complex ways, resulting in heterogeneous cellular states and dynamics. Inferring gene regulatory network (GRN) models from expression data can help characterize this regulation-driven heterogeneity, but network inference requires many statistical samples, traditionally limiting GRNs to cluster-level analyses that ignore intra-cluster heterogeneity. We propose to move beyond cluster-based analyses by usingcontextualizedlearning, a multi-task learning paradigm which allows us to infer sample-specific models using phenotypic, molecular, and environmental information pertinent to the model, encoded as the model's \"context\" to be conditioned on. We unify three network model classes (Correlation, Markov, Neighborhood) and estimate context-specific GRNs for 7997 tumors across 25 tumor types, with each network contextualized by copy number and driver mutation profiles, tumor microenvironment, and patient demographics. Contextualized GRNs provide a structured view of expression dynamics at sample-specific resolution, which reveal co-expression modules in correlation networks (CNs), as well as cliques and independent regulatory elements in Markov Networks (MNs) and Neighborhood Regression Networks (NNs). Our generative modeling approach allows us to predict GRNs for unseen tumor types based on a pan-cancer model of how somatic mutations affect gene regulation. Finally, contextualized networks enable GRN-based precision oncology, explaining known biomarkers in terms of\u00a0\u2026",
            "url": "https://www.biorxiv.org/content/10.1101/2023.12.01.569658.full.pdf",
            "publicationVenue": "bioRxiv",
            "citation_count": 0
        },
        {
            "title": "GET: a foundation model of transcription across human cell types",
            "authors": "Xi Fu, Shentong Mo, Anqi Shao, Anouchka Laurent, Alejandro Buendia, Adolfo A Ferrando, Alberto Ciccia, Yanyan Lan, Teresa Palomero, David M Owens, Eric P Xing, Raul Rabadan",
            "year": "2023",
            "abstract": "Transcriptional regulation, involving the complex interplay between regulatory sequences and proteins, directs all biological processes. Computational models of transcriptions lack generalizability to accurately extrapolate in unseen cell types and conditions. Here, we introduce GET, an interpretable foundation model, designed to uncover regulatory grammars across 213 human fetal and adult cell types. Relying exclusively on chromatin accessibility data and sequence information, GET achieves experimental-level accuracy in predicting gene expression even in previously unseen cell types. GET showcases remarkable adaptability across new sequencing platforms and assays, enabling regulatory inference across a broad range of cell types and conditions, and uncovering universal and cell type specific transcription factor interaction networks. We evaluated its performance on prediction of regulatory activity, inference of regulatory elements and regulators, and identification of physical interactions between transcription factors. Specifically, we show GET outperforms current models in predicting lentivirus-based massive parallel reporter assay readout with reduced input data. In Fetal erythroblast, we identify distal (>1Mbp) regulatory regions that were missed by previous models. In B cell, we identified a lymphocyte-specific transcription factor-transcription factor interaction that explains the functional significance of a lymphoma-risk predisposing germline mutation. In sum, we provide a generalizable and accurate model for transcription together with catalogs of gene regulation and transcription factor interactions, all with cell type specificity. A\u00a0\u2026",
            "url": "https://www.biorxiv.org/content/biorxiv/early/2023/09/24/2023.09.24.559168.full.pdf",
            "publicationVenue": "bioRxiv",
            "citation_count": 0
        },
        {
            "title": "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality",
            "authors": "Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, Ion Stoica, Eric P Xing",
            "year": "2023",
            "abstract": null,
            "url": null,
            "publicationVenue": "See https://vicuna. lmsys. org (accessed 14 April 2023)",
            "citation_count": 826
        },
        {
            "title": "3d semantic segmentation in the wild: Learning generalized models for adverse-condition point clouds",
            "authors": "Aoran Xiao, Jiaxing Huang, Weihao Xuan, Ruijie Ren, Kangcheng Liu, Dayan Guan, Abdulmotaleb El Saddik, Shijian Lu, Eric P Xing",
            "year": "2023",
            "abstract": "Robust point cloud parsing under all-weather conditions is crucial to level-5 autonomy in autonomous driving. However, how to learn a universal 3D semantic segmentation (3DSS) model is largely neglected as most existing benchmarks are dominated by point clouds captured under normal weather. We introduce SemanticSTF, an adverse-weather point cloud dataset that provides dense point-level annotations and allows to study 3DSS under various adverse weather conditions. We investigate universal 3DSS modeling with two tasks: 1) domain adaptive 3DSS that adapts from normal-weather data to adverse-weather data; 2) domain generalized 3DSS that learns a generalizable model from normal-weather data. Our studies reveal the challenge while existing 3DSS methods encounter adverse-weather data, showing the great value of SemanticSTF in steering the future endeavor along this very meaningful research direction. In addition, we design a domain randomization technique that alternatively randomizes the geometry styles of point clouds and aggregates their encoded embeddings, ultimately leading to a generalizable model that effectively improves 3DSS under various adverse weather. The SemanticSTF and related codes are available at https://github. com/xiaoaoran/SemanticSTF.",
            "url": "http://openaccess.thecvf.com/content/CVPR2023/papers/Xiao_3D_Semantic_Segmentation_in_the_Wild_Learning_Generalized_Models_for_CVPR_2023_paper.pdf",
            "publicationVenue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "citation_count": 9
        },
        {
            "title": "KD-DLGAN: Data Limited Image Generation via Knowledge Distillation",
            "authors": "Kaiwen Cui, Yingchen Yu, Fangneng Zhan, Shengcai Liao, Shijian Lu, Eric P Xing",
            "year": "2023",
            "abstract": "Generative Adversarial Networks (GANs) rely heavily on large-scale training data for training high-quality image generation models. With limited training data, the GAN discriminator often suffers from severe overfitting which directly leads to degraded generation especially in generation diversity. Inspired by the recent advances in knowledge distillation (KD), we propose KD-GAN, a knowledge-distillation based generation framework that introduces pre-trained vision-language models for training effective data-limited image generation models. KD-GAN consists of two innovative designs. The first is aggregated generative KD that mitigates the discriminator overfitting by challenging the discriminator with harder learning tasks and distilling more generalizable knowledge from the pre-trained models. The second is correlated generative KD that improves the generation diversity by distilling and preserving the diverse image-text correlation within the pre-trained models. Extensive experiments over multiple benchmarks show that KD-GAN achieves superior image generation with limited training data. In addition, KD-GAN complements the state-of-the-art with consistent and substantial performance gains. Note that codes will be released.",
            "url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Cui_KD-DLGAN_Data_Limited_Image_Generation_via_Knowledge_Distillation_CVPR_2023_paper.pdf",
            "publicationVenue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "citation_count": 9
        },
        {
            "title": "Understanding Masked Autoencoders via Hierarchical Latent Variable Models",
            "authors": "Lingjing Kong, Martin Q Ma, Guangyi Chen, Eric P Xing, Yuejie Chi, Louis-Philippe Morency, Kun Zhang",
            "year": "2023",
            "abstract": "Masked autoencoder (MAE), a simple and effective self-supervised learning framework based on the reconstruction of masked image regions, has recently achieved prominent success in a variety of vision tasks. Despite the emergence of intriguing empirical observations on MAE, a theoretically principled understanding is still lacking. In this work, we formally characterize and justify existing empirical insights and provide theoretical guarantees of MAE. We formulate the underlying data-generating process as a hierarchical latent variable model, and show that under reasonable assumptions, MAE provably identifies a set of latent variables in the hierarchical model, explaining why MAE can extract high-level information from pixels. Further, we show how key hyperparameters in MAE (the masking ratio and the patch size) determine which true latent variables to be recovered, therefore influencing the level of semantic information in the representation. Specifically, extremely large or small masking ratios inevitably lead to low-level representations. Our theory offers coherent explanations of existing empirical observations and provides insights for potential empirical improvements and fundamental limitations of the masked-reconstruction paradigm. We conduct extensive experiments to validate our theoretical insights.",
            "url": "http://openaccess.thecvf.com/content/CVPR2023/papers/Kong_Understanding_Masked_Autoencoders_via_Hierarchical_Latent_Variable_Models_CVPR_2023_paper.pdf",
            "publicationVenue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "citation_count": 8
        },
        {
            "title": "StyleRF: Zero-shot 3D Style Transfer of Neural Radiance Fields",
            "authors": "Kunhao Liu, Fangneng Zhan, Yiwen Chen, Jiahui Zhang, Yingchen Yu, Abdulmotaleb El Saddik, Shijian Lu, Eric P Xing",
            "year": "2023",
            "abstract": "3D style transfer aims to render stylized novel views of a 3D scene with multi-view consistency. However, most existing work suffers from a three-way dilemma over accurate geometry reconstruction, high-quality stylization, and being generalizable to arbitrary new styles. We propose StyleRF (Style Radiance Fields), an innovative 3D style transfer technique that resolves the three-way dilemma by performing style transformation within the feature space of a radiance field. StyleRF employs an explicit grid of high-level features to represent 3D scenes, with which high-fidelity geometry can be reliably restored via volume rendering. In addition, it transforms the grid features according to the reference style which directly leads to high-quality zero-shot style transfer. StyleRF consists of two innovative designs. The first is sampling-invariant content transformation that makes the transformation invariant to the holistic statistics of the sampled 3D points and accordingly ensures multi-view consistency. The second is deferred style transformation of 2D feature maps which is equivalent to the transformation of 3D points but greatly reduces memory footprint without degrading multi-view consistency. Extensive experiments show that StyleRF achieves superior 3D stylization quality with precise geometry reconstruction and it can generalize to various new styles in a zero-shot manner. Project website: https://kunhao-liu. github. io/StyleRF/",
            "url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_StyleRF_Zero-Shot_3D_Style_Transfer_of_Neural_Radiance_Fields_CVPR_2023_paper.pdf",
            "publicationVenue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "citation_count": 23
        }
    ],
    "Chenyan Xiong": [
        {
            "title": "Using a multi-task-trained neural network to guide interaction with a query-processing system via useful suggestions",
            "authors": null,
            "year": "2023",
            "abstract": "A computer-implemented technique is described herein for assisting a user in advancing a task objective. The technique uses a suggestion-generating system (SGS) to provide one or more suggestions to a user in response to at least a last-submitted query provided by the user. The SGS may correspond to a classification-type or generative-type neural network. The SGS uses a machine-trained model that is trained using a multi-task training framework based on plural groups of training examples, which, in turn, are produced using different respective example-generating methods. One such example-generating method constructs a training example from queries in a search session. It operates by identifying the task-related intent the queries, and then identifying at least one sequence of queries in the search session that exhibits a coherent task-related intent. A training example is constructed based on queries in\u00a0\u2026",
            "url": "https://patentimages.storage.googleapis.com/2f/de/2e/2b554001599ea4/US11853362B2.pdf",
            "publicationVenue": null,
            "citation_count": 4
        },
        {
            "title": "An In-depth Look at Gemini's Language Abilities",
            "authors": "Syeda Nahida Akter, Zichun Yu, Aashiq Muhamed, Tianyue Ou, Alex B\u00e4uerle, \u00c1ngel Alexander Cabrera, Krish Dholakia, Chenyan Xiong, Graham Neubig",
            "year": "2023",
            "abstract": "The recently released Google Gemini class of models are the first to comprehensively report results that rival the OpenAI GPT series across a wide variety of tasks. In this paper, we do an in-depth exploration of Gemini's language abilities, making two contributions. First, we provide a third-party, objective comparison of the abilities of the OpenAI GPT and Google Gemini models with reproducible code and fully transparent results. Second, we take a closer look at the results, identifying areas where one of the two model classes excels. We perform this analysis over 10 datasets testing a variety of language abilities, including reasoning, answering knowledge-based questions, solving math problems, translating between languages, generating code, and acting as instruction-following agents. From this analysis, we find that Gemini Pro achieves accuracy that is close but slightly inferior to the corresponding GPT 3.5 Turbo on all tasks that we benchmarked. We further provide explanations for some of this under-performance, including failures in mathematical reasoning with many digits, sensitivity to multiple-choice answer ordering, aggressive content filtering, and others. We also identify areas where Gemini demonstrates comparably high performance, including generation into non-English languages, and handling longer and more complex reasoning chains. Code and data for reproduction can be found at https://github.com/neulab/gemini-benchmark",
            "url": "https://arxiv.org/pdf/2312.11444.pdf?trk=public_post_comment-text",
            "publicationVenue": "arXiv preprint arXiv:2312.11444",
            "citation_count": 2
        },
        {
            "title": "Augmenting Zero-Shot Dense Retrievers with Plug-in Mixture-of-Memories",
            "authors": "Suyu Ge, Chenyan Xiong, Corby Rosset, Arnold Overwijk, Jiawei Han, Paul Bennett",
            "year": "2023",
            "abstract": "In this paper we improve the zero-shot generalization ability of language models via Mixture-Of-Memory Augmentation (MoMA), a mechanism that retrieves augmentation documents from multiple information corpora (\"external memories\"), with the option to \"plug in\" new memory at inference time. We develop a joint learning mechanism that trains the augmentation component with latent labels derived from the end retrieval task, paired with hard negatives from the memory mixture. We instantiate the model in a zero-shot dense retrieval setting by augmenting a strong T5-based retriever with MoMA. Our model, MoMA, obtains strong zero-shot retrieval accuracy on the eighteen tasks included in the standard BEIR benchmark. It outperforms systems that seek generalization from increased model parameters and computation steps. Our analysis further illustrates the necessity of augmenting with mixture-of-memory for robust generalization, the benefits of augmentation learning, and how MoMA utilizes the plug-in memory at inference time without changing its parameters. We plan to open source our code.",
            "url": "https://arxiv.org/pdf/2302.03754",
            "publicationVenue": "EMNLP 2023",
            "citation_count": 4
        },
        {
            "title": "An In-depth Look at Gemini's Language Abilities",
            "authors": "Syeda Nahida Akter, Zichun Yu, Aashiq Muhamed, Tianyue Ou, Alex B\u00e4uerle, \u00c1ngel Alexander Cabrera, Krish Dholakia, Chenyan Xiong, Graham Neubig",
            "year": "2023",
            "abstract": "The recently released Google Gemini class of models are the first to comprehensively report results that rival the OpenAI GPT series across a wide variety of tasks. In this paper, we do an in-depth exploration of Gemini's language abilities, making two contributions. First, we provide a third-party, objective comparison of the abilities of the OpenAI GPT and Google Gemini models with reproducible code and fully transparent results. Second, we take a closer look at the results, identifying areas where one of the two model classes excels. We perform this analysis over 10 datasets testing a variety of language abilities, including reasoning, answering knowledge-based questions, solving math problems, translating between languages, generating code, and acting as instruction-following agents. From this analysis, we find that Gemini Pro achieves accuracy that is close but slightly inferior to the corresponding GPT 3.5\u00a0\u2026",
            "url": null,
            "publicationVenue": "arXiv e-prints",
            "citation_count": 0
        },
        {
            "title": "CompleQA: Benchmarking the Impacts of Knowledge Graph Completion Methods on Question Answering",
            "authors": "Donghan Yu, Yu Gu, Chenyan Xiong, Yiming Yang",
            "year": "2023",
            "abstract": "How much success in Knowledge Graph Completion (KGC) would translate into the performance enhancement in downstream tasks is an important question that has not been studied in depth. In this paper, we introduce a novel benchmark, namely CompleQA, to comprehensively assess the influence of representative KGC methods on Knowledge Graph Question Answering (KGQA), one of the most important downstream applications. This benchmark includes a knowledge graph with 3 million triplets across 5 distinct domains, coupled with over 5000 question-answering pairs and a completion dataset that is well-aligned with these questions. Our evaluation of four well-known KGC methods in combination with two state-of-the-art KGQA systems shows that effective KGC can significantly mitigate the impact of knowledge graph incompleteness on question-answering performance. Surprisingly, we also find that the best-performing KGC method (s) does not necessarily lead to the best QA results, underscoring the need to consider downstream applications when doing KGC.",
            "url": "https://aclanthology.org/2023.findings-emnlp.849.pdf",
            "publicationVenue": "Findings of the Association for Computational Linguistics: EMNLP 2023",
            "citation_count": 0
        },
        {
            "title": "Document body vectorization and noise-contrastive training",
            "authors": null,
            "year": "2023",
            "abstract": "Document embedding vectors for each document of a corpus may be generated by combining embedding vectors for document subparts, thereby yielding a final embedding vector for the document. A machine learning model is trained using a query corpus and the document corpus, where the model generates a ranking score for a given (query, document) pair. During training, rankings scores are generated using the model, such that the training dataset is further refined using the generated ranking scores. For example, top documents and a negative document may be determined for a given query and subsequently used as training data. Multiple negative documents may therefore be determined for a given query. A negative document for a given query may be determined from the negative documents using noise-contrastive estimation. Such determined negative documents may be evaluated using a loss function\u00a0\u2026",
            "url": "https://patentimages.storage.googleapis.com/dc/99/a5/d24ff6dc61674f/US20220179871A1.pdf",
            "publicationVenue": null,
            "citation_count": 0
        },
        {
            "title": "Text compression with predicted continuations",
            "authors": null,
            "year": "2023",
            "abstract": "A method for text compression comprises recognizing a prefix string of one or more text characters preceding a target string of a plurality of text characters to be compressed. The prefix string is provided to a natural language generation (NLG) model configured to output one or more predicted continuations each having an associated rank. If the one or more predicted continuations include a matching predicted continuation relative to the next one or more text characters of the target string, the next one or more text characters are compressed as an NLG-type compressed representation. If no predicted continuations match the next one or more text characters of the target string, a longest matching entry in a compression dictionary is identified. The next one or more text characters of the target string are compressed as a dictionary-type compressed representation that includes the dictionary index value of the longest\u00a0\u2026",
            "url": null,
            "publicationVenue": null,
            "citation_count": 0
        },
        {
            "title": "Distributionally Robust Unsupervised Dense Retrieval Training on Web Graphs",
            "authors": "Peixuan Han, Zhenghao Liu, Zhiyuan Liu, Chenyan Xiong",
            "year": "2023",
            "abstract": "This paper introduces Web-DRO, an unsupervised dense retrieval model, which clusters documents based on web structures and reweights the groups during contrastive training. Specifically, we first leverage web graph links and contrastively train an embedding model for clustering anchor-document pairs. Then we use Group Distributional Robust Optimization to reweight different clusters of anchor-document pairs, which guides the model to assign more weights to the group with higher contrastive loss and pay more attention to the worst case during training. Our experiments on MS MARCO and BEIR show that our model, Web-DRO, significantly improves the retrieval effectiveness in unsupervised scenarios. A comparison of clustering techniques shows that training on the web graph combining URL information reaches optimal performance on clustering. Further analysis confirms that group weights are stable and valid, indicating consistent model preferences as well as effective up-weighting of valuable groups and down-weighting of uninformative ones. The code of this paper can be obtained from https://github.com/OpenMatch/Web-DRO.",
            "url": "https://arxiv.org/pdf/2310.16605",
            "publicationVenue": "arXiv preprint arXiv:2310.16605",
            "citation_count": 0
        },
        {
            "title": "Unlock Multi-Modal Capability of Dense Retrieval via Visual Module Plugin",
            "authors": "Tianshuo Zhou, Sen Mei, Xinze Li, Zhenghao Liu, Chenyan Xiong, Zhiyuan Liu, Yu Gu, Ge Yu",
            "year": "2023",
            "abstract": "This paper proposes Multi-modAl Retrieval model via Visual modulE pLugin (MARVEL) to learn an embedding space for queries and multi-modal documents to conduct retrieval. MARVEL encodes queries and multi-modal documents with a unified encoder model, which helps to alleviate the modality gap between images and texts. Specifically, we enable the image understanding ability of a well-trained dense retriever, T5-ANCE, by incorporating the image features encoded by the visual module as its inputs. To facilitate the multi-modal retrieval tasks, we build the ClueWeb22-MM dataset based on the ClueWeb22 dataset, which regards anchor texts as queries, and exact the related texts and image documents from anchor linked web pages. Our experiments show that MARVEL significantly outperforms the state-of-the-art methods on the multi-modal retrieval dataset WebQA and ClueWeb22-MM. Our further analyses show that the visual module plugin method is tailored to enable the image understanding ability for an existing dense retrieval model. Besides, we also show that the language model has the ability to extract image semantics from image encoders and adapt the image features in the input space of language models. All codes are available at https://github.com/OpenMatch/MARVEL.",
            "url": "https://arxiv.org/html/2310.14037v3",
            "publicationVenue": "arXiv preprint arXiv:2310.14037",
            "citation_count": 0
        },
        {
            "title": "Text Matching Improves Sequential Recommendation by Reducing Popularity Biases",
            "authors": "Zhenghao Liu, Sen Mei, Chenyan Xiong, Xiaohua Li, Shi Yu, Zhiyuan Liu, Yu Gu, Ge Yu",
            "year": "2023",
            "abstract": "This paper proposes Text mAtching based SequenTial rEcommenda-tion model (TASTE), which maps items and users in an embedding space and recommends items by matching their text representations. TASTE verbalizes items and user-item interactions using identifiers and attributes of items. To better characterize user behaviors, TASTE additionally proposes an attention sparsity method, which enables TASTE to model longer user-item interactions by reducing the self-attention computations during encoding. Our experiments show that TASTE outperforms the state-of-the-art methods on widely used sequential recommendation datasets. TASTE alleviates the cold start problem by representing long-tail items using full-text modeling and bringing the benefits of pretrained language models to recommendation systems. Our further analyses illustrate that TASTE significantly improves the recommendation accuracy\u00a0\u2026",
            "url": "https://arxiv.org/pdf/2308.14029",
            "publicationVenue": "CIKM 2023",
            "citation_count": 4
        },
        {
            "title": "Automated structured textual content categorization accuracy with neural networks",
            "authors": null,
            "year": "2023",
            "abstract": "To provide automated categorization of structured textual content individual nodes of textual content, from a document object model encapsulation of the structured textual content, have a multidimensional vector associated with them, where the values of the various dimensions of the multidimensional vector are based on the textual content in the corresponding node, the visual features applied or associated with the textual content of the corresponding node, and positional information of the textual content of the corresponding node. The multidimensional vectors are input to a neighbor-imbuing neural network. The enhanced multidimensional vectors output by the neighbor-imbuing neural network are then be provided to a categorization neural network. The resulting output can be in the form of multidimensional vectors whose dimensionality is proportional to categories into which the structured textual content is to\u00a0\u2026",
            "url": null,
            "publicationVenue": null,
            "citation_count": 0
        },
        {
            "title": "Openmatch-v2: An all-in-one multi-modality plm-based information retrieval toolkit",
            "authors": "Shi Yu, Zhenghao Liu, Chenyan Xiong, Zhiyuan Liu",
            "year": "2023",
            "abstract": "Pre-trained language models (PLMs) have emerged as the foundation of the most advanced Information Retrieval (IR) models. Powered by PLMs, the latest IR research has proposed novel models, new domain adaptation algorithms as well as enlarged datasets. In this paper, we present a Python-based IR toolkit OpenMatch-v2. As a full upgrade of OpenMatch proposed in 2021, OpenMatch-v2 incorporates the most recent advancements of PLM-based IR research, providing support for new, cross-modality models and enhanced domain adaptation techniques with a streamlined, optimized infrastructure. The code of OpenMatch is publicly available at https://github.com/OpenMatch/OpenMatch.",
            "url": "https://dl.acm.org/doi/pdf/10.1145/3539618.3591813",
            "publicationVenue": "Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval",
            "citation_count": 6
        },
        {
            "title": "Improving Multitask Retrieval by Promoting Task Specialization",
            "authors": "Wenzheng Zhang, Chenyan Xiong, Karl Stratos, Arnold Overwijk",
            "year": "2023",
            "abstract": "In multitask retrieval, a single retriever is trained to retrieve relevant contexts for multiple tasks. Despite its practical appeal, naive multitask retrieval lags behind task-specific retrieval, in which a separate retriever is trained for each task. We show that it is possible to train a multitask retriever that outperforms task-specific retrievers by promoting task specialization. The main ingredients are: (1) a better choice of pretrained model\u2014one that is explicitly optimized for multitasking\u2014along with compatible prompting, and (2) a novel adaptive learning method that encourages each parameter to specialize in a particular task. The resulting multitask retriever is highly performant on the KILT benchmark. Upon analysis, we find that the model indeed learns parameters that are more task-specialized compared to naive multitasking without prompting or adaptive learning.",
            "url": "https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00597/117689",
            "publicationVenue": "TACL 2023",
            "citation_count": 0
        },
        {
            "title": "Structure-Aware Language Model Pretraining Improves Dense Retrieval on Structured Data",
            "authors": "Xinze Li, Zhenghao Liu, Chenyan Xiong, Shi Yu, Yu Gu, Zhiyuan Liu, Ge Yu",
            "year": "2023",
            "abstract": "This paper presents Structure Aware Dense Retrieval (SANTA) model, which encodes user queries and structured data in one universal embedding space for retrieving structured data. SANTA proposes two pretraining methods to make language models structure-aware and learn effective representations for structured data: 1) Structured Data Alignment, which utilizes the natural alignment relations between structured data and unstructured data for structure-aware pretraining. It contrastively trains language models to represent multi-modal text data and teaches models to distinguish matched structured data for unstructured texts. 2) Masked Entity Prediction, which designs an entity-oriented mask strategy and asks language models to fill in the masked entities. Our experiments show that SANTA achieves state-of-the-art on code search and product search and conducts convincing results in the zero-shot setting. SANTA learns tailored representations for multi-modal text data by aligning structured and unstructured data pairs and capturing structural semantics by masking and predicting entities in the structured data. All codes are available at https://github.com/OpenMatch/OpenMatch.",
            "url": "https://arxiv.org/pdf/2305.19912",
            "publicationVenue": "ACL 2023",
            "citation_count": 1
        },
        {
            "title": "Augmentation-Adapted Retriever Improves Generalization of Language Models as Generic Plug-In",
            "authors": "Zichun Yu, Chenyan Xiong, Shi Yu, Zhiyuan Liu",
            "year": "2023",
            "abstract": "Retrieval augmentation can aid language models (LMs) in knowledge-intensive tasks by supplying them with external information. Prior works on retrieval augmentation usually jointly fine-tune the retriever and the LM, making them closely coupled. In this paper, we explore the scheme of generic retrieval plug-in: the retriever is to assist target LMs that may not be known beforehand or are unable to be fine-tuned together. To retrieve useful documents for unseen target LMs, we propose augmentation-adapted retriever (AAR), which learns LM's preferences obtained from a known source LM. Experiments on the MMLU and PopQA datasets demonstrate that our AAR trained with a small source LM is able to significantly improve the zero-shot generalization of larger target LMs ranging from 250M Flan-T5 to 175B InstructGPT. Further analysis indicates that the preferences of different LMs overlap, enabling AAR trained with a single source LM to serve as a generic plug-in for various target LMs. Our code is open-sourced at https://github.com/OpenMatch/Augmentation-Adapted-Retriever.",
            "url": "https://arxiv.org/pdf/2305.17331",
            "publicationVenue": "ACL 2023",
            "citation_count": 10
        },
        {
            "title": "Fusion-in-T5: Unifying Document Ranking Signals for Improved Information Retrieval",
            "authors": "Shi Yu, Chenghao Fan, Chenyan Xiong, David Jin, Zhiyuan Liu, Zhenghao Liu",
            "year": "2023",
            "abstract": "Common IR pipelines are typically cascade systems that may involve multiple rankers and/or fusion models to integrate different information step-by-step. In this paper, we propose a novel re-ranker named Fusion-in-T5 (FiT5), which integrates document text information, retrieval features, and global document information into a single unified model using templated-based input and global attention. Experiments on passage ranking benchmarks MS MARCO and TREC DL show that FiT5 significantly improves ranking performance over prior pipelines. Analyses find that through global attention, FiT5 is able to jointly utilize the ranking features via gradually attending to related documents, and thus improve the detection of subtle nuances between them. Our code will be open-sourced.",
            "url": "https://arxiv.org/pdf/2305.14685",
            "publicationVenue": "arXiv preprint arXiv:2305.14685",
            "citation_count": 2
        },
        {
            "title": "Keyphase extraction beyond language modeling",
            "authors": null,
            "year": "2023",
            "abstract": "A system for extracting a key phrase from a document includes a neural key phrase extraction model (\u201cBLING-KPE\u201d) having a first layer to extract a word sequence from the document, a second layer to represent each word in the word sequence by ELMo embedding, position embedding, and visual features, and a third layer to concatenate the ELMo embedding, the position embedding, and the visual features to produce hybrid word embeddings. A convolutional transformer models the hybrid word embeddings to n-gram embeddings, and a feedforward layer converts the n-gram embeddings into a probability distribution over a set of n-grams and calculates a key phrase score of each n-gram. The neural key phrase extraction model is trained on annotated data based on a labeled loss function to compute cross entropy loss of the key phrase score of each n-gram as compared with a label from the annotated dataset.",
            "url": "https://patentimages.storage.googleapis.com/67/6d/6a/b27e27d03b8662/US11657223.pdf",
            "publicationVenue": null,
            "citation_count": 1
        },
        {
            "title": "Model-Generated Pretraining Signals Improves Zero-Shot Generalization of Text-to-Text Transformers",
            "authors": "Linyuan Gong, Chenyan Xiong, Xiaodong Liu, Payal Bajaj, Yiqing Xie, Alvin Cheung, Jianfeng Gao, Xia Song",
            "year": "2023",
            "abstract": "This paper explores the effectiveness of model-generated signals in improving zero-shot generalization of text-to-text Transformers such as T5. We study various designs to pretrain T5 using an auxiliary model to construct more challenging token replacements for the main model to denoise. Key aspects under study include the decoding target, the location of the RTD head, and the masking pattern. Based on these studies, we develop a new model, METRO-T0, which is pretrained using the redesigned ELECTRA-Style pretraining strategies and then prompt-finetuned on a mixture of NLP tasks. METRO-T0 outperforms all similar-sized baselines on prompted NLP benchmarks, such as T0 Eval and MMLU, and rivals the state-of-the-art T0-11B model with only 8% of its parameters. Our analysis on model's neural activation and parameter sensitivity reveals that the effectiveness of METRO-T0 stems from more balanced contribution of parameters and better utilization of their capacity. The code and model checkpoints are available at https://github.com/gonglinyuan/metro_t0.",
            "url": "https://arxiv.org/pdf/2305.12567",
            "publicationVenue": "ACL 2023",
            "citation_count": 0
        },
        {
            "title": "Unsupervised Dense Retrieval Training with Web Anchors",
            "authors": "Yiqing Xie, Xiao Liu, Chenyan Xiong",
            "year": "2023",
            "abstract": "In this work, we present an unsupervised retrieval method with contrastive learning on web anchors. The anchor text describes the content that is referenced from the linked page. This shows similarities to search queries that aim to retrieve pertinent information from relevant documents. Based on their commonalities, we train an unsupervised dense retriever, Anchor-DR, with a contrastive learning task that matches the anchor text and the linked document. To filter out uninformative anchors (such as ``homepage'' or other functional anchors), we present a novel filtering technique to only select anchors that contain similar types of information as search queries. Experiments show that Anchor-DR outperforms state-of-the-art methods on unsupervised dense retrieval by a large margin (e.g., by 5.3% NDCG@10 on MSMARCO). The gain of our method is especially significant for search and question answering tasks. Our analysis further reveals that the pattern of anchor-document pairs is similar to that of search query-document pairs. Code available at https://github.com/Veronicium/AnchorDR.",
            "url": "https://arxiv.org/pdf/2305.05834",
            "publicationVenue": "SIGIR 2023",
            "citation_count": 4
        },
        {
            "title": "Universal Multi-Modality Retrieval with One Unified Embedding Space",
            "authors": "Zhenghao Liu, Chenyan Xiong, Yuanhuiyi Lv, Zhiyuan Liu, Ge Yu",
            "year": "2023",
            "abstract": "This paper presents Vision-Language Universal Search (VL-UnivSearch), which builds a unified model for multi-modality retrieval. VL-UnivSearch encodes query and multi-modality sources in a universal embedding space for searching related candidates and routing modalities. To learn a tailored embedding space for multi-modality retrieval, VL-UnivSearch proposes two techniques: 1) Universal embedding optimization, which contrastively optimizes the embedding space using the modality-balanced hard negatives; 2) Image verbalization method, which bridges the modality gap between images and texts in the raw data space. VL-UnivSearch achieves the state-of-the-art on the multi-modality open-domain question answering benchmark, WebQA, and outperforms all retrieval models in each single modality task. It demonstrates that universal multi-modality search is feasible to replace the divide-and-conquer pipeline with a united model and also benefit per modality tasks. All source codes of this work will be released via Github.",
            "url": "https://arxiv.org/pdf/2209.00179",
            "publicationVenue": "ICLR 2023",
            "citation_count": 15
        },
        {
            "title": "Proactive Human-Machine Conversations",
            "authors": "Jianfeng Gao, Chenyan Xiong, Paul Bennett, Nick Craswell",
            "year": "2023",
            "abstract": "This chapter discusses methods and techniques that aim to equip a CIR system with the capability of proactively assisting a user to clarify her search intent by asking clarifying questions, guiding the user to explore the topic being discussed by suggesting useful queries, and recommending new topics and potential items of interest such as news articles in the hope of producing more fruitful search results.",
            "url": null,
            "publicationVenue": "Neural Approaches to Conversational Information Retrieval",
            "citation_count": 0
        },
        {
            "title": "Conversational Search",
            "authors": "Jianfeng Gao, Chenyan Xiong, Paul Bennett, Nick Craswell",
            "year": "2023",
            "abstract": "This chapter discusses conversational document search, which can be viewed as a sub-system of the CIR system. We start with an introduction to the task and public benchmarks; review transformation-based pre-trained language models, which are the building blocks of many CIR modules; and then describe the main components of the conversational search system, including contextual query understanding, sparse and dense document retrieval, and neural document ranking.",
            "url": null,
            "publicationVenue": "Neural Approaches to Conversational Information Retrieval",
            "citation_count": 0
        },
        {
            "title": "Neural Approaches to Conversational Information Retrieval",
            "authors": "Jianfeng Gao, Chenyan Xiong, Paul Bennett, Nick Craswell",
            "year": "2023",
            "abstract": "A conversational information retrieval (CIR) system is an information retrieval (IR) system with a conversational interface which allows users to interact with the system to seek information via multi-turn conversations of natural language, in spoken or written form. Recent progress in deep learning has brought tremendous improvements in natural language processing (NLP) and conversational AI, leading to a plethora of commercial conversational services that allow naturally spoken and typed interaction, increasing the need for more human-centric interactions in IR. As a result, we have witnessed a resurgent interest in developing modern CIR systems in both research communities and industry. This book surveys recent advances in CIR, focusing on neural approaches that have been developed in the last few years. This book is based on the authors' tutorial at SIGIR'2020 (Gao et al., 2020b), with IR and NLP communities as the primary target audience. However, audiences with other background, such as machine learning and human-computer interaction, will also find it an accessible introduction to CIR. We hope that this book will prove a valuable resource for students, researchers, and software developers. This manuscript is a working draft. Comments are welcome.",
            "url": "https://arxiv.org/pdf/2201.05176",
            "publicationVenue": "arXiv preprint arXiv:2201.05176",
            "citation_count": 57
        }
    ],
    "Yiming Yang": [
        {
            "title": "AutoMix: Mixing Models with Few-shot Self and Meta Verification",
            "authors": "Aman Madaan, Pranjal Aggarwal, Ankit Anand, Srividya Pranavi Potharaju, Swaroop Mishra, Pei Zhou, Aditya Gupta, Dheeraj Rajagopal, Yiming Yang, Shyam Upadhyay, Manaal Faruqui",
            "year": "2023",
            "abstract": "Large language models (LLMs) are now available in various sizes and configurations from cloud API providers. While this diversity offers a broad spectrum of choices, effectively leveraging the options to optimize computational cost and performance remains challenging. In this work, we present AutoMix, an approach that strategically routes queries to larger LMs, based on the approximate correctness of outputs from a smaller LM. Central to AutoMix is a few-shot self-verification mechanism, which estimates the reliability of its own outputs without requiring training. Given that verifications can be noisy, we employ a meta verifier in \\ours to refine the accuracy of these assessments. Our experiments using LLAMA2-13B and LLAMA2-70B, on five context-grounded reasoning datasets demonstrate that AutoMix surpasses established baselines, improving the incremental benefit per cost by up to 57%.",
            "url": "https://openreview.net/pdf?id=FJo2lroF7R",
            "publicationVenue": "R0-FoMo: Robustness of Few-shot and Zero-shot Learning in Large Foundation Models",
            "citation_count": 1
        },
        {
            "title": "CompleQA: Benchmarking the Impacts of Knowledge Graph Completion Methods on Question Answering",
            "authors": "Donghan Yu, Yu Gu, Chenyan Xiong, Yiming Yang",
            "year": "2023",
            "abstract": "How much success in Knowledge Graph Completion (KGC) would translate into the performance enhancement in downstream tasks is an important question that has not been studied in depth. In this paper, we introduce a novel benchmark, namely CompleQA, to comprehensively assess the influence of representative KGC methods on Knowledge Graph Question Answering (KGQA), one of the most important downstream applications. This benchmark includes a knowledge graph with 3 million triplets across 5 distinct domains, coupled with over 5000 question-answering pairs and a completion dataset that is well-aligned with these questions. Our evaluation of four well-known KGC methods in combination with two state-of-the-art KGQA systems shows that effective KGC can significantly mitigate the impact of knowledge graph incompleteness on question-answering performance. Surprisingly, we also find that the best-performing KGC method (s) does not necessarily lead to the best QA results, underscoring the need to consider downstream applications when doing KGC.",
            "url": "https://aclanthology.org/2023.findings-emnlp.849.pdf",
            "publicationVenue": "Findings of the Association for Computational Linguistics: EMNLP 2023",
            "citation_count": 0
        },
        {
            "title": "Let\u2019s Sample Step by Step: Adaptive-Consistency for Efficient Reasoning and Coding with LLMs",
            "authors": "Pranjal Aggarwal, Aman Madaan, Yiming Yang",
            "year": "2023",
            "abstract": "A popular approach for improving the correctness of output from large language models (LLMs) is Self-Consistency-poll the LLM multiple times and output the most frequent solution. Existing Self-Consistency techniques always generate a constant number of samples per question, where a better approach will be to non-uniformly distribute the available budget based on the amount of agreement in the samples generated so far. In response, we introduce Adaptive-Consistency, a cost-efficient, model-agnostic technique that dynamically adjusts the number of samples per question using a lightweight stopping criterion. Our experiments over 17 reasoning and code generation datasets and three LLMs demonstrate that Adaptive-Consistency reduces sample budget by up to 7.9 times with an average accuracy drop of less than 0.1%",
            "url": "https://aclanthology.org/2023.emnlp-main.761.pdf",
            "publicationVenue": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
            "citation_count": 0
        },
        {
            "title": "A Self-enhancement Approach for Domain-specific Chatbot Training via Knowledge Mining and Digest",
            "authors": "Ruohong Zhang, Luyu Gao, Chen Zheng, Zhen Fan, Guokun Lai, Zheng Zhang, Fangzhou Ai, Yiming Yang, Hongxia Yang",
            "year": "2023",
            "abstract": "Large Language Models (LLMs), despite their great power in language generation, often encounter challenges when dealing with intricate and knowledge-demanding queries in specific domains. This paper introduces a novel approach to enhance LLMs by effectively extracting the relevant knowledge from domain-specific textual sources, and the adaptive training of a chatbot with domain-specific inquiries. Our two-step approach starts from training a knowledge miner, namely LLMiner, which autonomously extracts Question-Answer pairs from relevant documents through a chain-of-thought reasoning process. Subsequently, we blend the mined QA pairs with a conversational dataset to fine-tune the LLM as a chatbot, thereby enriching its domain-specific expertise and conversational capabilities. We also developed a new evaluation benchmark which comprises four domain-specific text corpora and associated human-crafted QA pairs for testing. Our model shows remarkable performance improvement over generally aligned LLM and surpasses domain-adapted models directly fine-tuned on domain corpus. In particular, LLMiner achieves this with minimal human intervention, requiring only 600 seed instances, thereby providing a pathway towards self-improvement of LLMs through model-synthesized training data.",
            "url": "https://arxiv.org/pdf/2311.10614",
            "publicationVenue": "arXiv preprint arXiv:2311.10614",
            "citation_count": 0
        },
        {
            "title": "AutoMix: Automatically Mixing Language Models",
            "authors": "Aman Madaan, Pranjal Aggarwal, Ankit Anand, Srividya Pranavi Potharaju, Swaroop Mishra, Pei Zhou, Aditya Gupta, Dheeraj Rajagopal, Karthik Kappaganthu, Yiming Yang, Shyam Upadhyay, Manaal Faruqui",
            "year": "2023",
            "abstract": "Large language models (LLMs) are now available in various sizes and configurations from cloud API providers. While this diversity offers a broad spectrum of choices, effectively leveraging the options to optimize computational cost and performance remains challenging. In this work, we present AutoMix, an approach that strategically routes queries to larger LMs, based on the approximate correctness of outputs from a smaller LM. Central to AutoMix is a few-shot self-verification mechanism, which estimates the reliability of its own outputs without requiring training. Given that verifications can be noisy, we employ a meta verifier in AutoMix to refine the accuracy of these assessments. Our experiments using LLAMA2-13/70B, on five context-grounded reasoning datasets demonstrate that AutoMix surpasses established baselines, improving the incremental benefit per cost by up to 89%. Our code and data are available at https://github.com/automix-llm/automix.",
            "url": "https://arxiv.org/pdf/2310.12963",
            "publicationVenue": "arXiv preprint arXiv:2310.12963",
            "citation_count": 1
        },
        {
            "title": "Aligning large multimodal models with factually augmented rlhf",
            "authors": "Zhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu, Chunyuan Li, Yikang Shen, Chuang Gan, Liang-Yan Gui, Yu-Xiong Wang, Yiming Yang, Kurt Keutzer, Trevor Darrell",
            "year": "2023",
            "abstract": "Large Multimodal Models (LMM) are built across modalities and the misalignment between two modalities can result in \"hallucination\", generating textual outputs that are not grounded by the multimodal information in context. To address the multimodal misalignment issue, we adapt the Reinforcement Learning from Human Feedback (RLHF) from the text domain to the task of vision-language alignment, where human annotators are asked to compare two responses and pinpoint the more hallucinated one, and the vision-language model is trained to maximize the simulated human rewards. We propose a new alignment algorithm called Factually Augmented RLHF that augments the reward model with additional factual information such as image captions and ground-truth multi-choice options, which alleviates the reward hacking phenomenon in RLHF and further improves the performance. We also enhance the GPT-4-generated training data (for vision instruction tuning) with previously available human-written image-text pairs to improve the general capabilities of our model. To evaluate the proposed approach in real-world scenarios, we develop a new evaluation benchmark MMHAL-BENCH with a special focus on penalizing hallucinations. As the first LMM trained with RLHF, our approach achieves remarkable improvement on the LLaVA-Bench dataset with the 94% performance level of the text-only GPT-4 (while previous best methods can only achieve the 87% level), and an improvement by 60% on MMHAL-BENCH over other baselines. We opensource our code, model, data at https://llava-rlhf.github.io.",
            "url": "https://arxiv.org/pdf/2309.14525",
            "publicationVenue": "arXiv preprint arXiv:2309.14525",
            "citation_count": 43
        },
        {
            "title": "Accelerating Diffusion-based Combinatorial Optimization Solvers by Progressive Distillation",
            "authors": "Junwei Huang, Zhiqing Sun, Yiming Yang",
            "year": "2023",
            "abstract": "Graph-based diffusion models have shown promising results in terms of generating high-quality solutions to NP-complete (NPC) combinatorial optimization (CO) problems. However, those models are often inefficient in inference, due to the iterative evaluation nature of the denoising diffusion process. This paper proposes to use progressive distillation to speed up the inference by taking fewer steps (e.g., forecasting two steps ahead within a single step) during the denoising process. Our experimental results show that the progressively distilled model can perform inference 16 times faster with only 0.019% degradation in performance on the TSP-50 dataset.",
            "url": "https://arxiv.org/pdf/2308.06644",
            "publicationVenue": "arXiv preprint arXiv:2308.06644",
            "citation_count": 0
        },
        {
            "title": "Retrieval-enhanced generative model for large-scale knowledge graph completion",
            "authors": "Donghan Yu, Yiming Yang",
            "year": "2023",
            "abstract": "The task of knowledge graph completion (KGC) is of great importance. To achieve scalability when dealing with large-scale knowledge graphs, recent works formulate KGC as a sequence-to-sequence process, where the incomplete triplet (input) and the missing entity (output) are both verbalized as text sequences. However, inference with these methods relies solely on the model parameters for implicit reasoning and neglects the use of KG itself, which limits the performance since the model lacks the capacity to memorize a vast number of triplets. To tackle this issue, we introduce ReSKGC, a Retrieval-enhanced Seq2seq KGC model, which selects semantically relevant triplets from the KG and uses them as evidence to guide output generation with explicit reasoning. Our method has demonstrated state-of-the-art performance on benchmark datasets Wikidata5M and WikiKG90Mv2, which contain about 5M and\u00a0\u2026",
            "url": "https://dl.acm.org/doi/pdf/10.1145/3539618.3592052",
            "publicationVenue": "Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval",
            "citation_count": 3
        },
        {
            "title": "Pal: Program-aided language models",
            "authors": "Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, Graham Neubig",
            "year": "2023",
            "abstract": "Large language models (LLMs) have demonstrated an impressive ability to perform arithmetic and symbolic reasoning tasks, when provided with a few examples at test time (\" few-shot prompting\"). Much of this success can be attributed to prompting methods such as\" chain-of-thought\", which employ LLMs for both understanding the problem description by decomposing it into steps, as well as solving each step of the problem. While LLMs seem to be adept at this sort of step-by-step decomposition, LLMs often make logical and arithmetic mistakes in the solution part, even when the problem is decomposed correctly. In this paper, we present Program-Aided Language models (PAL): a novel approach that uses the LLM to read natural language problems and generate programs as the intermediate reasoning steps, but offloads the solution step to a runtime such as a Python interpreter. With PAL, decomposing the natural language problem into runnable steps remains the only learning task for the LLM, while solving is delegated to the interpreter. We demonstrate this synergy between a neural LLM and a symbolic interpreter across 13 mathematical, symbolic, and algorithmic reasoning tasks from BIG-Bench Hard and others. In all these natural language reasoning tasks, generating code using an LLM and reasoning using a Python interpreter leads to more accurate results than much larger models. For example, PAL using Codex achieves state-of-the-art few-shot accuracy on GSM8K, surpassing PaLM which uses chain-of-thought by absolute 15% top-1.",
            "url": "https://proceedings.mlr.press/v202/gao23f/gao23f.pdf",
            "publicationVenue": "International Conference on Machine Learning",
            "citation_count": 327
        },
        {
            "title": "Let's Sample Step by Step: Adaptive-Consistency for Efficient Reasoning with LLMs",
            "authors": "Pranjal Aggarwal, Aman Madaan, Yiming Yang",
            "year": "2023",
            "abstract": "A popular approach for improving the correctness of output from large language models (LLMs) is Self-Consistency - poll the LLM multiple times and output the most frequent solution. Existing Self-Consistency techniques always draw a constant number of samples per question, where a better approach will be to non-uniformly distribute the available budget based on the amount of agreement in the samples drawn so far. In response, we introduce Adaptive-Consistency, a cost-efficient, model-agnostic technique that dynamically adjusts the number of samples per question using a lightweight stopping criterion. Our experiments over 13 datasets and two LLMs demonstrate that Adaptive-Consistency reduces sample budget by up to 6.0 times with an average accuracy drop of less than 0.1%.",
            "url": "https://arxiv.org/pdf/2305.11860",
            "publicationVenue": "arXiv preprint arXiv:2305.11860",
            "citation_count": 3
        },
        {
            "title": "Active retrieval augmented generation",
            "authors": "Zhengbao Jiang, Frank F Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig",
            "year": "2023",
            "abstract": "Despite the remarkable ability of large language models (LMs) to comprehend and generate language, they have a tendency to hallucinate and create factually inaccurate output. Augmenting LMs by retrieving information from external knowledge resources is one promising solution. Most existing retrieval-augmented LMs employ a retrieve-and-generate setup that only retrieves information once based on the input. This is limiting, however, in more general scenarios involving generation of long texts, where continually gathering information throughout the generation process is essential. There have been some past efforts to retrieve information multiple times while generating outputs, which mostly retrieve documents at fixed intervals using the previous context as queries. In this work, we provide a generalized view of active retrieval augmented generation, methods that actively decide when and what to retrieve across the course of the generation. We propose Forward-Looking Active REtrieval augmented generation (FLARE), a generic retrieval-augmented generation method which iteratively uses a prediction of the upcoming sentence to anticipate future content, which is then utilized as a query to retrieve relevant documents to regenerate the sentence if it contains low-confidence tokens. We test FLARE along with baselines comprehensively over 4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves superior or competitive performance on all tasks, demonstrating the effectiveness of our method. Code and datasets are available at https://github.com/jzbjyb/FLARE.",
            "url": "https://arxiv.org/pdf/2305.06983",
            "publicationVenue": "arXiv preprint arXiv:2305.06983",
            "citation_count": 88
        },
        {
            "title": "Long-tailed Extreme Multi-label Text Classification by the Retrieval of Generated Pseudo Label Descriptions",
            "authors": "Ruohong Zhang, Yau-shian Wang, Yiming Yang, Donghan Yu, Tom Vu, Likun Lei",
            "year": "2023",
            "abstract": "Extreme Multi-label Text Classification (XMTC) has been a tough challenge in machine learning research and applications due to the sheer sizes of the label spaces and the severe data scarcity problem associated with the long tail of rare labels in highly skewed distributions. This paper addresses the challenge of tail label prediction by leveraging the power of dense neural retrieval model in mapping input documents (as queries) to relevant label descriptions. To further enhance the quality of label descriptions, we propose to generate pseudo label descriptions from a trained bag-of-words (BoW) classifier, which demonstrates better classification performance under severe scarce data conditions. The proposed approach achieves the state-of-the-art (SOTA) performance of overall label prediction on XMTC benchmark datasets and especially outperforms the SOTA models in the tail label prediction. We also provide a theoretical analysis for relating the BoW and neural models wrt performance lower bound.",
            "url": "https://aclanthology.org/2023.findings-eacl.81.pdf",
            "publicationVenue": "Findings of the Association for Computational Linguistics: EACL 2023",
            "citation_count": 3
        },
        {
            "title": "A Neural PDE Solver with Temporal Stencil Modeling",
            "authors": "Zhiqing Sun, Yiming Yang, Shinjae Yoo",
            "year": "2023",
            "abstract": "Numerical simulation of non-linear partial differential equations plays a crucial role in modeling physical science and engineering phenomena, such as weather, climate, and aerodynamics. Recent Machine Learning (ML) models trained on low-resolution spatio-temporal signals have shown new promises in capturing important dynamics in high-resolution signals, under the condition that the models can effectively recover the missing details. However, this study shows that significant information is often lost in the low-resolution down-sampled features. To address such issues, we propose a new approach, namely Temporal Stencil Modeling (TSM), which combines the strengths of advanced time-series sequence modeling (with the HiPPO features) and state-of-the-art neural PDE solvers (with learnable stencil modeling). TSM aims to recover the lost information from the PDE trajectories and can be regarded as a temporal generalization of classic finite volume methods such as WENO. Our experimental results show that TSM achieves the new state-of-the-art simulation accuracy for 2-D incompressible Navier-Stokes turbulent flows: it significantly outperforms the previously reported best results by 19.9% in terms of the highly-correlated duration time and reduces the inference latency into 80%. We also show a strong generalization ability of the proposed method to various out-of-distribution turbulent flow settings. Our code is available at \"https://github.com/Edward-Sun/TSM-PDE\".",
            "url": "https://arxiv.org/pdf/2302.08105",
            "publicationVenue": "arXiv preprint arXiv:2302.08105",
            "citation_count": 3
        },
        {
            "title": "Learning performance-improving code edits",
            "authors": "Alexander Shypula, Aman Madaan, Yimeng Zeng, Uri Alon, Jacob Gardner, Milad Hashemi, Graham Neubig, Parthasarathy Ranganathan, Osbert Bastani, Amir Yazdanbakhsh",
            "year": "2023",
            "abstract": "With the waning of Moore's law, optimizing program performance has become a major focus of software research. However, high-level optimizations such as API and algorithm changes remain elusive due to the difficulty of understanding the semantics of code. Simultaneously, pretrained large language models (LLMs) have demonstrated strong capabilities at solving a wide range of programming tasks. To that end, we introduce a framework for adapting LLMs to high-level program optimization. First, we curate a dataset of performance-improving edits made by human programmers of over 77K competitive C++ programming submission pairs, accompanied by extensive unit tests. A major challenge is the significant variability of measuring performance on commodity hardware, which can lead to spurious \"improvements\". To isolate and reliably evaluate the impact of program optimizations, we design an environment based on the gem5 full system simulator, the de facto simulator used in academia and industry. Next, we propose a broad range of adaptation strategies for code optimization; for prompting, these include retrieval-based few-shot prompting and chain-of-thought, and for finetuning, these include performance-conditioned generation and synthetic data augmentation based on self-play. A combination of these techniques achieves an average speedup of 5.65X on CodeLlama-13B and 6.86X on GPT-3.5, surpassing the best human performance (4.06X). We find our proposed performance-conditioned generation is particularly effective at improving performance as well as increasing the fraction of optimized programs.",
            "url": "https://arxiv.org/pdf/2302.07867",
            "publicationVenue": "arXiv preprint arXiv:2302.07867",
            "citation_count": 31
        }
    ],
    "Rita Singh": [
        {
            "title": "Completing visual objects via bridging generation and segmentation",
            "authors": "Xiang Li, Yinpeng Chen, Chung-Ching Lin, Hao Chen, Kai Hu, Rita Singh, Bhiksha Raj, Lijuan Wang, Zicheng Liu",
            "year": "2023",
            "abstract": "This paper presents a novel approach to object completion, with the primary goal of reconstructing a complete object from its partially visible components. Our method, named MaskComp, delineates the completion process through iterative stages of generation and segmentation. In each iteration, the object mask is provided as an additional condition to boost image generation, and, in return, the generated images can lead to a more accurate mask by fusing the segmentation of images. We demonstrate that the combination of one generation and one segmentation stage effectively functions as a mask denoiser. Through alternation between the generation and segmentation stages, the partial object mask is progressively refined, providing precise shape guidance and yielding superior object completion results. Our experiments demonstrate the superiority of MaskComp over existing approaches, e.g., ControlNet and Stable Diffusion, establishing it as an effective solution for object completion.",
            "url": "https://arxiv.org/pdf/2310.00808",
            "citation_count": 1
        },
        {
            "title": "Towards noise-tolerant speech-referring video object segmentation: Bridging speech and text",
            "authors": "Xiang Li, Jinglu Wang, Xiaohao Xu, Muqiao Yang, Fan Yang, Yizhou Zhao, Rita Singh, Bhiksha Raj",
            "year": "2023",
            "abstract": "Linguistic communication is prevalent in Human-Computer Interaction (HCI). Speech (spoken language) serves as a convenient yet potentially ambiguous form due to noise and accents, exposing a gap compared to text. In this study, we investigate the prominent HCI task, Referring Video Object Segmentation (R-VOS), which aims to segment and track objects using linguistic references. While text input is well-investigated, speech input is under-explored. Our objective is to bridge the gap between speech and text, enabling the adaptation of existing text-input R-VOS models to accommodate noisy speech input effectively. Specifically, we propose a method to align the semantic spaces between speech and text by incorporating two key modules: 1) Noise-Aware Semantic Adjustment (NSA) for clear semantics extraction from noisy speech; and 2) Semantic Jitter Suppression (SJS) enabling R-VOS models to tolerate noisy queries. Comprehensive experiments conducted on the challenging AVOS benchmarks reveal that our proposed method outperforms state-of-the-art approaches.",
            "url": "https://aclanthology.org/2023.emnlp-main.140/",
            "citation_count": 1
        },
        {
            "title": "BASS: Block-wise Adaptation for Speech Summarization",
            "authors": "Roshan Sharma, Kenneth Zheng, Siddhant Arora, Shinji Watanabe, Rita Singh, Bhiksha Raj",
            "year": "2023",
            "abstract": "End-to-end speech summarization has been shown to improve performance over cascade baselines. However, such models are difficult to train on very large inputs (dozens of minutes or hours) owing to compute restrictions and are hence trained with truncated model inputs. Truncation leads to poorer models, and a solution to this problem rests in block-wise modeling, i.e., processing a portion of the input frames at a time. In this paper, we develop a method that allows one to train summarization models on very long sequences in an incremental manner. Speech summarization is realized as a streaming process, where hypothesis summaries are updated every block based on new acoustic information. We devise and test strategies to pass semantic context across the blocks. Experiments on the How2 dataset demonstrate that the proposed block-wise training method improves by 3 points absolute on ROUGE-L over a truncated input baseline.",
            "url": "https://arxiv.org/pdf/2307.08217",
            "citation_count": 0
        },
        {
            "title": "Prompting Audios Using Acoustic Properties For Emotion Representation",
            "authors": "Hira Dhamyal, Benjamin Elizalde, Soham Deshmukh, Huaming Wang, Bhiksha Raj, Rita Singh",
            "year": "2023",
            "abstract": "Emotions lie on a continuum, but current models treat emotions as a finite valued discrete variable. This representation does not capture the diversity in the expression of emotion. To better represent emotions we propose the use of natural language descriptions (or prompts). In this work, we address the challenge of automatically generating these prompts and training a model to better learn emotion representations from audio and prompt pairs. We use acoustic properties that are correlated to emotion like pitch, intensity, speech rate, and articulation rate to automatically generate prompts i.e. 'acoustic prompts'. We use a contrastive learning objective to map speech to their respective acoustic prompts. We evaluate our model on Emotion Audio Retrieval and Speech Emotion Recognition. Our results show that the acoustic prompts significantly improve the model's performance in EAR, in various Precision@K metrics. In SER, we observe a 3.8% relative accuracy improvement on the Ravdess dataset.",
            "url": "https://arxiv.org/pdf/2310.02298",
            "citation_count": 1
        },
        {
            "title": "The hidden dance of phonemes and visage: Unveiling the enigmatic link between phonemes and facial features",
            "authors": "Liao Qu, Xianwei Zou, Xiang Li, Yandong Wen, Rita Singh, Bhiksha Raj",
            "year": "2023",
            "abstract": "This work unveils the enigmatic link between phonemes and facial features. Traditional studies on voice-face correlations typically involve using a long period of voice input, including generating face images from voices and reconstructing 3D face meshes from voices. However, in situations like voice-based crimes, the available voice evidence may be short and limited. Additionally, from a physiological perspective, each segment of speech -- phoneme -- corresponds to different types of airflow and movements in the face. Therefore, it is advantageous to discover the hidden link between phonemes and face attributes. In this paper, we propose an analysis pipeline to help us explore the voice-face relationship in a fine-grained manner, i.e., phonemes v.s. facial anthropometric measurements (AM). We build an estimator for each phoneme-AM pair and evaluate the correlation through hypothesis testing. Our results indicate that AMs are more predictable from vowels compared to consonants, particularly with plosives. Additionally, we observe that if a specific AM exhibits more movement during phoneme pronunciation, it is more predictable. Our findings support those in physiology regarding correlation and lay the groundwork for future research on speech-face multimodal learning.",
            "url": "https://arxiv.org/pdf/2307.13953",
            "citation_count": 3
        },
        {
            "title": "Token Prediction as Implicit Classification to Identify LLM-Generated Text",
            "authors": "Yutian Chen, Hao Kang, Vivian Zhai, Liangze Li, Rita Singh, Bhiksha Raj",
            "year": "2023",
            "abstract": "This paper introduces a novel approach for identifying the possible large language models (LLMs) involved in text generation. Instead of adding an additional classification layer to a base LM, we reframe the classification task as a next-token prediction task and directly fine-tune the base LM to perform it. We utilize the Text-to-Text Transfer Transformer (T5) model as the backbone for our experiments. We compared our approach to the more direct approach of utilizing hidden states for classification. Evaluation shows the exceptional performance of our method in the text classification task, highlighting its simplicity and efficiency. Furthermore, interpretability studies on the features extracted by our model reveal its ability to differentiate distinctive writing styles among various LLMs even in the absence of an explicit classifier. We also collected a dataset named OpenLLMText, containing approximately 340k text samples from human and LLMs, including GPT3.5, PaLM, LLaMA, and GPT2.",
            "url": "https://arxiv.org/pdf/2311.08723",
            "citation_count": 2
        },
        {
            "title": "The Interactive Machine Learning Paradigm",
            "authors": "Mark Lindsey, Richard M Stern, Bhiksha Raj, Aswin Sankaranarayanan, Rita Singh, Francis Kubala",
            "year": "2023",
            "abstract": "Machine Learning (ML) is a powerful tool that can assist humans in doing a wide variety of tasks. However, typical ML paradigms tend to remove the human element from the equation as frequently as possible in the name of automation. This approach may be useful for creating powerful solutions to a selection of pre-defined tasks, but it has lead to a dearth of methods for quickly training a machine to assist a human domain expert in a task previously undefined for ML. This problem is compounded in cases where data for the intended task are scarce or proprietary, since typical ML paradigms rely on large amounts of training data. To address this and other related issues with typical ML paradigms, I propose an Interactive Machine Learning (IML) paradigm in which a machine classifier is trained to perform a new task alongside a human domain expert as part of his or her operational workflow. This proposal includes a description of the IML paradigm and how it differs from other existing paradigms, as well as preliminary results showing the efficacy of the IML paradigm compared to typical supervised training. It also includes three pieces of proposed work that build on the existing IML pipeline to jointly optimize the performance of the classifier being trained and the amount of feedback required of the domain expert. These include a mathematical formulation of the IML objective function based on the joint optimization problem, a method for handling weighted error functions and imbalanced class distributions, and methods for exploiting the temporal nature of the IML training process.",
            "url": "http://cvis.cs.cmu.edu/cvis/docs/mark_proposal_final.pdf",
            "publicationVenue": null,
            "citation_count": 0
        },
        {
            "title": "GPT-Sentinel: Distinguishing Human and ChatGPT Generated Content",
            "authors": "Yutian Chen, Hao Kang, Vivian Zhai, Liangze Li, Rita Singh, Bhiksha Raj",
            "year": "2023",
            "abstract": "This paper presents a novel approach for detecting ChatGPT-generated vs. human-written text using language models. To this end, we first collected and released a pre-processed dataset named OpenGPTText, which consists of rephrased content generated using ChatGPT. We then designed, implemented, and trained two different models for text classification, using Robustly Optimized BERT Pretraining Approach (RoBERTa) and Text-to-Text Transfer Transformer (T5), respectively. Our models achieved remarkable results, with an accuracy of over 97% on the test dataset, as evaluated through various metrics. Furthermore, we conducted an interpretability study to showcase our model's ability to extract and differentiate key features between human-written and ChatGPT-generated text. Our findings provide important insights into the effective use of language models to detect generated text.",
            "url": "https://arxiv.org/pdf/2305.07969",
            "citation_count": 21
        },
        {
            "title": "Deriving vocal fold oscillation information from recorded voice signals using models of phonation",
            "authors": "Wayne Zhao, Rita Singh",
            "year": "2023",
            "abstract": "During phonation, the vocal folds exhibit a self-sustained oscillatory motion, which is influenced by the physical properties of the speakers vocal folds and driven by the balance of bio-mechanical and aerodynamic forces across the glottis. Subtle changes in the speakers physical state can affect voice production and alter these oscillatory patterns. Measuring these can be valuable in developing computational tools that analyze voice to infer the speakers state. Traditionally, vocal fold oscillations (VFOs) are measured directly using physical devices in clinical settings. In this paper, we propose a novel analysis-by-synthesis approach that allows us to infer the VFOs directly from recorded speech signals on an individualized, speaker-by-speaker basis. The approach, called the ADLES-VFT algorithm, is proposed in the context of a joint model that combines a phonation model (with a glottal flow waveform as the output) and a vocal tract acoustic wave propagation model such that the output of the joint model is an estimated waveform. The ADLES-VFT algorithm is a forward-backward algorithm which minimizes the error between the recorded waveform and the output of this joint model to estimate its parameters. Once estimated, these parameter values are used in conjunction with a phonation model to obtain its solutions. Since the parameters correlate with the physical properties of the vocal folds of the speaker, model solutions obtained using them represent the individualized VFOs for each speaker. The approach is flexible and can be applied to various phonation models. In addition to presenting the methodology, we show how the VFOs can be quantified from a dynamical systems perspective for classification purposes. Mathematical derivations are provided in an appendix for better readability.",
            "url": "https://www.mdpi.com/1099-4300/25/7/1039",
            "citation_count": 2
        },
        {
            "title": "Loft: Local proxy fine-tuning for improving transferability of adversarial attacks against large language model",
            "authors": "Muhammad Ahmed Shah, Roshan Sharma, Hira Dhamyal, Raphael Olivier, Ankit Shah, Joseph Konan, Dareen Alharthi, Hazim T Bukhari, Massa Baali, Soham Deshmukh, Michael Kuhlmann, Bhiksha Raj, Rita Singh",
            "year": "2023",
            "abstract": "It has been shown that Large Language Model (LLM) alignments can be circumvented by appending specially crafted attack suffixes with harmful queries to elicit harmful responses. To conduct attacks against private target models whose characterization is unknown, public models can be used as proxies to fashion the attack, with successful attacks being transferred from public proxies to private target models. The success rate of attack depends on how closely the proxy model approximates the private model. We hypothesize that for attacks to be transferrable, it is sufficient if the proxy can approximate the target model in the neighborhood of the harmful query. Therefore, in this paper, we propose \\emph{Local Fine-Tuning (LoFT)}, \\textit{i.e.}, fine-tuning proxy models on similar queries that lie in the lexico-semantic neighborhood of harmful queries to decrease the divergence between the proxy and target models. First, we demonstrate three approaches to prompt private target models to obtain similar queries given harmful queries. Next, we obtain data for local fine-tuning by eliciting responses from target models for the generated similar queries. Then, we optimize attack suffixes to generate attack prompts and evaluate the impact of our local fine-tuning on the attack's success rate. Experiments show that local fine-tuning of proxy models improves attack transferability and increases attack success rate by $39\\%$, $7\\%$, and $0.5\\%$ (absolute) on target models ChatGPT, GPT-4, and Claude respectively.",
            "url": "https://arxiv.org/pdf/2310.04445",
            "citation_count": 5
        },
        {
            "title": "Towards Robust Audiovisual Segmentation in Complex Environments with Quantization-based Semantic Decomposition",
            "authors": "Xiang Li, Jinglu Wang, Xiaohao Xu, Xiulian Peng, Rita Singh, Yan Lu, Bhiksha Raj",
            "year": "2023",
            "abstract": "Audiovisual segmentation (AVS) is a challenging task that aims to segment visual objects in videos according to their associated acoustic cues. With multiple sound sources and background disturbances involved, establishing robust correspondences between audio and visual contents poses unique challenges due to (1) complex entanglement across sound sources and (2) frequent changes in the occurrence of distinct sound events. Assuming sound events occur independently, the multi-source semantic space can be represented as the Cartesian product of single-source sub-spaces. We are motivated to decompose the multi-source audio semantics into single-source semantics for more effective interactions with visual content. We propose a semantic decomposition method based on product quantization, where the multi-source semantics can be decomposed and represented by several disentangled and noise-suppressed single-source semantics. Furthermore, we introduce a global-to-local quantization mechanism, which distills knowledge from stable global (clip-level) features into local (frame-level) ones, to handle frequent changes in audio semantics. Extensive experiments demonstrate that our semantically decomposed audio representation significantly improves AVS performance, e.g., +21.2% mIoU on the challenging AVS-Semantic benchmark with ResNet50 backbone. this https URL.",
            "url": "https://arxiv.org/pdf/2310.00132",
            "citation_count": 0
        },
        {
            "title": "A Gene-Based Algorithm for Identifying Factors That May Affect a Speaker's Voice",
            "authors": "Rita Singh",
            "year": "2023",
            "abstract": "Over the past decades, many machine-learning- and artificial-intelligence-based technologies have been created to deduce biometric or bio-relevant parameters of speakers from their voice. These voice profiling technologies have targeted a wide range of parameters, from diseases to environmental factors, based largely on the fact that they are known to influence voice. Recently, some have also explored the prediction of parameters whose influence on voice is not easily observable through data-opportunistic biomarker discovery techniques. However, given the enormous range of factors that can possibly influence voice, more informed methods for selecting those that may be potentially deducible from voice are needed. To this end, this paper proposes a simple path-finding algorithm that attempts to find links between vocal characteristics and perturbing factors using cytogenetic and genomic data. The links represent reasonable selection criteria for use by computational by profiling technologies only, and are not intended to establish any unknown biological facts. The proposed algorithm is validated using a simple example from medical literaturethat of the clinically observed effects of specific chromosomal microdeletion syndromes on the vocal characteristics of affected people. In this example, the algorithm attempts to link the genes involved in these syndromes to a single example gene (FOXP2) that is known to play a broad role in voice production. We show that in cases where strong links are exposed, vocal characteristics of the patients are indeed reported to be correspondingly affected. Validation experiments and subsequent analyses confirm that the methodology could be potentially useful in predicting the existence of vocal signatures in nave cases where their existence has not been otherwise observed.",
            "url": "https://www.mdpi.com/1099-4300/25/6/897",
            "citation_count": 0
        },
        {
            "title": "Importance of negative sampling in weak label learning",
            "authors": "Ankit Shah, Fuyu Tang, Zelin Ye, Rita Singh, Bhiksha Raj",
            "year": "2023",
            "abstract": "Weak-label learning is a challenging task that requires learning from data \"bags\" containing positive and negative instances, but only the bag labels are known. The pool of negative instances is usually larger than positive instances, thus making selecting the most informative negative instance critical for performance. Such a selection strategy for negative instances from each bag is an open problem that has not been well studied for weak-label learning. In this paper, we study several sampling strategies that can measure the usefulness of negative instances for weak-label learning and select them accordingly. We test our method on CIFAR-10 and AudioSet datasets and show that it improves the weak-label classification performance and reduces the computational cost compared to random sampling methods. Our work reveals that negative instances are not all equally irrelevant, and selecting them wisely can benefit weak-label learning.",
            "url": "https://arxiv.org/pdf/2309.13227",
            "citation_count": 0
        }
    ]
}