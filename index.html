<!DOCTYPE HTML>
<html lang="en">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Danny To Eun Kim</title>
  
  <meta name="author" content="Danny To Eun Kim">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta property='og:title' content='TEKnology'/>
  <meta property='og:image' content='https://github.com/kimdanny/kimdanny.github.io/blob/master/images/profilepic.jpeg'/>
  <meta property='og:description' content='Website of Danny To Eun Kim'/>
  <meta property='og:url' content='https://kimdanny.github.io'/>
  <meta property='og:image:width' content='1200' />
  <meta property='og:image:height' content='627' />
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
  <!-- <link rel="image_src" href="https://github.com/kimdanny/kimdanny.github.io/blob/master/images/profilepic.jpeg"> -->
</head>

<body onload="zoom()">
  <script type="text/javascript">
    function zoom() {
            document.body.style.zoom = "100%" 
    }
  </script>
  <!-- <div style="overflow-x:auto;"> -->
  <div>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <div id="top">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:20px;width:70%;vertical-align:middle">
                  <p style="text-align:center">
                    <name>(Danny) To Eun Kim</name> 
                    <br>ÍπÄÎèÑÏùÄ
                    <br>toeunkim{at}cmu{dot}edu
                    <div class="responsive-text">
                      (View from wider screen to see with pictures!)
                    </div>
                  </p>
                  
                  <!-- TODO: make separate news section which can be folded -->
                  <!-- <p>
                    <b>News</b>: I am excited to join <a href="https://lti.cs.cmu.edu">The Language Technologies Institute</a> 
                    at <a href="https://www.cs.cmu.edu">CMU School of Computer Science</a> as a PhD Student this Fall 2023!
                  </p> -->

                  <p>
                    I'm a third year PhD student at CMU <a href="https://lti.cs.cmu.edu">Language Technologies Institute (LTI)</a>, advised by Prof. <a href="https://841.io">Fernando Diaz</a>.
                    My research interests span the fields of Natural Language Processing (NLP) and Information Retrieval (IR), 
                      with recent emphasis on <b>retrieval-enhanced machine learning (REML) and known-item retrieval</b>. 
                  </p>

                  <p>
                    Prior to CMU, while I was in the UK, 
                    I was a graduate researcher at <a href="https://wi.cs.ucl.ac.uk">Web Intelligence Group</a>, <a href="https://www.ucl.ac.uk">University College London (UCL)</a>, 
                      where I received M.Eng. in Computer Science in 2022.
                    I was working with Prof. <a href="https://sites.google.com/site/emineyilmaz/">Emine Yilmaz</a> and Prof. <a href="https://aldolipani.com">Aldo Lipani</a>
                      on <b>Conversational AI and User Simulation</b>.
                    Also, I was a member of both <a href="https://www.turing.ac.uk/research/interest-groups/knowledge-graphs">Knowledge Graphs</a> 
                      and <a href="https://www.turing.ac.uk/research/interest-groups/natural-language-processing">NLP</a> interest group at <a href="https://www.turing.ac.uk">The Alan Turing Institute</a>.
                  </p>

                  <p>
                    During the summer of 2022, I interned at <a href="https://www.raft.ai">Raft</a> as a Machine Learning Engineer, 
                      automating the paperwork in freight forwarding industry by OCR and NLP.
                    <br>
                    From May 2021, I joined team <a href="https://www.amazon.science/alexa-prize/teams/university-college-london-condita">Condita</a> as a main developer to compete in the first Amazon <a href="https://www.amazon.science/alexa-prize/taskbot-challenge">Alexa Prize TaskBot Challenge</a>, 
                      where I spent significant amount of time, constructing a real-time, multi-modal, knowledge-intensive, and interactive conversational assistant; 
                      our team has made to the quarterfinals.
                    <br>
                    During my undergraduate degree, I worked with Prof. <a href="https://uclic.ucl.ac.uk/people/marianna-obrist">Marianna Obrist</a> at <a href="https://uclic.ucl.ac.uk">UCL (Human-Computer) Interaction Centre</a>,
                      finding ways to cluster text-based stories by authors' smell experience.
                  </p>

                  <p>
                    I take pride in being one of the early members of the <a href="https://uclaisociety.co.uk">UCL Artifical Intelligence Society</a>, and during my three years of active involvement, 
                    I had the privilege of founding the first <a href="https://github.com/UCLAIS/Machine-Learning-Tutorials">Machine Learning tutorial series</a>, which has become an annual tradition. 
                  </p>
                  
                  <p style="text-align:center">
                    <a href="assets/CV.pdf">CV</a> &nbsp/&nbsp
                    <a href="https://scholar.google.com/citations?user=3ymamHAAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                    <a href="https://kimdanny.github.io/blog/">Blog</a> &nbsp/&nbsp
                    <a href="https://twitter.com/TEKnologyy">X.com</a> &nbsp/&nbsp
                    <a href="https://github.com/kimdanny/">GitHub</a> 
                    <br><br>
                    <a href="https://twitter.com/TEKnologyy?ref_src=twsrc%5Etfw" class="twitter-follow-button" data-show-count="false">Follow @TEKnologyy</a><script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
                  </p>
                </td>
                <td style="width:30%;" class="responsive-image">
                  <!-- Uncomment the below anchor tag to enable the link of the profile pic -->
                  <!-- <a href="images/profilepic.jpeg"> -->
                    <img style="width:100%;max-width:100%" alt="profile photo" src="images/profilepic.jpeg" class="hoverZoomLink">
                  <!-- </a> -->
                </td>
              </tr>
            </tbody>
          </table>
        </div>
        
        <!-- Table of Contents -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>
            <tr>
              <td align="center" width="100%" style="padding:10px;vertical-align:middle" bgcolor="#eeeeee">
                <b>Table of Contents</b> &nbsp 
                  <br><br>
                  <a href="#research">Research</a> &nbsp/&nbsp
                  <a href="#publications">Publications</a> &nbsp/&nbsp
                  <!-- <br> -->
                  <!-- <a href="#presentations">Presentations & Workshops</a> &nbsp/&nbsp
                  <br> -->
                  <a href="#teaching">Teaching & Mentoring</a>
              </td>
            </tr>
          </tbody>
        </table>
        

        <!-- Research Section -->
        <div id="research">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <heading><b>Research</b></heading>
                <!-- <p>
                  The ultimate goal of my research aims for a real-time conversational assistants.
                  Therefore, I try to find better ways to <b>encode</b> human knowledge into computable forms, i.e., language models or graphs, 
                  and make the knowledge <b>retrievable</b> by (conversational) search systems.
                </p> -->

                <p>
                  <ul>
                    <!-- #1 -->
                    <li>Retrieval-Enhanced Machine Learning (REML) and RAG</li>
                    <ul>
                      <li>REML Framework: </li>
                      <ul>
                        <li><a href="https://arxiv.org/abs/2407.12982">[Survey]</a> <a href="https://dl.acm.org/doi/10.1145/3673791.3698439">[SIGIR-AP'24 Tutorial]</a> <a href="https://dl.acm.org/doi/10.1145/3726302.3731695">[SIGIR'25 Tutorial]</a></li>
                      </ul>
                      <li>Efficient and Effective RAG: </li>
                      <ul>
                        <li><a href="https://arxiv.org/abs/2506.13743">[SIGIR'25 LiveRAG]</a> <a href="https://www.arxiv.org/abs/2506.15862">[EMNLP'25]</a></li>
                      </ul>
                    </ul>
                    <!-- #2 -->
                    <li>Real-World AI Systems</li>
                    <ul>
                      <li>Data Provider Retention and Attribution: </li>
                      <ul>
                        <li><a href="https://dl.acm.org/doi/10.1145/3731120.3744599">[ICTIR'25]</a></li>
                      </ul>
                      <li>Advertisement in Conversational Search:</li>
                      <ul>
                        <li><a href="https://arxiv.org/abs/2507.00509">[CLEF'25 Touch√© Lab]</a></li>
                      </ul>
                      <li>Full-Stack Chatbot Development:</li>
                      <ul>
                        <li><a href="https://www.amazon.science/alexa-prize/proceedings/condita-a-state-machine-like-architecture-for-multi-modal-task-bots">[Alexa Prize TaskBot Challenge'21]</a></li>
                      </ul>
                      <li>Asking Clarifying Questions:</li>
                      <ul>
                        <li><a href="https://arxiv.org/abs/2305.05754">[NeurIPS'22 IGLU Competition]</a></li>
                      </ul>
                    </ul>
                    <!-- #3 -->
                    <li>Simulated Evaluation and Benchmarking of AI Systems</li>
                    <ul>
                      <li>User/Query Simulation:</li>
                      <ul>
                        <li><a href="https://dl.acm.org/doi/10.1145/3726302.3730335">[SIGIR'25]</a> <a href="https://dl.acm.org/doi/abs/10.1145/3477495.3531814">[SIGIR'22]</a></li>
                      </ul>
                      <li>Tip of the Tongue Retrieval:</li>
                      <ul>
                        <li>[NTCIR-19] [TREC'25] <a href="https://trec.nist.gov/pubs/trec33/papers/Overview_tot.pdf">[TREC'24]</a></li>
                      </ul>
                    </ul>
                  </ul>
                </p>

                <p>
                  <b>Theses</b>
                  <ul>
                    <li>
                      Master's: <a href="https://kimdanny.github.io/assets/ucl-master-thesis.pdf">Multi-Task Neural User Simulator for Task Oriented Dialogue System</a>
                    </li>
                    <li>
                      Bachelor's: <a href="https://drive.google.com/file/d/1lhXtb0c5mApa0n78qHJL6rdkxGpHI1e9/view">Exploring the Potential of Automating the Process of Clustering Smell Stories</a>
                    </li>
                  </ul>
                </p>
              </td>
            </tr>
            </tbody>
          </table>
        </div>

        <hr style="width:100%;text-align:left;margin-left:0">

        <!-- Publications Section -->
        <div id="publications">
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody>
            <tr>
              <td>
                <heading><b>Selected Publications</b></heading>
                <br>
                For full list of publications, please refer to my <a href="https://scholar.google.com/citations?user=3ymamHAAAAAJ&hl=en">Google Scholar</a>.
              </td>
            </tr>
            </tbody>
          </table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>

              <!-- MoR -->
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle" class="responsive-image">
                  <img src='images/mor.png' width="200" height="200">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://www.arxiv.org/abs/2506.15862">
                    <papertitle>MoR: Better Handling Diverse Queries with a Mixture of Sparse, Dense, and Human Retrievers</papertitle>
                  </a>
                  <br>
                  Jushaan Singh Kalra<sup>*</sup>, Xinran Zhao<sup>*</sup>, <strong>To Eun Kim</strong>, Fengyu Cai, Fernando Diaz, Tongshuang Wu
                  <br>
                  <em>EMNLP 2025 (Main)</em>
                  <br>
                  <a href="https://www.arxiv.org/abs/2506.15862">arXiv</a> / 
                  <a href="https://github.com/Josh1108/MixtureRetrievers">Code</a>
                  <p>
                    Can we dynamically select and integrate multiple retrievers for each individual query, without the need for manual selection? 
                    In this work, we validate this intuition with quantitative analysis and introduce mixture of retrievers: a zero-shot, weighted combination of heterogeneous retrievers.
                  </p>
                </td>
              </tr>

              <!-- Ad-RAG -->
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle" class="responsive-image">
                  <img src='images/ad-rag.png' width="200" height="200">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2507.00509">
                    <papertitle>TeamCMU at Touch√©: Adversarial Co-Evolution for Advertisement Integration and Detection in Conversational Search</papertitle>
                  </a>
                  <br>
                  <strong>To Eun Kim</strong>, Jo√£o Coelho, Gbemileke Onilude, Jai Singh
                  <br>
                  <em>CLEF 2025 Touch√© Lab</em><br>
                  üèÜ Best Paper Award
                  <br>
                  <a href="https://arxiv.org/abs/2507.00509">arXiv</a> / 
                  <a href="https://github.com/kimdanny/TeamCMU-AdRAG">Code</a>
                  <p>
                    We propose a modular pipeline for advertisement management in RAG-based conversational systems, consisting of an ad-rewriter for 
                    seamless ad integration and a robust ad-classifier for detection.
                  </p>
                </td>
              </tr>

              <!-- LTRR -->
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle" class="responsive-image">
                  <img src='images/ltrr.png' width="200" height="200">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2506.13743">
                    <papertitle>LTRR: Learning To Rank Retrievers for LLMs</papertitle>
                  </a>
                  <br>
                  <strong>To Eun Kim</strong>, Fernando Diaz
                  <br>
                  <em>SIGIR 2025 LiveRAG Workshop Spotlight</em>
                  <br>
                  <a href="https://arxiv.org/abs/2506.13743">arXiv</a> / 
                  <a href="https://github.com/kimdanny/Starlight-LiveRAG">Code</a>
                  <p>
                    Ranking the Rankers.
                    Exploration of a query routing approach that dynamically selects from a pool of retrievers based on the query, 
                    using both train-free heuristics and learned routing models. We frame routing as a learning-to-rank (LTR) problem and introduce LTRR, 
                    a framework that learns to rank retrievers by their expected utility gain to downstream LLM performance.
                  </p>
                </td>
              </tr>

              <!-- TOT Elicitation -->
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle" class="responsive-image">
                  <img src='images/tot-elicitation.png' width="200" height="200">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://dl.acm.org/doi/10.1145/3726302.3730335">
                    <papertitle>Tip of the Tongue Query Elicitation for Simulated Evaluation</papertitle>
                  </a>
                  <br>
                  Yifan He<sup>*</sup>, <strong>To Eun Kim<sup>*</sup></strong>, Fernando Diaz, Jaime Arguello, Bhaskar Mitra
                  <br>
                  * denotes equal contribution. <br>
                  <em>SIGIR 2025</em>
                  <br>
                  <a href="https://dl.acm.org/doi/10.1145/3726302.3730335">paper</a> / 
                  <a href="https://github.com/kimdanny/llm-tot-query-elicitation">LLM-Elicitation Code</a> /
                  <a href="https://github.com/kimdanny/human-tot-query-elicitation-mturk">Human-Elicitation Interface Code</a>
                  <p>
                    We introduce methods for eliciting TOT queries using large language models (LLMs) and human participants, enabling scalable and systematic evaluation of TOT retrieval systems. 
                    Our LLM-based simulator generates synthetic TOT queries that align well with human-created ones, while our human elicitation interface uses visual stimuli to induce the TOT state. 
                    These approaches reduce reliance on community question-answering data and expand query coverage to new domains. 
                    Our LLM-elicited queries are featured in the TREC 2024 TOT track, with the inclusion of human-elicited queries scheduled for TREC 2025. 
                  </p>
                </td>
              </tr>
              
              <!-- Fair RAG -->
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle" class="responsive-image">
                  <img src='images/fair-rag.png' width="200" height="200">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://dl.acm.org/doi/10.1145/3731120.3744599">
                    <papertitle>Towards Fair RAG: On the Impact of Fair Ranking in Retrieval-Augmented Generation</papertitle>
                  </a>
                  <br>
                  <strong>To Eun Kim</strong>, Fernando Diaz
                  <br>
                  <em>ICTIR 2025</em> <br>
                  <em>NeurIPS 2024 AFME Workshop Spotlight</em>
                  <br>
                  <a href="https://dl.acm.org/doi/10.1145/3731120.3744599">paper</a> / 
                  <a href="https://github.com/kimdanny/Fair-RAG">code</a>
                  <p>
                    We conduct the first comprehensive study of fairness-aware ranking in RAG systems, focusing on both ranking fairness and attribution fairness‚Äîensuring balanced exposure of retrieved documents and equitable crediting of sources. 
                    Analyzing 12 RAG models across 7 tasks, we find that fairness-aware retrieval often maintains or enhances both ranking effectiveness and generation quality. 
                    Our results highlight the importance of item-side fairness in retrieval and generation, providing key insights for building more responsible and equitable RAG systems.
                  </p>
                </td>
              </tr>


              <!-- REML Survey -->
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle" class="responsive-image">
                  <img src='images/reml-survey.png' width="200" height="200">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2407.12982">
                    <papertitle>Retrieval-Enhanced Machine Learning: Synthesis and Opportunities</papertitle>
                  </a>
                  <br>
                  <strong>To Eun Kim</strong>, Alireza Salemi, Andrew Drozdov, Fernando Diaz, Hamed Zamani
                  <br>
                  Preprint, 2024
                  <br>
                  <a href="https://arxiv.org/abs/2407.12982">arXiv</a>
                  <p>
                    In this work, we posit that the paradigm of retrieval-enhancement can be extended to a broader spectrum of machine learning (ML) 
                    such as computer vision, time series prediction, and computational biology, not just limited to NLP. 
                    We introduce a formal framework of Retrieval-Enhanced Machine Learning (REML), 
                    by synthesizing the literature in various domains in ML with consistent notations.
                  </p>
                </td>
              </tr>

              <!-- Diffusion Survey -->
              <!-- <tr>
                <td style="padding:20px;width:25%;vertical-align:middle" class="responsive-image">
                  <img src='images/diffusion-survey.png' width="200" height="200">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2306.04139">
                    <papertitle>A Comprehensive Survey on Generative Diffusion Models for Structured Data</papertitle>
                  </a>
                  <br>
                  Heejoon Koo, <strong>To Eun Kim</strong>
                  <br>
                  Preprint, 2023
                  <br>
                  <a href="https://arxiv.org/abs/2306.04139">arXiv</a>
                  <p>
                    There is still a lack of literature and its reviews on structured data modelling via diffusion models, compared to other data modalities such as visual and textual data. 
                    To address this gap, we present a comprehensive review of recently proposed diffusion models in the field of structured data. 
                    First, this survey provides a concise overview of the score-based diffusion model theory, 
                      subsequently proceeding to the technical descriptions of the majority of pioneering works that used structured data in both data-driven general tasks and domain-specific applications. 
                    Thereafter, we analyse and discuss the limitations and challenges shown in existing works and suggest potential research directions. 
                  </p>
                </td>
              </tr> -->

              <!-- IGLU challenge -->
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle" class="responsive-image">
                  <img src='images/iglu-chall.png' width="200" height="200">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2305.05754">
                    <papertitle>When and What to Ask Through World States and Text Instructions: IGLU NLP Challenge Solution</papertitle>
                  </a>
                  <br>
                  Z. Shi*, J. Ramos*, <strong>To Eun Kim</strong>, X. Wang, H. Rahmani, Aldo Lipani
                  <br>
                  * denotes equal contribution. <br>
                  <em>NeurIPS IGLU Workshop 2022</em> 
                  <br>
                  üèÜ Research Prize Winner
                  <br>
                  <a href="https://arxiv.org/abs/2305.05754">arXiv</a>
                  <p>
                    In the NeurIPS 2022 IGLU Challenge NLP Task, we address two key research questions: 1) when should the agent ask for clarification, and 
                    2) what clarification questions should it ask.
                    In this report, we briefly introduce our methods for the classification and ranking task. 
                    For the classification task, our model achieves an F1 score of 0.757, which placed the 3rd on the leaderboard. 
                    For the ranking task, our model achieves about 0.38 for Mean Reciprocal Rank by extending the traditional ranking model. 
                    Lastly, we discuss various neural approaches for the ranking task and future direction.
                  </p>
                </td>
              </tr>

              <!-- sigir -->
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle" class="responsive-image">
                  <img src='images/sigir22-t5.png' width="200" height="200">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://doi.org/10.1145/3477495.3531814">
                    <papertitle>A Multi-Task Based Neural Model to Simulate Users in Goal-Oriented Dialogue Systems</papertitle>
                  </a>
                  <br>
                  <strong>To Eun Kim</strong>, Aldo Lipani
                  <br>
                  <em>SIGIR 2022</em>
                  <br>
                  <a href="https://doi.org/10.1145/3477495.3531814">paper</a> /
                  <a href="https://www.researchgate.net/publication/362704102_A_Multi-Task_Based_Neural_Model_to_Simulate_Users_in_Goal-Oriented_Dialogue_Systems">poster</a> /
                  <a href="https://github.com/kimdanny/user-simulation-t5">code</a>
                  <p></p>
                  <p>
                    Conversational User Simulator that 1) generates user-side utterance, 2) predicts user's next action and 3) satisfaction level by multi-task learning.
                    SOTA in Satisfaction and Action prediction in USS dataset
                  </p>
                </td>
              </tr>

              <!-- alexaprize -->
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle" onmouseout="condita_stop()" onmouseover="condita_start()" class="responsive-image">
                  <div class="one">
                    <div class="two" id='condita_image'>
                      <img src='images/condita-after.png' width="200" height="190">
                    </div>
                    <img src='images/condita-before.png' width="200" height="190">
                  </div>
                </td>
                <script type="text/javascript">
                  function condita_start() {
                    document.getElementById('condita_image').style.opacity = "1";
                  }
  
                  function condita_stop() {
                    document.getElementById('condita_image').style.opacity = "0";
                  }
                  condita_stop()
                </script>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://www.amazon.science/alexa-prize/proceedings/condita-a-state-machine-like-architecture-for-multi-modal-task-bots">
                    <papertitle>Condita: A State Machine Like Architecture for Multi-Modal Task Bots</papertitle>
                  </a>
                  <br>
                  Jerome Ramos*, <strong>To Eun Kim*</strong>, Z. Shi, X. Fu, F. Ye, Y. Feng, Aldo Lipani
                  <br>
                  * denotes equal contribution. <br>
                  <em>Alexa Prize TaskBot Challenge Proceedings 2022</em>
                  <br>
                  <a href="https://www.amazon.science/alexa-prize/proceedings/condita-a-state-machine-like-architecture-for-multi-modal-task-bots">paper</a> 
                  <p>
                    We present <b>CO</b>oking-a<b>N</b>d-<b>DI</b>y-<b>TA</b>sk-based (Condita) task-oriented dialogue system, for the 2021 Alexa Prize TaskBot Challenge. 
                    Condita provides an engaging multi-modal agent that assists users in cooking and home improvement tasks, creating a memorable and enjoyable experience to users. 
                    We discuss Condita's state machine like architecture and analyze the various conversational strategies implemented that allowed us to achieve excellent performance throughout the competition.
                  </p>
                </td>
              </tr>

              

              <!-- esann -->
              <!-- <tr>
                <td style="padding:20px;width:25%;vertical-align:middle" class="responsive-image">
                  <img src='images/esann-attention.png' width="200" height="200">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://www.esann.org/sites/default/files/proceedings/2022/ES2022-10.pdf">
                    <papertitle>Attention-based Ingredient Phrase Parser</papertitle>
                  </a>
                  <br>
                  Z. Shi, P. Ni, M. Wang, <strong>To Eun Kim</strong>,
                  <a href="https://aldolipani.com">Aldo Lipani</a>
                  <br>
                  <em>ESANN 2022</em>
                  <br>
                  <a href="https://www.esann.org/sites/default/files/proceedings/2022/ES2022-10.pdf">paper</a> /
                  <a href="https://arxiv.org/abs/2210.02535">arXiv</a> /
                  <a href="https://github.com/ZhengxiangShi/IngredientParsing">code</a>
                  <p>
                    Spin-off research from the Alexa Prize TaskBot Challenge. 
                    Assisting users to cook is one of these tasks that are expected to be solved by intelligent assistants, 
                      where ingredients and its corresponding attributes, such as name, unit, and quantity, should be provided to users precisely and promptly. 
                    To provide an engaged and successful conversational service to users for cooking tasks, we propose a new ingredient parsing model. 
                  </p>
                </td>
              </tr> -->

            </tbody>
          </table>
        </div>

        <hr style="width:100%;text-align:left;margin-left:0">


        <!-- Presentations Section -->
        <!-- <div id="presentations">
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody>
            <tr>
              <td>
                <heading><b>Presentations & Workshops</b></heading>
              </td>
            </tr>
            </tbody>
          </table>
          <table width="100%" align="center" border="0" cellpadding="20">
            <tbody>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle" class="responsive-image">
                  <img src="images/logos/ATI.png" width="160" height="160">
                </td>
                <td width="75%" valign="center">
                  <ul>
                    <li>
                      <b>Can Dialogue Systems Have Theory-of-Mind Ability?</b>
                      <br>
                      Presented at the <a href="https://www.turing.ac.uk/research/interest-groups/natural-language-processing">NLP Interest Group</a>
                      <br>
                      at The Alan Turing Institute. <br>
                      24th November, 2022
                      <br>
                      <a href="https://www.youtube.com/watch?v=Cj9M88oelHg">Recording</a> / <a href="https://drive.google.com/file/d/1gpAGb6hy3Drn7JIq4P1FfD-5ZrDg4m8m/view">Slides</a>
                    </li>
                    
                    <br>

                    <li>
                      <b>Personal Knowledge Graphs: Towards Better Conversational Assistants</b>
                      <br>
                      Presented at the 5th Meet-up of <a href="https://www.turing.ac.uk/research/interest-groups/knowledge-graphs">Knowledge Graphs Interest Group</a>
                      <br>
                      at The Alan Turing Institute. <br>
                      27th October, 2022
                      <br>
                      <a href="https://www.youtube.com/watch?v=jn2NjUIZIoE">Recording</a> / <a href="https://drive.google.com/file/d/1q6O9NXCtJn1aabwFriRkVC3LMc6SYGEL/view">Slides</a>
                    </li>
                  </ul>
                </td>
              </tr>
              
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle" class="responsive-image">
                  <img src="images/scai2022.png" width="160" height="160">
                </td>
                <td width="75%" valign="center">
                  <b>A Multi-Task Based Neural Model to Simulate Users in Goal-Oriented Dialogue Systems</b>
                  <br>
                  Presented at the <a href="https://scai.info">SCAI: Search-Oriented Conversational AI</a> Workshop
                  <br>
                  15th July, 2022
                  <br>
                  <a href="https://www.youtube.com/watch?v=bBkPByrt3Cg&t=2075s">Recording</a> /
                  <a href="https://drive.google.com/file/d/1SuJOeeH8am0WQ4Upp3e-GyEFhlJz1sAY/view">Slides</a>
                </td>
              </tr>
            
            </tbody>
          </table>
        </div>

        <hr style="width:100%;text-align:left;margin-left:0"> -->

        <!-- Teaching and Society Section -->
        <div id="teaching">
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody>
            <tr>
              <td>
                <heading><b>Teaching & Mentoring</b></heading>
              </td>
            </tr>
            </tbody>
          </table>
          <table width="100%" align="center" border="0" cellpadding="20">
            <tbody>

              <!-- At CMU -->
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle" class="responsive-image">
                  <img src="images/logos/cmu-wordmark.png" width="160" height="160">
                </td>
                <td width="75%" valign="center">
                  
                  <!-- Mentoring -->
                  <b>Mentoring</b>
                  <ul>
                    <li>CMU Paths to AI (undergraduate AI mentoring)</li>
                    <li>Graduate Application Support Program</li>
                  </ul>

                </td>
              </tr>

              <!-- At UCL -->
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle" class="responsive-image">
                  <img src="images/logos/ucl-logo.png" width="160" height="160">
                </td>
                <td width="75%" valign="center">
                  
                  <!-- IPPC -->
                  <b>Course Co-organiser</b>
                  <ul>
                    <li>Intensive Python Programming Course (for MSc students' dissertation)</li>
                  </ul>

                  <!-- TA -->
                  <b>Teaching Assistants</b> (MSc courses)
                  <ul>
                    <li>CEGE0096: Geospatial Programming (Fall 2022)
                      <ul>
                        <li>Coursework marking automation</li>
                      </ul>
                    </li>
                    <li>CEGE0004: Machine Learning for Data Science (Spring 2023)</li>
                    <li>COMP0071 Software Engineering (Spring 2023)</li>
                    <li>COMP0189: Applied Artificial Intelligence (Spring 2023)</li>
                    <ul>
                      <li>Leading practical classes <a href="https://github.com/kimdanny/COMP0189-practical">[GitHub Repo]</a></li>
                    </ul>
                  </ul>
                  
                  <!-- Transition Mentor -->
                  <b>Transition Mentor</b>
                  <ul>
                    <li>Helping 1<sup>st</sup> year CS students with programming (C, Java, Python, Haskell)</li>
                  </ul>
                </td>
              </tr>
              
              <!-- UCL AI Society -->
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle" class="responsive-image">
                  <img src="images/logos/ucl-ai-soc.png" width="160" height="160">
                </td>
                <td width="75%" valign="center">
                  <a href="https://github.com/UCLAIS"><b>UCL Artificial Intelligence Society</b></a>
                  <br> 
                  <i>A Founder, Maintainer, and Lecturer of <a href="https://github.com/UCLAIS/Machine-Learning-Tutorials">Machine Learning Tutorial Series</a></i>
                  <br>
                  <ul>
                    <li>
                      Season 20/21 : 
                      <a href="https://github.com/UCLAIS/Machine-Learning-Tutorials">[GitHub]</a>
                      <a href="https://www.youtube.com/playlist?list=PL4JaWnfkTBbOLFG8xW5Ggtj_nRpgyDhIQ">[Lecture Recordings]</a>
                    </li>
                    <li>
                      Season 21/22 :
                      <a href="https://github.com/UCLAIS/ML-Tutorials-Season-2">[GitHub]</a>
                      <a href="https://kimdanny.github.io/blog/random/aisoc-tutorial-event/">[Pictures in blog]</a>
                    </li>
                  </ul>
                </td>
              </tr>
            
            </tbody>
          </table>
        </div>

        <hr style="width:100%;text-align:left;margin-left:0">

        <!-- Twitter Embedding | Source Code Creditation -->
        <div id="bottom">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="text-align:center; vertical-align:middle;">
                  <p>
                    You can know me better from my <a href="https://kimdanny.github.io/assets/CV.pdf">CV</a>
                  </p>
                  <!-- Enable cluster map -->
                  <!-- <p>
                    <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=nM14s1efSnD8YZnGHFblS1nLn-2T_22Vd38bBQLSQv0&cl=ffffff&w=a"></script>
                  </p> -->
                </td>
              </tr>
              <!-- <tr>
                <td style="text-align:center; vertical-align:middle; display:flex; justify-content:center;">
                  <a class="twitter-timeline" style="text-align:center; vertical-align:middle;"
                      data-height="400" data-width="400"
                      href="https://twitter.com/TEKnologyy?ref_src=twsrc%5Etfw">Tweets by TEKnologyy</a> 
                      <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
                </td>

              </tr> -->
              <tr>
                <td style="padding:0px">
                  <br>
                  <p style="text-align:center;font-size:small;">
                    Jump to the <a href="#top">top</a> of this page. <br>
                    Website source code is adapted from <a href="https://github.com/jonbarron/jonbarron_website">here</a>.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
        </div>

        <!-- end -->
      </td>
    </tr>
  </table>
  </div>
</body>

</html>
